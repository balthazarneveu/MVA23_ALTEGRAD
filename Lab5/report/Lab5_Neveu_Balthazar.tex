\documentclass[a4paper]{article}
\input{style/head.tex}
\newcommand{\yourname}{Balthazar Neveu}
\newcommand{\youremail}{balthazarneveu@gmail.com}
\newcommand{\assignmentnumber}{5}

\begin{document}

\input{style/header.tex}
\section*{Code}
For part 1, I set up a 4 CPU TensorDock remote machine to avoid working on my local machine.
This took some time. Pre-requisite setup.sh \& download\_models.sh .

More info:
\href{https://github.com/balthazarneveu/MVA23_ALTEGRAD/#readme}{MVA ALTEGRAD Balthazar Neveu on Github}

\section{Unsupervised node embeddings using Deepwalk}

We're trying to construct meaningful for a graph made of crawled french websites.

\begin{itemize}
    \item Number of nodes: 33226 . Each node is a website link.
    \item Number of edges: 354529 . Edges means the presence of hyperlinks between 2 websites.
    \item Dimension of embeddings of Word2Vec: 128
    \item Random walk length = 20
    \item At each node, $n_{walks}=10$ will be pre-stored.
\end{itemize}

\subsection*{Task 1 \& 2: Details on random walks sampling implementations}
Unexpectedly, initial na√Øve implementation took way too much time. 
Profiling led me to spot two bottlenecks:
\begin{itemize}
    \item I should not use \textit{np.random.choice} but rather \textit{random.randint} 
    to sample the next node in the walk
among neighbors.
    \item In addition, I used a dictionary to pre-store the neighbors instead of using the remove redundant calls to \textit{list(G.neighbors(node))}
    \item In this dataset, nodes are not indexes (e.g. integers) but string (hyperlinks).
    I tried to relabel to ints but it didn't really sped up and addressing the first two points was enough.
\end{itemize}

\begin{verbatim}
# Create a mapping from strings to integers
node_mapping = {node: i for i, node in enumerate(G.nodes())}

# Create a reverse mapping if you need to convert back to strings later
reverse_mapping = {i: node for node, i in node_mapping.items()}
G_int = nx.relabel_nodes(G, node_mapping)
\end{verbatim}

\subsection*{Task 4 : Node embdeddings visualization}
Refer to  \ref{fig:node_embeddings}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/tSNE_node_embeddings.png}
    \caption{t-SNE is used on the high (128) dimensional node embeddings to reduce dimensionality to 2D coordinates,
    which allows visualization of a few samples (here $n=100$) - embdeddings look relevant from the name of the website e.g.
    Pinterest, Tumblr, Wordpress appear in the same location for instance.}
    \label{fig:node_embeddings}
\end{figure}

\break
\subsection*{Question 1}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/m_k2_graph.png}
    \caption{Visualization of the $M=6$-$K_2$ components graph.}
    \label{fig:m_k2_graph}
\end{figure}

\subsection*{Nodes within Connected Components}
For nodes within each $K_2$ component, we expect a high cosine similarity in their embeddings. \\
In a $K_2$ graph, the only possible walks are between the two connected nodes. \\
The DeepWalk algorithm, which relies on these walks to generate embeddings,
will frequently observe these two nodes in proximity. 
Consequently, their vector embeddings will be very similar,
reflecting their direct and exclusive connection in the graph.

\subsection*{Nodes in Different Connected Components}
In contrast, nodes in different $K_2$ components are expected to exhibit lower cosine similarity in their embeddings. \\
Each $K_2$ is an isolated connected component, 
implying that random walks within one $K_2$ component do not include nodes from another $K_2$ component. \\
As a result, the embeddings generated by DeepWalk will capture this lack of relation in the graph structure, 
leading to lower similarity in the vector space for nodes from different components.

In \ref{fig:m_components}, I did a quick experiments to validate this result. This reveals the relavance of the features.
Notebook is available to reproduce these experiments.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/m_k2_graph_2d_embeddings.png}
    \caption{Relevant 2D embeddings from DeepWalk for  M-$K_2$ components. Nodes inside the same components (same color) have 
    very similar 2D embeddings whereas nodes from different components are farther away.}
    \label{fig:m_components}
\end{figure}

\break

\section{Node classification}
\subsection*{Task 5: visualization of Zachary's karate club}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{figures/labeled_karate_graph.png}
    \caption{Visualization of the Karate graph with 2 labels (groundtruth)}
    \label{fig:karate_labeled}
\end{figure}


\subsection*{Task 6/7: Classification of Zachary's karate club}


\subsection*{Task 8: Comparison of DeepWalk embeddings and spectral embeddings}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\textwidth]{figures/deepwalk_vs_laplacian_embeddings.png}
    \caption{Average classification accuracy of 10 runs (various seeds). Also report the min \& max accuracy}
    \label{fig:deepwalk_vs_laplacian_embdeddings}
\end{figure}


\section{Graph Neural Networks}
\subsection*{Task 11: GNN Classification on karate club - one-hot encoded input features \\ 100\% accuracy}
\begin{verbatim}
Epoch: 100 loss_train: 0.0640 acc_train: 1.0000 time: 0.0025s
\end{verbatim}

\subsection*{Task 12: GNN Classification on karate club - identical input features \\ 66.6\% accuracy}
Network cannot make a distinction between the input nodes features.
For instance, it prevents the network from memorizing a mapping between the training node "indexes" and the output class.
It forces the network to detect inherent similarity.

\begin{verbatim}
Epoch: 100 loss_train: 0.6528 acc_train: 0.6667 time: 0.0227s
\end{verbatim}
\bibliographystyle{plain}
\bibliography{references} % citation records are in the references.bib document

\end{document}