\documentclass[a4paper]{article}
\input{style/head.tex}
\newcommand{\yourname}{Balthazar Neveu}
\newcommand{\youremail}{balthazarneveu@gmail.com}
\newcommand{\assignmentnumber}{5}

\begin{document}

\input{style/header.tex}
\section*{Code}
For part 1, I set up a 4 CPU TensorDock remote machine to avoid working on my local machine.
This took some time. Pre-requisite setup.sh \& download\_models.sh .

More info:
\href{https://github.com/balthazarneveu/MVA23_ALTEGRAD/#readme}{MVA ALTEGRAD Balthazar Neveu on Github}

\section{Unsupervised node embeddings using Deepwalk}
We're trying to construct meaningful for a graph made of crawled french websites.

\begin{itemize}
    \item Number of nodes: 33226 . Each node is a website link.
    \item Number of edges: 354529 . Edges means the presence of hyperlinks between 2 websites.
    \item Dimension of embeddings of Word2Vec: 128
    \item Random walk length = 20
    \item At each node, $n_{walks}=10$ will be pre-stored.
\end{itemize}
\subsection*{Notes on task 1}
- Initial na√Øve implementation took way too much time. Profiling led me to realize I should not use
\textit{np.random.choice} but rather \textit{random.randint} to sample the next node in the walk
among neighbors. \\
- In addition, I used a dictionary to pre-store the neighbors instead of using the remove redundant calls to \textit{list(G.neighbors(node))}
\subsection*{Task 4 : Visualization}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/tSNE_node_embeddings.png}
    \caption{t-SNE is used on the high (128) dimensional node embeddings to reduce dimensionality to 2D coordinates,
    which allows visualization of a few samples (here $n=100$)}
    \label{fig:node_embeddings}
\end{figure}

\subsection*{Question 1}


\subsection*{Nodes within Connected Components}
For nodes within each $K_2$ component, we expect a high cosine similarity in their embeddings. \\
In a $K_2$ graph, the only possible walks are between the two connected nodes. \\
The DeepWalk algorithm, which relies on these walks to generate embeddings,
will frequently observe these two nodes in proximity. 
Consequently, their vector embeddings will be very similar,
reflecting their direct and exclusive connection in the graph.

\subsection*{Nodes in Different Connected Components}
In contrast, nodes in different $K_2$ components are expected to exhibit lower cosine similarity in their embeddings. \\
Each $K_2$ is an isolated connected component, 
implying that random walks within one $K_2$ component do not include nodes from another $K_2$ component. \\
As a result, the embeddings generated by DeepWalk will capture this lack of relation in the graph structure, 
leading to lower similarity in the vector space for nodes from different components.


\subsection{Task 6: Classification of Zachary's karate club - 100\% accuracy}

\begin{verbatim}
Epoch: 100 loss_train: 0.0640 acc_train: 1.0000 time: 0.0025s
\end{verbatim}

\bibliographystyle{plain}
\bibliography{references} % citation records are in the references.bib document

\end{document}