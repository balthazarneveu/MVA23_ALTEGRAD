{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> [fill me]\n",
        "\n",
        "</center>\n",
        "\n",
        "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset."
      ],
      "metadata": {
        "id": "DsD-LMKT7XMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Preparing the environment and installing libraries, models and data for Part 1 and Part 2</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
        "\n",
        "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. </b>\n",
        "\n",
        "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library."
      ],
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs\n",
        "%cd altegrad.lab3/libs\n",
        "!git clone https://github.com/hadi-abdine/fairseq\n",
        "!pip install git+https://github.com/hadi-abdine/fairseq\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "6cGqZ9ZB84Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir models\n",
        "%cd ../models\n",
        "\n",
        "!wget -c 'https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip' -O \"model_fairseq.zip\"\n",
        "!unzip model_fairseq.zip\n",
        "!rm model_fairseq.zip\n",
        "!rm -rf __MACOSX/\n",
        "\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\" -O \"model_huggingface.zip\"\n",
        "!unzip model_huggingface.zip\n",
        "!rm model_huggingface.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "rxHffsPm-EqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir data\n",
        "%cd ../data\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\" -O \"cls.books.zip\"\n",
        "!unzip cls.books.zip\n",
        "!rm cls.books.zip\n",
        "!rm -rf __MACOSX/\n",
        "!mkdir cls.books-json\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HfZ_znATNdya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 1: Fairseq</b>\n",
        "\n",
        "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
        "\n",
        " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
        " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
        " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
        "\n",
        " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>."
      ],
      "metadata": {
        "id": "H40TxVIvEWyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Number of parameters of the model</b>\n",
        "\n",
        "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch (Only the base model with out the head). (<b>Hint:</b> you can check the architecture of the model using model['model'])"
      ],
      "metadata": {
        "id": "TvpyZEexOXHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")\n",
        "\n",
        "print(\"The total number of parameters: \",     ) #fill the gap"
      ],
      "metadata": {
        "id": "j7isz60LOwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tokenizing the reviews</b>\n",
        "\n",
        "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
        "\n",
        "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
        "\n",
        "Documentation: https://github.com/google/sentencepiece#readme"
      ],
      "metadata": {
        "id": "gz8fnWOSI0eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "s = spm.SentencePieceProcessor(model_file='models/RoBERTa_small_fr/sentencepiece.bpe.model')\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "SENTS=\"review\"\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.'+SENTS, 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "        reviews = # tokenize the data using s.encode and a loop(check the documentation)\n",
        "\n",
        "        # It should look something like this :\n",
        "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
        "\n",
        "    with open('data/cls.books/'+split+'.spm.'+SENTS, 'w') as f:\n",
        "        for review in reviews:\n",
        "          f.write(review+'\\n')"
      ],
      "metadata": {
        "id": "D-hOlotmOW7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
        "\n",
        "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
        "\n",
        "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>. You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
        "\n",
        "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
        "\n",
        "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html"
      ],
      "metadata": {
        "id": "iGNK19XuKBk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "\n",
        "              --workers 8)#fill me - binarize the tokenized reviews\n",
        "\n",
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "\n",
        "              --workers 8)#fill me - binarize the labels"
      ],
      "metadata": {
        "id": "jIF1wvWoFp4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
        "\n",
        "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ],
      "metadata": {
        "id": "2SSjBcQJnuSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK= # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH= # fill me\n",
        "MODEL_PATH= # fill me\n",
        "MAX_EPOCH= # fill me\n",
        "MAX_SENTENCES= # fill me, batch size\n",
        "MAX_UPDATE= # fill me, number of backward propagation steps\n",
        "LR= # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = # fill me, use the accuracy metric\n",
        "NUM_CLASSES=#fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "JV2112YPJEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)\n"
      ],
      "metadata": {
        "id": "6Mdznms-EYyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
        "\n",
        "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error)"
      ],
      "metadata": {
        "id": "wi1U19Uunnse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK= # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr_random'\n",
        "DATA_PATH= # fill me\n",
        "MODEL_PATH= # fill me\n",
        "MAX_EPOCH= # fill me\n",
        "MAX_SENTENCES= # fill me, batch size\n",
        "MAX_UPDATE= # fill me, n_epochs * n_train_examples / total batch size\n",
        "LR= # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = # fill me, use the accuracy metric\n",
        "NUM_CLASSES=#fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "lSLWP6VUhUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $((6*$MAX_UPDATE/100)) \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)"
      ],
      "metadata": {
        "id": "qtsCysc4hb42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tensorboard Visualisation </B>\n",
        "\n",
        "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
      ],
      "metadata": {
        "id": "eHACXaPSLwu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tensorboard_logs"
      ],
      "metadata": {
        "id": "pwVvJNExS2dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 2: Finetuning $RoBERTa_{small}^{fr}$ using HuggingFace's Transfromers</b>\n",
        "In this section of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning using their run_glue script)."
      ],
      "metadata": {
        "id": "PAR_P343MCKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
        "\n",
        "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "c3M090L45oPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:\n",
        "        #fill the gap here to create train.json, valid.json and test.json\n"
      ],
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
      ],
      "metadata": {
        "id": "ICnN2FvnhTbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "0c6vM239VsAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES= # fill me, batch size.\n",
        "LR=#fill me, learning rate\n",
        "MAX_EPOCH=#fill me\n",
        "NUM_CLASSES=#fill me\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0"
      ],
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "\n",
        "  )#fill me"
      ],
      "metadata": {
        "id": "lV2Zla33hK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir checkpoints --port 6007"
      ],
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 2: Finetuning $BLOOM-560m$ using HuggingFace's Transfromers</b>\n",
        "In this part, we will fintune $BLOOM-560m$:https://huggingface.co/bigscience/bloom-560m on a question/answer dataset. We will equally use LoRA and quantization during the finetuning.\n",
        "\n",
        "## <b>Preparing the environment and installing libraries:<b>"
      ],
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.39.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\n",
        "!pip install -qqq datasets==2.12.0 --progress-bar off\n",
        "!pip install -qqq loralib==0.1.1 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off"
      ],
      "metadata": {
        "id": "8-VOzEaS7Wpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Loading the model and the tokenizer:</b>\n",
        "In this section, we will load the BLOOM model while using the BitsAndBytes library for quantization."
      ],
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bigscience/bloom-560m\"\n",
        "\n",
        "bnb_config = # fill the gap\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        # fill the gap: get the number of trainable parameters: trainable_params\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Configuring LoRA:<b>"
      ],
      "metadata": {
        "id": "UTgKyxhJMeEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "duTYSKKYVamH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Test the model before finetuning:<b>"
      ],
      "metadata": {
        "id": "5H1bBQaSNVsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = # fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>: \", with an empty response from the assistant\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "sRW7HPX6WCmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "DnmKXlqSWPQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Loading the question/answer dataset from HuggingFace:<b>"
      ],
      "metadata": {
        "id": "nbhQySqVMo2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"OpenLLM-France/Tutoriel\", data_files=\"ecommerce-faq-fr.json\")\n",
        "pd.DataFrame(data[\"train\"])"
      ],
      "metadata": {
        "id": "2zR54r9AWQ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Preparing the finetuning data:<b>"
      ],
      "metadata": {
        "id": "9oSZX9UcNBsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(data_point):\n",
        "    return # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\"\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "id": "cQiJpF41WZEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning:<b>"
      ],
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=1,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=80,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sjBMVb6yW_74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ],
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Test the model after the finetuning:<b>"
      ],
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "tCYynNlrXDhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question: str) -> str:\n",
        "    prompt = # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: \" with an empty response\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + len(assistant_start) :].strip()"
      ],
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Puis-je retourner un produit s'il s'agit d'un article en liquidation ou en vente finale ?\"\n",
        "print('-', prompt,'\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "prompt = \"Que se passe-t-il lorsque je retourne un article en déstockage ?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "prompt = \"Comment puis-je savoir quand je recevrai ma commande ?\"\n",
        "print(generate_response(prompt))"
      ],
      "metadata": {
        "id": "sYaO6H_hXsvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}