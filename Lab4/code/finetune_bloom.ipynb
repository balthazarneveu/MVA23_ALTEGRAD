{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"gray\">\n",
    "\n",
    "# <b>Part 2: Finetuning $BLOOM-560m$ using HuggingFace's Transfromers</b>\n",
    "In this part, we will fintune [BLOOM-560m](https://huggingface.co/bigscience/bloom-560m) on a question/answer dataset. \n",
    "\n",
    "We will equally use LoRA and quantization during the finetuning.\n",
    "\n",
    "## <b>Preparing the environment and installing libraries:<b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: Loading binary /home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"gray\">\n",
    "\n",
    "## <b>Loading the model and the tokenizer:</b>\n",
    "In this section, we will load the BLOOM model while using the BitsAndBytes library for quantization.\n",
    "\n",
    "</font>\n",
    "\n",
    "[Bloom 560m](https://huggingface.co/bigscience/bloom-560m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: BitsAndBytes configuration [4 bits quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bigscience/bloom-560m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Task 6\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m    load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m    bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m    bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m    bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     MODEL_NAME,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:490\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    489\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    491\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    494\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/transformers/modeling_utils.py:2815\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2811\u001b[0m         device_map_without_lm_head \u001b[39m=\u001b[39m {\n\u001b[1;32m   2812\u001b[0m             key: device_map[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   2813\u001b[0m         }\n\u001b[1;32m   2814\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues() \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m-> 2815\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2816\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2817\u001b[0m \u001b[39m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m \u001b[39m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m \u001b[39m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   2820\u001b[0m \u001b[39m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m \u001b[39m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \u001b[39m                for more details.\u001b[39;00m\n\u001b[1;32m   2823\u001b[0m \u001b[39m                \"\"\"\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m             )\n\u001b[1;32m   2825\u001b[0m         \u001b[39mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   2827\u001b[0m \u001b[39mif\u001b[39;00m from_tf:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "\n",
    "# Task 6\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_trainable_parameters(model):\n",
    "\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        # fill the gap: get the number of trainable parameters: trainable_params\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m config \u001b[39m=\u001b[39m LoraConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     r\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     lora_alpha\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCAUSAL_LM\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m get_peft_model(model, config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m print_trainable_parameters(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"gray\">\n",
    "\n",
    "## <b>Test the model before finetuning:<b>\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "[Hugging Face Transformers: Generation configuration](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Comment je peux créer un compte?  \n",
      "<assistant>:  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>:\", with an empty response from the assistant\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(prompt)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m generation_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m generation_config\u001b[39m.\u001b[39mmax_new_tokens \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/finetune_bloom.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m generation_config\u001b[39m.\u001b[39mtemperature \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"<human>: Comment je peux créer un compte?  \\n<assistant>:  \" \n",
    "# fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>:\", with an empty response from the assistant\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:767\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 767\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    768\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:767\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 767\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    768\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_bloom/lib/python3.9/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode(): # Inference, not training.\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"gray\">\n",
    "\n",
    "# Q/A dataset from Hugging face\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pour créer un compte, cliquez sur le bouton \"S...</td>\n",
       "      <td>Comment puis-je créer un compte ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nous acceptons les principales cartes de crédi...</td>\n",
       "      <td>Quels sont les modes de paiement acceptés ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vous pouvez suivre votre commande en vous conn...</td>\n",
       "      <td>Comment puis-je suivre ma commande ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notre politique de retour vous permet de renvo...</td>\n",
       "      <td>Quelle est votre politique de retour ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vous pouvez annuler votre commande si elle n'a...</td>\n",
       "      <td>Puis-je annuler ma commande ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Si un produit est listé comme \"épuisé\" mais di...</td>\n",
       "      <td>Puis-je commander un produit s'il est listé co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Oui, vous pouvez retourner un produit acheté a...</td>\n",
       "      <td>Puis-je retourner un produit acheté avec une c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Si un produit n'est pas disponible dans la cou...</td>\n",
       "      <td>Puis-je demander un produit s'il n'est pas dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Si un produit est listé comme \"bientôt disponi...</td>\n",
       "      <td>Puis-je commander un produit s'il est listé co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Oui, vous pouvez retourner un produit acheté l...</td>\n",
       "      <td>Puis-je retourner un produit acheté lors d'un ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer  \\\n",
       "0   Pour créer un compte, cliquez sur le bouton \"S...   \n",
       "1   Nous acceptons les principales cartes de crédi...   \n",
       "2   Vous pouvez suivre votre commande en vous conn...   \n",
       "3   Notre politique de retour vous permet de renvo...   \n",
       "4   Vous pouvez annuler votre commande si elle n'a...   \n",
       "..                                                ...   \n",
       "74  Si un produit est listé comme \"épuisé\" mais di...   \n",
       "75  Oui, vous pouvez retourner un produit acheté a...   \n",
       "76  Si un produit n'est pas disponible dans la cou...   \n",
       "77  Si un produit est listé comme \"bientôt disponi...   \n",
       "78  Oui, vous pouvez retourner un produit acheté l...   \n",
       "\n",
       "                                             question  \n",
       "0                   Comment puis-je créer un compte ?  \n",
       "1         Quels sont les modes de paiement acceptés ?  \n",
       "2                Comment puis-je suivre ma commande ?  \n",
       "3              Quelle est votre politique de retour ?  \n",
       "4                       Puis-je annuler ma commande ?  \n",
       "..                                                ...  \n",
       "74  Puis-je commander un produit s'il est listé co...  \n",
       "75  Puis-je retourner un produit acheté avec une c...  \n",
       "76  Puis-je demander un produit s'il n'est pas dis...  \n",
       "77  Puis-je commander un produit s'il est listé co...  \n",
       "78  Puis-je retourner un produit acheté lors d'un ...  \n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"OpenLLM-France/Tutoriel\", data_files=\"ecommerce-faq-fr.json\")\n",
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Generate prompts\n",
    "[Huggin face Transformers generation](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/text_generation#transformers.GenerationMixin.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 79/79 [00:00<00:00, 1515.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'question', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 79\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"<human>: {data_point['question']}  \\n <assistant>: {data_point['answer']}\" \n",
    "    # transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\"\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=80,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Test the model after the finetuning:<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"<human>: {question}  \\n <assistant>: \"\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Puis-je retourner un produit s'il s'agit d'un article en liquidation ou en vente finale ?\"\n",
    "print('-', prompt,'\\n')\n",
    "print(generate_response(prompt))\n",
    "\n",
    "prompt = \"Que se passe-t-il lorsque je retourne un article en déstockage ?\"\n",
    "print('\\n\\n\\n-', prompt, '\\n')\n",
    "print(generate_response(prompt))\n",
    "\n",
    "prompt = \"Comment puis-je savoir quand je recevrai ma commande ?\"\n",
    "\n",
    "print('\\n\\n\\n-', prompt, '\\n')\n",
    "print(generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bloom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
