{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def abspath(pth): return Path(pth).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## Task 2: Preprocessing the dataset\n",
    "Prepare the data for finetuning. To do so you have to:\n",
    "1. Use the provided sentencepiece model to tokenize the text.\n",
    "2. Binarize the data using `fairseq-preprocess` command\n",
    "\n",
    "### Tokenizing the reviews\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize \n",
    "- the input three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> \n",
    "- and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
    "\n",
    "Documentation: https://github.com/google/sentencepiece#readme\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Tokenization\n",
    "When we split the sentences into tokens here, we use a provided byte pair encoding model.\n",
    "\n",
    "When training from scratch, we should start from scratch and tokenize a corpus from scratch...\n",
    "\n",
    "but if we want to **re-use a pretrained model**, we need to follow its vocabulary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS=['train', 'test', 'valid']\n",
    "TOKENIZER_MODEL_PATH = '../models/RoBERTa_small_fr/sentencepiece.bpe.model'\n",
    "DATA_BOOKS = Path('../data/cls.books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2.1\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)\n",
    "\n",
    "SENTS=\"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(DATA_BOOKS/(split+'.'+SENTS), 'r') as f:\n",
    "        reviews = f.readlines()\n",
    "        \n",
    "        tokenized = [\" \".join(s.encode(review, out_type=str)) for review in reviews]\n",
    "        print(f\"Original: {reviews[0][:80]}, \\t Tokenized: {tokenized[0][:80]}\")\n",
    "        # tokenize the data using s.encode and a loop(check the documentation)\n",
    "\n",
    "        # It should look something like this :\n",
    "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "    \n",
    "    with open(DATA_BOOKS/(split+'.spm' + \".\" + SENTS), 'w') as f:\n",
    "        f.writelines(\"\\n\".join(tokenized)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. \n",
    "> Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>.\n",
    ">\n",
    "> You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Binarization\n",
    "\n",
    "- `--only-source` : Only process the source language\n",
    "- `srcdict` allows forcing the tokenization dictionary.\n",
    "- `-s \"spm.review\"` for source language , `-t \"\"` for an undefined target language.\n",
    "\n",
    "Note on compression\n",
    "> - train.review `1102kb`\n",
    "> - train.spm `1741kb`  (added extra characters to get ready for binarization)\n",
    "> - train.bin : `569kb` (file appears compressed, sentences read from disk will be lighter and decoded on the CPU).\n",
    "\n",
    "\n",
    "In the context of `fairseq` ,  `.bin` and `.idx` files are integral to the data format used for efficient storage and retrieval of the training data. \n",
    "\n",
    "Here's what each of these file types represents:\n",
    "\n",
    "1. **`.bin` Files**: \n",
    "   - These are binary files that contain the actual training data.\n",
    "   - In `fairseq`, data (such as tokenized text) is converted into a numerical format (indices corresponding to tokens in the dictionary) and then stored in a binary format.\n",
    "   - This binary format is more space-efficient and faster to **read from disk compared to plain text**, which is crucial for large datasets commonly used in machine learning.\n",
    "\n",
    "2. **`.idx` Files**: \n",
    "   - These files are index files that accompany the `.bin` files.\n",
    "   - The `.idx` file stores the byte offsets of each example (like a sentence or a document) in the corresponding `.bin` file.\n",
    "   - When the training process requires a specific example from the dataset, it uses the `.idx` file to quickly find where that example starts and ends in the `.bin` file.\n",
    "   - This allows for efficient random access to examples in the dataset without having to read the entire `.bin` file sequentially, which is especially beneficial for large datasets.\n",
    "\n",
    "In summary, the combination of `.bin` and `.idx` files in `fairseq` enables efficient and fast access to large-scale datasets, a critical aspect of training modern neural networks, particularly in tasks like natural language processing where datasets can be extremely large.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2.2\n",
    "SRC_DICT = \"../models/RoBERTa_small_fr/dict.txt\"\n",
    "DESTINATION_ROOT = Path(\"../data/cls-books-bin\")\n",
    "\n",
    "# (file suffix, folder output, dictionary to use)\n",
    "CONFIG = [\n",
    "    (\".spm.review\", \"input0\", f\"--srcdict {SRC_DICT}\"), # binarize the tokenized reviews\n",
    "    (\".label\", \"label\", \"\"), # binarize the labels - fairseq preprocess will build the dictionary needed (0 & 1 basically)\n",
    "]\n",
    "for suffix, out_dir, src_dict in CONFIG:\n",
    "    CORPUS_TRAIN, CORPUS_VALID, CORPUS_TEST = [str(DATA_BOOKS/f\"{dataset_split}{suffix}\") for dataset_split in ['train', 'valid', 'test']]\n",
    "    DESTINATION_FOLDER = str(DESTINATION_ROOT/out_dir)\n",
    "    !(python ~/fairseq/fairseq_cli/preprocess.py \\\n",
    "                --only-source \\\n",
    "                --workers 8 \\\n",
    "                $src_dict \\\n",
    "                --destdir \"$DESTINATION_FOLDER\"\\\n",
    "                --trainpref \"$CORPUS_TRAIN\" \\\n",
    "                --validpref \"$CORPUS_VALID\" \\\n",
    "                --testpref \"$CORPUS_TEST\" \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
    "\n",
    "In this section you will use `fairseq/fairseq_cli/train.py` python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
    "\n",
    "Make sure to use the following hyper-parameters: \n",
    "- batch size=8\n",
    "- max number of epochs = 5\n",
    "- optimizer: Adam\n",
    "- max learning rate: 1e-05,\n",
    "- warm up ratio: 0.06, \n",
    "- learning rate scheduler: linear\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET='books'\n",
    "TASK= 'sentence_prediction' # sentence prediction task on fairseq\n",
    "MODEL='RoBERTa_small_fr'\n",
    "DATA_PATH= \"../data/cls-books-bin\"\n",
    "MODEL_PATH= \"../models/RoBERTa_small_fr/model.pt\"\n",
    "MAX_EPOCH= 5\n",
    "MAX_SENTENCES= 8 # batch size\n",
    "MAX_UPDATE = 1200 # number of backward propagation steps\n",
    "LR= 1.E-5\n",
    "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = 'accuracy' # use the accuracy metric\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "SEEDS=3\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "WARMUP = int(0.06 * MAX_UPDATE) # warmup ratio=6% of the whole training\n",
    "ARCHITECTURE = \"roberta_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user/MVA23_ALTEGRAD/Lab4/code/tokenizer.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.139/home/user/MVA23_ALTEGRAD/Lab4/code/tokenizer.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m Path(MODEL_PATH)\u001b[39m.\u001b[39mexists(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00mMODEL_PATH\u001b[39m}\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "assert Path(MODEL_PATH).exists(), f\"Error: {MODEL_PATH} does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1200, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/0', 'restore_file': '../models/RoBERTa_small_fr', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1200, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/0', restore_file='../models/RoBERTa_small_fr', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=72, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1200', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 72, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1200.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 16:51:36 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 16:51:36 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 16:51:36 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 16:51:36 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 16:51:36 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 16:51:36 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 16:51:36 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:51:36 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:51:36 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 16:51:36 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 16:51:36 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 16:51:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 16:51:37 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 16:51:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001: 100%|▉| 224/225 [02:27<00:00,  1.57it/s, loss=0.99, nll_loss=0.007, a2023-11-11 16:54:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|▎      | 1/25 [00:00<00:08,  2.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 2/25 [00:00<00:05,  3.96it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  12%|▊      | 3/25 [00:00<00:05,  4.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|█      | 4/25 [00:00<00:04,  4.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  20%|█▍     | 5/25 [00:01<00:04,  4.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:01<00:03,  5.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▉     | 7/25 [00:01<00:03,  5.34it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  32%|██▏    | 8/25 [00:01<00:03,  5.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|██▌    | 9/25 [00:01<00:02,  5.53it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|██▍   | 10/25 [00:02<00:02,  5.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  44%|██▋   | 11/25 [00:02<00:02,  5.38it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  48%|██▉   | 12/25 [00:02<00:02,  5.29it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  52%|███   | 13/25 [00:02<00:02,  5.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  56%|███▎  | 14/25 [00:02<00:02,  5.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|███▌  | 15/25 [00:02<00:01,  5.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  64%|███▊  | 16/25 [00:03<00:01,  5.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  68%|████  | 17/25 [00:03<00:01,  6.13it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  72%|████▎ | 18/25 [00:03<00:01,  6.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76%|████▌ | 19/25 [00:03<00:00,  6.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  80%|████▊ | 20/25 [00:03<00:00,  6.06it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|█████ | 21/25 [00:03<00:00,  6.01it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|█████▎| 22/25 [00:04<00:00,  5.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 23/25 [00:04<00:00,  5.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  96%|█████▊| 24/25 [00:04<00:00,  6.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 25/25 [00:04<00:00,  6.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-11-11 16:54:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1 | nll_loss 0.008 | accuracy 50 | wps 5917.3 | wpb 1048 | bsz 8 | num_updates 225\n",
      "2023-11-11 16:54:09 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
      "\n",
      "epoch 001 | valid on 'test' subset:   0%|               | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   0%|       | 1/250 [00:00<01:19,  3.12it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   1%|       | 2/250 [00:00<01:07,  3.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   1%|       | 3/250 [00:00<00:55,  4.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   2%|       | 4/250 [00:00<00:51,  4.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   2%|▏      | 5/250 [00:01<00:51,  4.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   2%|▏      | 6/250 [00:01<00:47,  5.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 7/250 [00:01<00:46,  5.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   3%|▏      | 8/250 [00:01<00:42,  5.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   4%|▎      | 9/250 [00:01<00:38,  6.34it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   4%|▏     | 10/250 [00:01<00:39,  6.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   4%|▎     | 11/250 [00:02<00:40,  5.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   5%|▎     | 12/250 [00:02<00:44,  5.41it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   5%|▎     | 13/250 [00:02<00:47,  5.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   6%|▎     | 14/250 [00:02<00:51,  4.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   6%|▎     | 15/250 [00:02<00:48,  4.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   6%|▍     | 16/250 [00:03<00:46,  5.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   7%|▍     | 17/250 [00:03<00:43,  5.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   7%|▍     | 18/250 [00:03<00:41,  5.61it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   8%|▍     | 19/250 [00:03<00:40,  5.65it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   8%|▍     | 20/250 [00:03<00:42,  5.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   8%|▌     | 21/250 [00:04<00:40,  5.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   9%|▌     | 22/250 [00:04<00:45,  5.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:   9%|▌     | 23/250 [00:04<00:41,  5.51it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  10%|▌     | 24/250 [00:04<00:40,  5.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  10%|▌     | 25/250 [00:04<00:35,  6.32it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  10%|▌     | 26/250 [00:04<00:36,  6.07it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  11%|▋     | 27/250 [00:05<00:37,  5.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  11%|▋     | 28/250 [00:05<00:37,  5.87it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  12%|▋     | 29/250 [00:05<00:37,  5.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  12%|▋     | 30/250 [00:05<00:33,  6.57it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  12%|▋     | 31/250 [00:05<00:35,  6.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  13%|▊     | 32/250 [00:05<00:36,  5.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  13%|▊     | 33/250 [00:06<00:37,  5.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▊     | 34/250 [00:06<00:36,  5.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▊     | 35/250 [00:06<00:38,  5.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  14%|▊     | 36/250 [00:06<00:39,  5.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  15%|▉     | 37/250 [00:06<00:39,  5.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  15%|▉     | 38/250 [00:06<00:39,  5.42it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  16%|▉     | 39/250 [00:07<00:36,  5.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  16%|▉     | 40/250 [00:07<00:36,  5.72it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  16%|▉     | 41/250 [00:07<00:36,  5.72it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  17%|█     | 42/250 [00:07<00:36,  5.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  17%|█     | 43/250 [00:07<00:36,  5.65it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  18%|█     | 44/250 [00:08<00:37,  5.55it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  18%|█     | 45/250 [00:08<00:36,  5.65it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  18%|█     | 46/250 [00:08<00:36,  5.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  19%|█▏    | 47/250 [00:08<00:36,  5.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  19%|█▏    | 48/250 [00:08<00:35,  5.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  20%|█▏    | 49/250 [00:08<00:35,  5.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  20%|█▏    | 50/250 [00:09<00:35,  5.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  20%|█▏    | 51/250 [00:09<00:34,  5.72it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  21%|█▏    | 52/250 [00:09<00:34,  5.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  21%|█▎    | 53/250 [00:09<00:35,  5.57it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  22%|█▎    | 54/250 [00:09<00:36,  5.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  22%|█▎    | 55/250 [00:10<00:37,  5.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  22%|█▎    | 56/250 [00:10<00:36,  5.25it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  23%|█▎    | 57/250 [00:10<00:36,  5.32it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  23%|█▍    | 58/250 [00:10<00:35,  5.38it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  24%|█▍    | 59/250 [00:10<00:33,  5.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  24%|█▍    | 60/250 [00:10<00:33,  5.73it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  24%|█▍    | 61/250 [00:11<00:31,  5.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  25%|█▍    | 62/250 [00:11<00:32,  5.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  25%|█▌    | 63/250 [00:11<00:32,  5.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  26%|█▌    | 64/250 [00:11<00:31,  5.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  26%|█▌    | 65/250 [00:11<00:32,  5.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  26%|█▌    | 66/250 [00:11<00:32,  5.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  27%|█▌    | 67/250 [00:12<00:35,  5.19it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  27%|█▋    | 68/250 [00:12<00:36,  5.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  28%|█▋    | 69/250 [00:12<00:34,  5.30it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  28%|█▋    | 70/250 [00:12<00:29,  6.11it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  28%|█▋    | 71/250 [00:12<00:28,  6.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  29%|█▋    | 72/250 [00:12<00:28,  6.19it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  29%|█▊    | 73/250 [00:13<00:28,  6.15it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▊    | 74/250 [00:13<00:29,  6.05it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▊    | 75/250 [00:13<00:29,  5.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  30%|█▊    | 76/250 [00:13<00:28,  6.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  31%|█▊    | 77/250 [00:13<00:28,  6.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  31%|█▊    | 78/250 [00:13<00:27,  6.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  32%|█▉    | 79/250 [00:14<00:27,  6.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  32%|█▉    | 80/250 [00:14<00:27,  6.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  32%|█▉    | 81/250 [00:14<00:28,  6.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  33%|█▉    | 82/250 [00:14<00:27,  6.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  33%|█▉    | 83/250 [00:14<00:28,  5.93it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  34%|██    | 84/250 [00:14<00:28,  5.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  34%|██    | 85/250 [00:15<00:29,  5.65it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  34%|██    | 86/250 [00:15<00:28,  5.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  35%|██    | 87/250 [00:15<00:25,  6.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  35%|██    | 88/250 [00:15<00:27,  5.87it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|██▏   | 89/250 [00:15<00:27,  5.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|██▏   | 90/250 [00:16<00:28,  5.64it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  36%|██▏   | 91/250 [00:16<00:27,  5.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  37%|██▏   | 92/250 [00:16<00:27,  5.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  37%|██▏   | 93/250 [00:16<00:27,  5.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  38%|██▎   | 94/250 [00:16<00:26,  5.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  38%|██▎   | 95/250 [00:16<00:27,  5.64it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  38%|██▎   | 96/250 [00:17<00:26,  5.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  39%|██▎   | 97/250 [00:17<00:25,  5.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  39%|██▎   | 98/250 [00:17<00:25,  5.93it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  40%|██▍   | 99/250 [00:17<00:25,  5.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  40%|██   | 100/250 [00:17<00:25,  5.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  40%|██   | 101/250 [00:17<00:23,  6.38it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  41%|██   | 102/250 [00:18<00:23,  6.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  42%|██   | 104/250 [00:18<00:21,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  42%|██   | 105/250 [00:18<00:22,  6.56it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  42%|██   | 106/250 [00:18<00:22,  6.28it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  43%|██▏  | 107/250 [00:18<00:23,  6.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  43%|██▏  | 108/250 [00:18<00:23,  6.11it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  44%|██▏  | 109/250 [00:19<00:23,  6.11it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  44%|██▏  | 110/250 [00:19<00:23,  5.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  44%|██▏  | 111/250 [00:19<00:23,  5.96it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  45%|██▏  | 112/250 [00:19<00:22,  6.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  45%|██▎  | 113/250 [00:19<00:22,  6.06it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  46%|██▎  | 114/250 [00:19<00:22,  6.00it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  46%|██▎  | 115/250 [00:20<00:23,  5.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  46%|██▎  | 116/250 [00:20<00:22,  5.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  47%|██▎  | 117/250 [00:20<00:22,  5.98it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  47%|██▎  | 118/250 [00:20<00:21,  6.01it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  48%|██▍  | 119/250 [00:20<00:22,  5.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  48%|██▍  | 120/250 [00:20<00:21,  6.01it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  48%|██▍  | 121/250 [00:21<00:22,  5.67it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  49%|██▍  | 122/250 [00:21<00:22,  5.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  49%|██▍  | 123/250 [00:21<00:22,  5.62it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  50%|██▍  | 124/250 [00:21<00:21,  5.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  50%|██▌  | 125/250 [00:21<00:21,  5.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  50%|██▌  | 126/250 [00:22<00:20,  5.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  51%|██▌  | 127/250 [00:22<00:20,  6.05it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  51%|██▌  | 128/250 [00:22<00:19,  6.16it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██▌  | 129/250 [00:22<00:19,  6.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██▌  | 130/250 [00:22<00:19,  6.10it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  52%|██▌  | 131/250 [00:22<00:19,  6.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  53%|██▋  | 133/250 [00:23<00:17,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  54%|██▋  | 134/250 [00:23<00:17,  6.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  54%|██▋  | 135/250 [00:23<00:17,  6.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  54%|██▋  | 136/250 [00:23<00:17,  6.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  55%|██▋  | 137/250 [00:23<00:17,  6.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  55%|██▊  | 138/250 [00:23<00:17,  6.33it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  56%|██▊  | 139/250 [00:24<00:17,  6.27it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  56%|██▊  | 140/250 [00:24<00:18,  5.95it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  56%|██▊  | 141/250 [00:24<00:17,  6.10it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  57%|██▊  | 142/250 [00:24<00:17,  6.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  57%|██▊  | 143/250 [00:24<00:17,  6.06it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  58%|██▉  | 144/250 [00:24<00:17,  6.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'test' subset:  58%|██▉  | 145/250 [00:25<00:18,  5.72it/s]\u001b[A^C\n",
      "\n",
      "Traceback (most recent call last):                                              \u001b[A\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 564, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/home/user/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 190, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 330, in train\n",
      "    valid_losses, should_stop = validate_and_save(\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 421, in validate_and_save\n",
      "    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 505, in validate\n",
      "    trainer.valid_step(sample)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq/trainer.py\", line 1121, in valid_step\n",
      "    _loss, sample_size, logging_output = self.task.valid_step(\n",
      "  File \"/home/user/fairseq/fairseq/tasks/fairseq_task.py\", line 527, in valid_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/criterions/sentence_prediction.py\", line 48, in forward\n",
      "    logits, _ = model(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 255, in forward\n",
      "    x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 601, in forward\n",
      "    x, extra = self.extract_features(\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 609, in extract_features\n",
      "    encoder_out = self.sentence_encoder(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/transformer/transformer_encoder.py\", line 165, in forward\n",
      "    return self.forward_scriptable(\n",
      "  File \"/home/user/fairseq/fairseq/models/transformer/transformer_encoder.py\", line 230, in forward_scriptable\n",
      "    lr = layer(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/modules/transformer_layer.py\", line 197, in forward\n",
      "    x, _ = self.self_attn(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/modules/multihead_attention.py\", line 539, in forward\n",
      "    return F.multi_head_attention_forward(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/functional.py\", line 5440, in multi_head_attention_forward\n",
      "    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "KeyboardInterrupt\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 16:54:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1200, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/1', 'restore_file': '../models/RoBERTa_small_fr', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1200, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/1', restore_file='../models/RoBERTa_small_fr', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=72, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1200', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 72, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1200.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 16:54:40 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 16:54:40 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 16:54:41 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 16:54:41 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 16:54:41 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 16:54:41 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:54:41 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:54:41 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 16:54:41 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 16:54:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 16:54:41 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 16:54:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:  18%|▏| 41/225 [00:26<02:00,  1.52it/s, loss=0.993, nll_loss=0.008, a^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 564, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/home/user/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 190, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 316, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq/trainer.py\", line 824, in train_step\n",
      "    loss, sample_size_i, logging_output = self.task.train_step(\n",
      "  File \"/home/user/fairseq/fairseq/tasks/fairseq_task.py\", line 521, in train_step\n",
      "    optimizer.backward(loss)\n",
      "  File \"/home/user/fairseq/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
      "    loss.backward()\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 16:55:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1200, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/2', 'restore_file': '../models/RoBERTa_small_fr', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1200, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1200_lr1e-05_me5/2', restore_file='../models/RoBERTa_small_fr', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=72, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1200', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 72, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1200.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 16:55:14 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 16:55:14 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 16:55:15 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 16:55:15 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 16:55:15 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 16:55:15 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:55:15 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_fr\n",
      "2023-11-11 16:55:15 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 16:55:15 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 16:55:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 16:55:15 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 16:55:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   3%| | 7/225 [00:04<02:14,  1.62it/s, loss=1.008, nll_loss=0.008, ac"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python ~/fairseq/fairseq_cli/train.py \\\n",
    "                $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch $ARCHITECTURE \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
