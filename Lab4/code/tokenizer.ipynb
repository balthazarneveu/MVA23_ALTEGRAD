{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def abspath(pth): return Path(pth).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## Task 2: Preprocessing the dataset\n",
    "Prepare the data for finetuning. To do so you have to:\n",
    "1. Use the provided sentencepiece model to tokenize the text.\n",
    "2. Binarize the data using `fairseq-preprocess` command\n",
    "\n",
    "### Tokenizing the reviews\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize \n",
    "- the input three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> \n",
    "- and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
    "\n",
    "Documentation: https://github.com/google/sentencepiece#readme\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Tokenization\n",
    "When we split the sentences into tokens here, we use a provided byte pair encoding model.\n",
    "\n",
    "When training from scratch, we should start from scratch and tokenize a corpus from scratch...\n",
    "\n",
    "but if we want to **re-use a pretrained model**, we need to follow its vocabulary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS=['train', 'test', 'valid']\n",
    "TOKENIZER_MODEL_PATH = '../models/RoBERTa_small_fr/sentencepiece.bpe.model'\n",
    "DATA_BOOKS = Path('../data/cls.books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ce livre est tout simplement magique !  il vaut le detour suspense, humour, magi, \t Tokenized: ▁Ce ▁livre ▁est ▁tout ▁simplement ▁mag ique ▁! ▁il ▁vaut ▁le ▁de tour ▁suspens e\n",
      "Original: J'ai lu ce livre car dans ma ville, tout le monde s'en sert et le commande. C'es, \t Tokenized: ▁J ' ai ▁lu ▁ce ▁livre ▁car ▁dans ▁ma ▁ville , ▁tout ▁le ▁monde ▁s ' en ▁sert ▁e\n",
      "Original: Ce livre explique techniquement et de façon très compréhensible, même pour des n, \t Tokenized: ▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.1\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)\n",
    "\n",
    "SENTS=\"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(DATA_BOOKS/(split+'.'+SENTS), 'r') as f:\n",
    "        reviews = f.readlines()\n",
    "        \n",
    "        tokenized = [\" \".join(s.encode(review, out_type=str)) for review in reviews]\n",
    "        print(f\"Original: {reviews[0][:80]}, \\t Tokenized: {tokenized[0][:80]}\")\n",
    "        # tokenize the data using s.encode and a loop(check the documentation)\n",
    "\n",
    "        # It should look something like this :\n",
    "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "    \n",
    "    with open(DATA_BOOKS/(split+'.spm' + \".\" + SENTS), 'w') as f:\n",
    "        f.writelines(\"\\n\".join(tokenized)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. \n",
    "> Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>.\n",
    ">\n",
    "> You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Binarization\n",
    "\n",
    "- `--only-source` : Only process the source language\n",
    "- `srcdict` allows forcing the tokenization dictionary.\n",
    "- `-s \"spm.review\"` for source language , `-t \"\"` for an undefined target language.\n",
    "\n",
    "Note on compression\n",
    "> - train.review `1102kb`\n",
    "> - train.spm `1741kb`  (added extra characters to get ready for binarization)\n",
    "> - train.bin : `569kb` (file appears compressed, sentences read from disk will be lighter and decoded on the CPU).\n",
    "\n",
    "\n",
    "In the context of `fairseq` ,  `.bin` and `.idx` files are integral to the data format used for efficient storage and retrieval of the training data. \n",
    "\n",
    "Here's what each of these file types represents:\n",
    "\n",
    "1. **`.bin` Files**: \n",
    "   - These are binary files that contain the actual training data.\n",
    "   - In `fairseq`, data (such as tokenized text) is converted into a numerical format (indices corresponding to tokens in the dictionary) and then stored in a binary format.\n",
    "   - This binary format is more space-efficient and faster to **read from disk compared to plain text**, which is crucial for large datasets commonly used in machine learning.\n",
    "\n",
    "2. **`.idx` Files**: \n",
    "   - These files are index files that accompany the `.bin` files.\n",
    "   - The `.idx` file stores the byte offsets of each example (like a sentence or a document) in the corresponding `.bin` file.\n",
    "   - When the training process requires a specific example from the dataset, it uses the `.idx` file to quickly find where that example starts and ends in the `.bin` file.\n",
    "   - This allows for efficient random access to examples in the dataset without having to read the entire `.bin` file sequentially, which is especially beneficial for large datasets.\n",
    "\n",
    "In summary, the combination of `.bin` and `.idx` files in `fairseq` enables efficient and fast access to large-scale datasets, a critical aspect of training modern neural networks, particularly in tasks like natural language processing where datasets can be extremely large.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:19 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.spm.review', validpref='../data/cls.books/valid.spm.review', testpref='../data/cls.books/test.spm.review', align_suffix=None, destdir='../data/cls-books-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:20 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/input0\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.label', validpref='../data/cls.books/valid.label', testpref='../data/cls.books/test.label', align_suffix=None, destdir='../data/cls-books-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/label\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.2\n",
    "SRC_DICT = \"../models/RoBERTa_small_fr/dict.txt\"\n",
    "DESTINATION_ROOT = Path(\"../data/cls-books-bin\")\n",
    "\n",
    "# (file suffix, folder output, dictionary to use)\n",
    "CONFIG = [\n",
    "    (\".spm.review\", \"input0\", f\"--srcdict {SRC_DICT}\"), # binarize the tokenized reviews\n",
    "    (\".label\", \"label\", \"\"), # binarize the labels - fairseq preprocess will build the dictionary needed (0 & 1 basically)\n",
    "]\n",
    "for suffix, out_dir, src_dict in CONFIG:\n",
    "    CORPUS_TRAIN, CORPUS_VALID, CORPUS_TEST = [str(DATA_BOOKS/f\"{dataset_split}{suffix}\") for dataset_split in ['train', 'valid', 'test']]\n",
    "    DESTINATION_FOLDER = str(DESTINATION_ROOT/out_dir)\n",
    "    !(python ~/fairseq/fairseq_cli/preprocess.py \\\n",
    "                --only-source \\\n",
    "                --workers 8 \\\n",
    "                $src_dict \\\n",
    "                --destdir \"$DESTINATION_FOLDER\"\\\n",
    "                --trainpref \"$CORPUS_TRAIN\" \\\n",
    "                --validpref \"$CORPUS_VALID\" \\\n",
    "                --testpref \"$CORPUS_TEST\" \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
    "\n",
    "In this section you will use `fairseq/fairseq_cli/train.py` python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
    "\n",
    "Make sure to use the following hyper-parameters: \n",
    "- batch size=8\n",
    "- max number of epochs = 5\n",
    "- optimizer: Adam\n",
    "- max learning rate: 1e-05,\n",
    "- warm up ratio: 0.06, \n",
    "- learning rate scheduler: linear\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET='books'\n",
    "TASK= 'sentence_prediction' # sentence prediction task on fairseq\n",
    "MODEL='RoBERTa_small_fr'\n",
    "DATA_PATH= \"../data/cls-books-bin\"\n",
    "MODEL_PATH= \"../models/RoBERTa_small_classif\"\n",
    "MAX_EPOCH= 5\n",
    "MAX_SENTENCES= 8 # batch size\n",
    "MAX_UPDATE= 50 # number of backward propagation steps\n",
    "LR= 1.E-5\n",
    "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = 'accuracy' # use the accuracy metric\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "SEEDS=3\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "WARMUP = int(0.06 * MAX_UPDATE) # warmup ratio=6% of the whole training\n",
    "ARCHITECTURE = \"roberta_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 15:07:04 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 50, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/0', 'restore_file': '../models/RoBERTa_small_classif', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=50, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/0', restore_file='../models/RoBERTa_small_classif', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=3, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='50', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 3, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 15:07:05 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 15:07:05 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 15:07:05 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 15:07:05 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 15:07:05 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 15:07:05 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 15:07:05 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:07:05 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:07:05 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 15:07:05 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 15:07:05 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 15:07:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 15:07:06 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 15:07:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:  22%|▏| 49/225 [00:34<02:00,  1.46it/s, loss=1.041, nll_loss=0.008, a2023-11-11 15:07:41 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50 >= max_update: 50\n",
      "2023-11-11 15:07:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|▎      | 1/25 [00:00<00:08,  2.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 2/25 [00:00<00:05,  3.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  12%|▊      | 3/25 [00:00<00:05,  4.01it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|█      | 4/25 [00:00<00:04,  4.46it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  20%|█▍     | 5/25 [00:01<00:04,  4.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▋     | 6/25 [00:01<00:04,  4.49it/s]\u001b[A^C\n",
      "\n",
      "Traceback (most recent call last):                                              \u001b[A\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 564, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/home/user/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 190, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 330, in train\n",
      "    valid_losses, should_stop = validate_and_save(\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 421, in validate_and_save\n",
      "    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 505, in validate\n",
      "    trainer.valid_step(sample)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq/trainer.py\", line 1121, in valid_step\n",
      "    _loss, sample_size, logging_output = self.task.valid_step(\n",
      "  File \"/home/user/fairseq/fairseq/tasks/fairseq_task.py\", line 527, in valid_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/criterions/sentence_prediction.py\", line 48, in forward\n",
      "    logits, _ = model(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 255, in forward\n",
      "    x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 601, in forward\n",
      "    x, extra = self.extract_features(\n",
      "  File \"/home/user/fairseq/fairseq/models/roberta/model.py\", line 609, in extract_features\n",
      "    encoder_out = self.sentence_encoder(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/models/transformer/transformer_encoder.py\", line 165, in forward\n",
      "    return self.forward_scriptable(\n",
      "  File \"/home/user/fairseq/fairseq/models/transformer/transformer_encoder.py\", line 230, in forward_scriptable\n",
      "    lr = layer(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/modules/transformer_layer.py\", line 197, in forward\n",
      "    x, _ = self.self_attn(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/modules/multihead_attention.py\", line 539, in forward\n",
      "    return F.multi_head_attention_forward(\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/nn/functional.py\", line 5443, in multi_head_attention_forward\n",
      "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "KeyboardInterrupt\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 15:07:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 50, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/1', 'restore_file': '../models/RoBERTa_small_classif', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=50, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/1', restore_file='../models/RoBERTa_small_classif', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=3, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='50', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 3, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 15:07:48 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 15:07:48 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 15:07:49 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 15:07:49 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 15:07:49 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 15:07:49 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:07:49 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:07:49 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 15:07:49 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 15:07:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 15:07:49 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 15:07:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   4%| | 10/225 [00:06<02:28,  1.45it/s, loss=1.006, nll_loss=0.007, a^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 564, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/home/user/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 190, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq_cli/train.py\", line 316, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/user/fairseq/fairseq/trainer.py\", line 951, in train_step\n",
      "    self.task.optimizer_step(\n",
      "  File \"/home/user/fairseq/fairseq/tasks/fairseq_task.py\", line 531, in optimizer_step\n",
      "    optimizer.step()\n",
      "  File \"/home/user/fairseq/fairseq/optim/fairseq_optimizer.py\", line 127, in step\n",
      "    self.optimizer.step(closure)\n",
      "  File \"/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/user/fairseq/fairseq/optim/adam.py\", line 216, in step\n",
      "    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
      "KeyboardInterrupt\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 15:08:02 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 50, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/2', 'restore_file': '../models/RoBERTa_small_classif', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=50, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu50_lr1e-05_me5/2', restore_file='../models/RoBERTa_small_classif', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='../data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=3, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='50', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '../data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 3, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-11-11 15:08:02 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
      "2023-11-11 15:08:02 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | RobertaModel(\n",
      "  (encoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerEncoder(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayerBase(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (activation_dropout_module): FairseqDropout()\n",
      "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict(\n",
      "    (sentence_classification_head): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | model: RobertaModel\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/input0/valid\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 200 examples from: ../data/cls-books-bin/label/valid\n",
      "2023-11-11 15:08:03 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/input0/test\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: ../data/cls-books-bin/label/test\n",
      "2023-11-11 15:08:03 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
      "2023-11-11 15:08:03 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
      "2023-11-11 15:08:03 | INFO | fairseq.trainer | Preparing to load checkpoint ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:08:03 | INFO | fairseq.trainer | No existing checkpoint found ../models/RoBERTa_small_classif\n",
      "2023-11-11 15:08:03 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/input0/train\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: ../data/cls-books-bin/label/train\n",
      "2023-11-11 15:08:03 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
      "2023-11-11 15:08:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
      "epoch 001:   0%|                                        | 0/225 [00:00<?, ?it/s]2023-11-11 15:08:03 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-11-11 15:08:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   9%| | 20/225 [00:14<02:35,  1.32it/s, loss=1, nll_loss=0.007, accurTerminated\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python ~/fairseq/fairseq_cli/train.py \\\n",
    "                $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch $ARCHITECTURE \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
