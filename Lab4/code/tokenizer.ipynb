{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def abspath(pth): return Path(pth).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## Task 2: Preprocessing the dataset\n",
    "Prepare the data for finetuning. To do so you have to:\n",
    "1. Use the provided sentencepiece model to tokenize the text.\n",
    "2. Binarize the data using `fairseq-preprocess` command\n",
    "\n",
    "### Tokenizing the reviews\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize \n",
    "- the input three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> \n",
    "- and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
    "\n",
    "Documentation: https://github.com/google/sentencepiece#readme\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Tokenization\n",
    "When we split the sentences into tokens here, we use a provided byte pair encoding model.\n",
    "\n",
    "When training from scratch, we should start from scratch and tokenize a corpus from scratch...\n",
    "\n",
    "but if we want to **re-use a pretrained model**, we need to follow its vocabulary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS=['train', 'test', 'valid']\n",
    "TOKENIZER_MODEL_PATH = '../models/RoBERTa_small_fr/sentencepiece.bpe.model'\n",
    "DATA_BOOKS = Path('../data/cls.books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ce livre est tout simplement magique !  il vaut le detour suspense, humour, magi, \t Tokenized: ▁Ce ▁livre ▁est ▁tout ▁simplement ▁mag ique ▁! ▁il ▁vaut ▁le ▁de tour ▁suspens e\n",
      "Original: J'ai lu ce livre car dans ma ville, tout le monde s'en sert et le commande. C'es, \t Tokenized: ▁J ' ai ▁lu ▁ce ▁livre ▁car ▁dans ▁ma ▁ville , ▁tout ▁le ▁monde ▁s ' en ▁sert ▁e\n",
      "Original: Ce livre explique techniquement et de façon très compréhensible, même pour des n, \t Tokenized: ▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.1\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)\n",
    "\n",
    "SENTS=\"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(DATA_BOOKS/(split+'.'+SENTS), 'r') as f:\n",
    "        reviews = f.readlines()\n",
    "        \n",
    "        tokenized = [\" \".join(s.encode(review, out_type=str)) for review in reviews]\n",
    "        print(f\"Original: {reviews[0][:80]}, \\t Tokenized: {tokenized[0][:80]}\")\n",
    "        # tokenize the data using s.encode and a loop(check the documentation)\n",
    "\n",
    "        # It should look something like this :\n",
    "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "    \n",
    "    with open(DATA_BOOKS/(split+'.spm' + \".\" + SENTS), 'w') as f:\n",
    "        f.writelines(\"\\n\".join(tokenized)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. \n",
    "> Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>.\n",
    ">\n",
    "> You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Binarization\n",
    "\n",
    "- `--only-source` : Only process the source language\n",
    "- `srcdict` allows forcing the tokenization dictionary.\n",
    "- `-s \"spm.review\"` for source language , `-t \"\"` for an undefined target language.\n",
    "\n",
    "Note on compression\n",
    "> - train.review `1102kb`\n",
    "> - train.spm `1741kb`  (added extra characters to get ready for binarization)\n",
    "> - train.bin : `569kb` (file appears compressed, sentences read from disk will be lighter and decoded on the CPU).\n",
    "\n",
    "\n",
    "In the context of `fairseq` ,  `.bin` and `.idx` files are integral to the data format used for efficient storage and retrieval of the training data. \n",
    "\n",
    "Here's what each of these file types represents:\n",
    "\n",
    "1. **`.bin` Files**: \n",
    "   - These are binary files that contain the actual training data.\n",
    "   - In `fairseq`, data (such as tokenized text) is converted into a numerical format (indices corresponding to tokens in the dictionary) and then stored in a binary format.\n",
    "   - This binary format is more space-efficient and faster to **read from disk compared to plain text**, which is crucial for large datasets commonly used in machine learning.\n",
    "\n",
    "2. **`.idx` Files**: \n",
    "   - These files are index files that accompany the `.bin` files.\n",
    "   - The `.idx` file stores the byte offsets of each example (like a sentence or a document) in the corresponding `.bin` file.\n",
    "   - When the training process requires a specific example from the dataset, it uses the `.idx` file to quickly find where that example starts and ends in the `.bin` file.\n",
    "   - This allows for efficient random access to examples in the dataset without having to read the entire `.bin` file sequentially, which is especially beneficial for large datasets.\n",
    "\n",
    "In summary, the combination of `.bin` and `.idx` files in `fairseq` enables efficient and fast access to large-scale datasets, a critical aspect of training modern neural networks, particularly in tasks like natural language processing where datasets can be extremely large.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:19 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.spm.review', validpref='../data/cls.books/valid.spm.review', testpref='../data/cls.books/test.spm.review', align_suffix=None, destdir='../data/cls-books-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:20 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/input0\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.label', validpref='../data/cls.books/valid.label', testpref='../data/cls.books/test.label', align_suffix=None, destdir='../data/cls-books-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/label\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.2\n",
    "SRC_DICT = \"../models/RoBERTa_small_fr/dict.txt\"\n",
    "DESTINATION_ROOT = Path(\"../data/cls-books-bin\")\n",
    "\n",
    "# (file suffix, folder output, dictionary to use)\n",
    "CONFIG = [\n",
    "    (\".spm.review\", \"input0\", f\"--srcdict {SRC_DICT}\"), # binarize the tokenized reviews\n",
    "    (\".label\", \"label\", \"\"), # binarize the labels - fairseq preprocess will build the dictionary needed (0 & 1 basically)\n",
    "]\n",
    "for suffix, out_dir, src_dict in CONFIG:\n",
    "    CORPUS_TRAIN, CORPUS_VALID, CORPUS_TEST = [str(DATA_BOOKS/f\"{dataset_split}{suffix}\") for dataset_split in ['train', 'valid', 'test']]\n",
    "    DESTINATION_FOLDER = str(DESTINATION_ROOT/out_dir)\n",
    "    !(python ~/fairseq/fairseq_cli/preprocess.py \\\n",
    "                --only-source \\\n",
    "                --workers 8 \\\n",
    "                $src_dict \\\n",
    "                --destdir \"$DESTINATION_FOLDER\"\\\n",
    "                --trainpref \"$CORPUS_TRAIN\" \\\n",
    "                --validpref \"$CORPUS_VALID\" \\\n",
    "                --testpref \"$CORPUS_TEST\" \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
    "\n",
    "In this section you will use `fairseq/fairseq_cli/train.py` python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
    "\n",
    "Make sure to use the following hyper-parameters: \n",
    "- batch size=8\n",
    "- max number of epochs = 5\n",
    "- optimizer: Adam\n",
    "- max learning rate: 1e-05,\n",
    "- warm up ratio: 0.06, \n",
    "- learning rate scheduler: linear\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET='books'\n",
    "TASK= 'sentence_prediction' # sentence prediction task on fairseq\n",
    "MODEL='RoBERTa_small_fr'\n",
    "DATA_PATH= \"../data/cls-books-bin\"\n",
    "MODEL_PATH= \"../models/RoBERTa_small_classif\"\n",
    "MAX_EPOCH= 5\n",
    "MAX_SENTENCES= 8 # batch size\n",
    "MAX_UPDATE= 50 # number of backward propagation steps\n",
    "LR= 1.E-5\n",
    "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = 'accuracy' # use the accuracy metric\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "SEEDS=3\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "WARMUP = 0.06 * MAX_UPDATE # warmup ratio=6% of the whole training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "usage: train.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]\n",
      "                [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]\n",
      "                [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16]\n",
      "                [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                [--amp-init-scale AMP_INIT_SCALE]\n",
      "                [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]\n",
      "                [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                [--tokenizer {moses,nltk,space}]\n",
      "                [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                [--skip-invalid-size-inputs-valid-test]\n",
      "                [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET]\n",
      "                [--combine-valid-subsets] [--ignore-unused-valid-subsets]\n",
      "                [--validate-interval VALIDATE_INTERVAL]\n",
      "                [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM]\n",
      "                [--gen-subset GEN_SUBSET] [--num-shards NUM_SHARDS]\n",
      "                [--shard-id SHARD_ID] [--grouped-shuffling]\n",
      "                [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                [--update-ordered-indices-seed]\n",
      "                [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                [--distributed-rank DISTRIBUTED_RANK]\n",
      "                [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\n",
      "                [--distributed-no-spawn]\n",
      "                [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB]\n",
      "                [--fix-batches-to-gpus] [--find-unused-parameters]\n",
      "                [--gradient-as-bucket-view] [--fast-stat-sync]\n",
      "                [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers]\n",
      "                [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                [--pipeline-model-parallel]\n",
      "                [--pipeline-balance PIPELINE_BALANCE]\n",
      "                [--pipeline-devices PIPELINE_DEVICES]\n",
      "                [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                [--pipeline-checkpoint {always,never,except_last}]\n",
      "                [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]\n",
      "                [--not-fsdp-flatten-parameters] [--arch ARCH]\n",
      "                [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]\n",
      "                [--stop-time-hours STOP_TIME_HOURS] [--clip-norm CLIP_NORM]\n",
      "                [--sentence-avg] [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                [--restore-file RESTORE_FILE] [--continue-once CONTINUE_ONCE]\n",
      "                [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]\n",
      "                [--reset-optimizer]\n",
      "                [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                [--save-interval SAVE_INTERVAL]\n",
      "                [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]\n",
      "                [--no-epoch-checkpoints] [--no-last-checkpoints]\n",
      "                [--no-save-optimizer-state]\n",
      "                [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                [--load-checkpoint-on-all-dp-ranks]\n",
      "                [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                [--ema-decay EMA_DECAY] [--ema-start-update EMA_START_UPDATE]\n",
      "                [--ema-seed-model EMA_SEED_MODEL]\n",
      "                [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "                [--encoder-layers L] [--encoder-embed-dim H]\n",
      "                [--encoder-ffn-embed-dim F] [--encoder-attention-heads A]\n",
      "                [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--pooler-activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--encoder-normalize-before] [--layernorm-embedding]\n",
      "                [--dropout D] [--attention-dropout D] [--activation-dropout D]\n",
      "                [--pooler-dropout D] [--max-positions MAX_POSITIONS]\n",
      "                [--load-checkpoint-heads] [--untie-weights-roberta]\n",
      "                [--encoder-layerdrop D]\n",
      "                [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]\n",
      "                [--quant-noise-pq D] [--quant-noise-pq-block-size D]\n",
      "                [--quant-noise-scalar D] [--spectral-norm-classification-head]\n",
      "                [--min-params-to-wrap D] [--mha-reg-scale-factor D]\n",
      "                [--ffn-reg-scale-factor D] [--mha-heads-to-keep D]\n",
      "                [--ffn-blocks-to-remove D] [--num-classes NUM_CLASSES]\n",
      "                [--init-token INIT_TOKEN] [--separator-token SEPARATOR_TOKEN]\n",
      "                [--no-shuffle] [--shorten-method {none,truncate,random_crop}]\n",
      "                [--shorten-data-split-list SHORTEN_DATA_SPLIT_LIST]\n",
      "                [--add-prev-output-tokens]\n",
      "                [--classification-head-name CLASSIFICATION_HEAD_NAME]\n",
      "                [--regression-target] [--adam-betas ADAM_BETAS]\n",
      "                [--adam-eps ADAM_EPS] [--weight-decay WEIGHT_DECAY]\n",
      "                [--use-old-adam] [--fp16-adam-stats]\n",
      "                [--warmup-updates WARMUP_UPDATES]\n",
      "                [--force-anneal FORCE_ANNEAL]\n",
      "                [--end-learning-rate END_LEARNING_RATE] [--power POWER]\n",
      "                [--total-num-update TOTAL_NUM_UPDATE] [--pad PAD] [--eos EOS]\n",
      "                [--unk UNK]\n",
      "                data\n",
      "train.py: error: argument --warmup-updates: invalid int value: '0.06'\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "usage: train.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]\n",
      "                [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]\n",
      "                [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16]\n",
      "                [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                [--amp-init-scale AMP_INIT_SCALE]\n",
      "                [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]\n",
      "                [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                [--tokenizer {moses,nltk,space}]\n",
      "                [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                [--skip-invalid-size-inputs-valid-test]\n",
      "                [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET]\n",
      "                [--combine-valid-subsets] [--ignore-unused-valid-subsets]\n",
      "                [--validate-interval VALIDATE_INTERVAL]\n",
      "                [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM]\n",
      "                [--gen-subset GEN_SUBSET] [--num-shards NUM_SHARDS]\n",
      "                [--shard-id SHARD_ID] [--grouped-shuffling]\n",
      "                [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                [--update-ordered-indices-seed]\n",
      "                [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                [--distributed-rank DISTRIBUTED_RANK]\n",
      "                [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\n",
      "                [--distributed-no-spawn]\n",
      "                [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB]\n",
      "                [--fix-batches-to-gpus] [--find-unused-parameters]\n",
      "                [--gradient-as-bucket-view] [--fast-stat-sync]\n",
      "                [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers]\n",
      "                [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                [--pipeline-model-parallel]\n",
      "                [--pipeline-balance PIPELINE_BALANCE]\n",
      "                [--pipeline-devices PIPELINE_DEVICES]\n",
      "                [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                [--pipeline-checkpoint {always,never,except_last}]\n",
      "                [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]\n",
      "                [--not-fsdp-flatten-parameters] [--arch ARCH]\n",
      "                [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]\n",
      "                [--stop-time-hours STOP_TIME_HOURS] [--clip-norm CLIP_NORM]\n",
      "                [--sentence-avg] [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                [--restore-file RESTORE_FILE] [--continue-once CONTINUE_ONCE]\n",
      "                [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]\n",
      "                [--reset-optimizer]\n",
      "                [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                [--save-interval SAVE_INTERVAL]\n",
      "                [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]\n",
      "                [--no-epoch-checkpoints] [--no-last-checkpoints]\n",
      "                [--no-save-optimizer-state]\n",
      "                [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                [--load-checkpoint-on-all-dp-ranks]\n",
      "                [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                [--ema-decay EMA_DECAY] [--ema-start-update EMA_START_UPDATE]\n",
      "                [--ema-seed-model EMA_SEED_MODEL]\n",
      "                [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "                [--encoder-layers L] [--encoder-embed-dim H]\n",
      "                [--encoder-ffn-embed-dim F] [--encoder-attention-heads A]\n",
      "                [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--pooler-activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--encoder-normalize-before] [--layernorm-embedding]\n",
      "                [--dropout D] [--attention-dropout D] [--activation-dropout D]\n",
      "                [--pooler-dropout D] [--max-positions MAX_POSITIONS]\n",
      "                [--load-checkpoint-heads] [--untie-weights-roberta]\n",
      "                [--encoder-layerdrop D]\n",
      "                [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]\n",
      "                [--quant-noise-pq D] [--quant-noise-pq-block-size D]\n",
      "                [--quant-noise-scalar D] [--spectral-norm-classification-head]\n",
      "                [--min-params-to-wrap D] [--mha-reg-scale-factor D]\n",
      "                [--ffn-reg-scale-factor D] [--mha-heads-to-keep D]\n",
      "                [--ffn-blocks-to-remove D] [--num-classes NUM_CLASSES]\n",
      "                [--init-token INIT_TOKEN] [--separator-token SEPARATOR_TOKEN]\n",
      "                [--no-shuffle] [--shorten-method {none,truncate,random_crop}]\n",
      "                [--shorten-data-split-list SHORTEN_DATA_SPLIT_LIST]\n",
      "                [--add-prev-output-tokens]\n",
      "                [--classification-head-name CLASSIFICATION_HEAD_NAME]\n",
      "                [--regression-target] [--adam-betas ADAM_BETAS]\n",
      "                [--adam-eps ADAM_EPS] [--weight-decay WEIGHT_DECAY]\n",
      "                [--use-old-adam] [--fp16-adam-stats]\n",
      "                [--warmup-updates WARMUP_UPDATES]\n",
      "                [--force-anneal FORCE_ANNEAL]\n",
      "                [--end-learning-rate END_LEARNING_RATE] [--power POWER]\n",
      "                [--total-num-update TOTAL_NUM_UPDATE] [--pad PAD] [--eos EOS]\n",
      "                [--unk UNK]\n",
      "                data\n",
      "train.py: error: argument --warmup-updates: invalid int value: '0.06'\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "usage: train.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]\n",
      "                [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]\n",
      "                [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16]\n",
      "                [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                [--amp-init-scale AMP_INIT_SCALE]\n",
      "                [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]\n",
      "                [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                [--tokenizer {moses,nltk,space}]\n",
      "                [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                [--skip-invalid-size-inputs-valid-test]\n",
      "                [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET]\n",
      "                [--combine-valid-subsets] [--ignore-unused-valid-subsets]\n",
      "                [--validate-interval VALIDATE_INTERVAL]\n",
      "                [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM]\n",
      "                [--gen-subset GEN_SUBSET] [--num-shards NUM_SHARDS]\n",
      "                [--shard-id SHARD_ID] [--grouped-shuffling]\n",
      "                [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                [--update-ordered-indices-seed]\n",
      "                [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                [--distributed-rank DISTRIBUTED_RANK]\n",
      "                [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\n",
      "                [--distributed-no-spawn]\n",
      "                [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB]\n",
      "                [--fix-batches-to-gpus] [--find-unused-parameters]\n",
      "                [--gradient-as-bucket-view] [--fast-stat-sync]\n",
      "                [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers]\n",
      "                [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                [--pipeline-model-parallel]\n",
      "                [--pipeline-balance PIPELINE_BALANCE]\n",
      "                [--pipeline-devices PIPELINE_DEVICES]\n",
      "                [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                [--pipeline-checkpoint {always,never,except_last}]\n",
      "                [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]\n",
      "                [--not-fsdp-flatten-parameters] [--arch ARCH]\n",
      "                [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]\n",
      "                [--stop-time-hours STOP_TIME_HOURS] [--clip-norm CLIP_NORM]\n",
      "                [--sentence-avg] [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                [--restore-file RESTORE_FILE] [--continue-once CONTINUE_ONCE]\n",
      "                [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]\n",
      "                [--reset-optimizer]\n",
      "                [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                [--save-interval SAVE_INTERVAL]\n",
      "                [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]\n",
      "                [--no-epoch-checkpoints] [--no-last-checkpoints]\n",
      "                [--no-save-optimizer-state]\n",
      "                [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                [--load-checkpoint-on-all-dp-ranks]\n",
      "                [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                [--ema-decay EMA_DECAY] [--ema-start-update EMA_START_UPDATE]\n",
      "                [--ema-seed-model EMA_SEED_MODEL]\n",
      "                [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "                [--encoder-layers L] [--encoder-embed-dim H]\n",
      "                [--encoder-ffn-embed-dim F] [--encoder-attention-heads A]\n",
      "                [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--pooler-activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                [--encoder-normalize-before] [--layernorm-embedding]\n",
      "                [--dropout D] [--attention-dropout D] [--activation-dropout D]\n",
      "                [--pooler-dropout D] [--max-positions MAX_POSITIONS]\n",
      "                [--load-checkpoint-heads] [--untie-weights-roberta]\n",
      "                [--encoder-layerdrop D]\n",
      "                [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]\n",
      "                [--quant-noise-pq D] [--quant-noise-pq-block-size D]\n",
      "                [--quant-noise-scalar D] [--spectral-norm-classification-head]\n",
      "                [--min-params-to-wrap D] [--mha-reg-scale-factor D]\n",
      "                [--ffn-reg-scale-factor D] [--mha-heads-to-keep D]\n",
      "                [--ffn-blocks-to-remove D] [--num-classes NUM_CLASSES]\n",
      "                [--init-token INIT_TOKEN] [--separator-token SEPARATOR_TOKEN]\n",
      "                [--no-shuffle] [--shorten-method {none,truncate,random_crop}]\n",
      "                [--shorten-data-split-list SHORTEN_DATA_SPLIT_LIST]\n",
      "                [--add-prev-output-tokens]\n",
      "                [--classification-head-name CLASSIFICATION_HEAD_NAME]\n",
      "                [--regression-target] [--adam-betas ADAM_BETAS]\n",
      "                [--adam-eps ADAM_EPS] [--weight-decay WEIGHT_DECAY]\n",
      "                [--use-old-adam] [--fp16-adam-stats]\n",
      "                [--warmup-updates WARMUP_UPDATES]\n",
      "                [--force-anneal FORCE_ANNEAL]\n",
      "                [--end-learning-rate END_LEARNING_RATE] [--power POWER]\n",
      "                [--total-num-update TOTAL_NUM_UPDATE] [--pad PAD] [--eos EOS]\n",
      "                [--unk UNK]\n",
      "                data\n",
      "train.py: error: argument --warmup-updates: invalid int value: '0.06'\n"
     ]
    }
   ],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python ~/fairseq/fairseq_cli/train.py \\\n",
    "                $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch roberta_small \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
