{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def abspath(pth): return Path(pth).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## Task 2: Preprocessing the dataset\n",
    "Prepare the data for finetuning. To do so you have to:\n",
    "1. Use the provided sentencepiece model to tokenize the text.\n",
    "2. Binarize the data using `fairseq-preprocess` command\n",
    "\n",
    "### Tokenizing the reviews\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize \n",
    "- the input three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> \n",
    "- and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
    "\n",
    "Documentation: https://github.com/google/sentencepiece#readme\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Tokenization\n",
    "When we split the sentences into tokens here, we use a provided byte pair encoding model.\n",
    "\n",
    "When training from scratch, we should start from scratch and tokenize a corpus from scratch...\n",
    "\n",
    "but if we want to **re-use a pretrained model**, we need to follow its vocabulary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS=['train', 'test', 'valid']\n",
    "TOKENIZER_MODEL_PATH = '../models/RoBERTa_small_fr/sentencepiece.bpe.model'\n",
    "DATA_BOOKS = Path('../data/cls.books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ce livre est tout simplement magique !  il vaut le detour suspense, humour, magi, \t Tokenized: ▁Ce ▁livre ▁est ▁tout ▁simplement ▁mag ique ▁! ▁il ▁vaut ▁le ▁de tour ▁suspens e\n",
      "Original: J'ai lu ce livre car dans ma ville, tout le monde s'en sert et le commande. C'es, \t Tokenized: ▁J ' ai ▁lu ▁ce ▁livre ▁car ▁dans ▁ma ▁ville , ▁tout ▁le ▁monde ▁s ' en ▁sert ▁e\n",
      "Original: Ce livre explique techniquement et de façon très compréhensible, même pour des n, \t Tokenized: ▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.1\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)\n",
    "\n",
    "SENTS=\"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(DATA_BOOKS/(split+'.'+SENTS), 'r') as f:\n",
    "        reviews = f.readlines()\n",
    "        \n",
    "        tokenized = [\" \".join(s.encode(review, out_type=str)) for review in reviews]\n",
    "        print(f\"Original: {reviews[0][:80]}, \\t Tokenized: {tokenized[0][:80]}\")\n",
    "        # tokenize the data using s.encode and a loop(check the documentation)\n",
    "\n",
    "        # It should look something like this :\n",
    "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "    \n",
    "    with open(DATA_BOOKS/(split+'.spm' + \".\" + SENTS), 'w') as f:\n",
    "        f.writelines(\"\\n\".join(tokenized)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. \n",
    "> Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>.\n",
    ">\n",
    "> You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Binarization\n",
    "\n",
    "- `--only-source` : Only process the source language\n",
    "- `srcdict` allows forcing the tokenization dictionary.\n",
    "- `-s \"spm.review\"` for source language , `-t \"\"` for an undefined target language.\n",
    "\n",
    "Note on compression\n",
    "> - train.review `1102kb`\n",
    "> - train.spm `1741kb`  (added extra characters to get ready for binarization)\n",
    "> - train.bin : `569kb` (file appears compressed, sentences read from disk will be lighter and decoded on the CPU).\n",
    "\n",
    "\n",
    "In the context of `fairseq` ,  `.bin` and `.idx` files are integral to the data format used for efficient storage and retrieval of the training data. \n",
    "\n",
    "Here's what each of these file types represents:\n",
    "\n",
    "1. **`.bin` Files**: \n",
    "   - These are binary files that contain the actual training data.\n",
    "   - In `fairseq`, data (such as tokenized text) is converted into a numerical format (indices corresponding to tokens in the dictionary) and then stored in a binary format.\n",
    "   - This binary format is more space-efficient and faster to **read from disk compared to plain text**, which is crucial for large datasets commonly used in machine learning.\n",
    "\n",
    "2. **`.idx` Files**: \n",
    "   - These files are index files that accompany the `.bin` files.\n",
    "   - The `.idx` file stores the byte offsets of each example (like a sentence or a document) in the corresponding `.bin` file.\n",
    "   - When the training process requires a specific example from the dataset, it uses the `.idx` file to quickly find where that example starts and ends in the `.bin` file.\n",
    "   - This allows for efficient random access to examples in the dataset without having to read the entire `.bin` file sequentially, which is especially beneficial for large datasets.\n",
    "\n",
    "In summary, the combination of `.bin` and `.idx` files in `fairseq` enables efficient and fast access to large-scale datasets, a critical aspect of training modern neural networks, particularly in tasks like natural language processing where datasets can be extremely large.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:19 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.spm.review', validpref='../data/cls.books/valid.spm.review', testpref='../data/cls.books/test.spm.review', align_suffix=None, destdir='../data/cls-books-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:20 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
      "2023-11-11 14:31:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
      "2023-11-11 14:31:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/input0\n",
      "/home/user/mambaforge/envs/llm/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='../data/cls.books/train.label', validpref='../data/cls.books/valid.label', testpref='../data/cls.books/test.label', align_suffix=None, destdir='../data/cls-books-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | [None] ../data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
      "2023-11-11 14:31:26 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/cls-books-bin/label\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.2\n",
    "SRC_DICT = \"../models/RoBERTa_small_fr/dict.txt\"\n",
    "DESTINATION_ROOT = Path(\"../data/cls-books-bin\")\n",
    "\n",
    "# (file suffix, folder output, dictionary to use)\n",
    "CONFIG = [\n",
    "    (\".spm.review\", \"input0\", f\"--srcdict {SRC_DICT}\"), # binarize the tokenized reviews\n",
    "    (\".label\", \"label\", \"\"), # binarize the labels - fairseq preprocess will build the dictionary needed (0 & 1 basically)\n",
    "]\n",
    "for suffix, out_dir, src_dict in CONFIG:\n",
    "    CORPUS_TRAIN, CORPUS_VALID, CORPUS_TEST = [str(DATA_BOOKS/f\"{dataset_split}{suffix}\") for dataset_split in ['train', 'valid', 'test']]\n",
    "    DESTINATION_FOLDER = str(DESTINATION_ROOT/out_dir)\n",
    "    !(python ~/fairseq/fairseq_cli/preprocess.py \\\n",
    "                --only-source \\\n",
    "                --workers 8 \\\n",
    "                $src_dict \\\n",
    "                --destdir \"$DESTINATION_FOLDER\"\\\n",
    "                --trainpref \"$CORPUS_TRAIN\" \\\n",
    "                --validpref \"$CORPUS_VALID\" \\\n",
    "                --testpref \"$CORPUS_TEST\" \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
    "\n",
    "In this section you will use `fairseq/fairseq_cli/train.py` python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
    "\n",
    "Make sure to use the following hyper-parameters: \n",
    "- batch size=8\n",
    "- max number of epochs = 5\n",
    "- optimizer: Adam\n",
    "- max learning rate: 1e-05,\n",
    "- warm up ratio: 0.06, \n",
    "- learning rate scheduler: linear\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET='books'\n",
    "TASK= 'sentence_prediction' # sentence prediction task on fairseq\n",
    "MODEL='RoBERTa_small_fr'\n",
    "DATA_PATH= \"../data/cls-books-bin\"\n",
    "MODEL_PATH= \"../models/RoBERTa_small_classif\"\n",
    "MAX_EPOCH= 5\n",
    "MAX_SENTENCES= 8 # batch size\n",
    "MAX_UPDATE = 1200 # number of backward propagation steps\n",
    "LR= 1.E-5\n",
    "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
    "METRIC = 'accuracy' # use the accuracy metric\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "SEEDS=3\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "WARMUP = int(0.06 * MAX_UPDATE) # warmup ratio=6% of the whole training\n",
    "ARCHITECTURE = \"roberta_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in range(SEEDS):\n",
    "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
    "  !(python ~/fairseq/fairseq_cli/train.py \\\n",
    "                $DATA_PATH \\\n",
    "                --restore-file $MODEL_PATH \\\n",
    "                --batch-size $MAX_SENTENCES \\\n",
    "                --task $TASK \\\n",
    "                --update-freq 1 \\\n",
    "                --seed $SEED \\\n",
    "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                --init-token 0 \\\n",
    "                --separator-token 2 \\\n",
    "                --arch $ARCHITECTURE \\\n",
    "                --criterion sentence_prediction \\\n",
    "                --num-classes $NUM_CLASSES \\\n",
    "                --weight-decay 0.01 \\\n",
    "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                --maximize-best-checkpoint-metric \\\n",
    "                --best-checkpoint-metric $METRIC \\\n",
    "                --save-dir $SAVE_DIR \\\n",
    "                --lr-scheduler polynomial_decay \\\n",
    "                --lr $LR \\\n",
    "                --max-update $MAX_UPDATE \\\n",
    "                --total-num-update $MAX_UPDATE \\\n",
    "                --no-epoch-checkpoints \\\n",
    "                --no-last-checkpoints \\\n",
    "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
    "                --log-interval 5 \\\n",
    "                --warmup-updates $WARMUP \\\n",
    "                --max-epoch $MAX_EPOCH \\\n",
    "                --keep-best-checkpoints 1 \\\n",
    "                --max-positions 256 \\\n",
    "                --valid-subset $VALID_SUBSET \\\n",
    "                --shorten-method 'truncate' \\\n",
    "                --no-save \\\n",
    "                --distributed-world-size 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
