{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Balthazar Neveu\n",
    "\n",
    "</center>\n",
    "<font color='gray'>\n",
    "\n",
    "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def abspath(pth): return Path(pth).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## Task 2: Preprocessing the dataset\n",
    "Prepare the data for finetuning. To do so you have to:\n",
    "1. Use the provided sentencepiece model to tokenize the text.\n",
    "2. Binarize the data using `fairseq-preprocess` command\n",
    "\n",
    "### Tokenizing the reviews\n",
    "\n",
    "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
    "\n",
    "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize \n",
    "- the input three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> \n",
    "- and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
    "\n",
    "Documentation: https://github.com/google/sentencepiece#readme\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Tokenization\n",
    "When we split the sentences into tokens here, we use a provided byte pair encoding model.\n",
    "\n",
    "When training from scratch, we should start from scratch and tokenize a corpus from scratch...\n",
    "\n",
    "but if we want to **re-use a pretrained model**, we need to follow its vocabulary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS=['train', 'test', 'valid']\n",
    "TOKENIZER_MODEL_PATH = '../models/RoBERTa_small_fr/sentencepiece.bpe.model'\n",
    "DATA_BOOKS = '../data/cls.books/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)\n",
    "\n",
    "SENTS=\"review\"\n",
    "\n",
    "for split in SPLITS:\n",
    "    with open(DATA_BOOKS+split+'.'+SENTS, 'r') as f:\n",
    "        reviews = f.readlines()\n",
    "        \n",
    "        tokenized = [\" \".join(s.encode(review, out_type=str)) for review in reviews]\n",
    "        print(f\"Original: {reviews[0][:80]}, \\t Tokenized: {tokenized[0][:80]}\")\n",
    "        # tokenize the data using s.encode and a loop(check the documentation)\n",
    "\n",
    "        # It should look something like this :\n",
    "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
    "    \n",
    "    with open(DATA_BOOKS+split+'.spm' + \".\" + SENTS, 'w') as f:\n",
    "        f.writelines(\"\\n\".join(tokenized)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "\n",
    "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
    "\n",
    "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
    "\n",
    "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. \n",
    "> Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>.\n",
    ">\n",
    "> You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
    "\n",
    "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
    "\n",
    "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 Binarization\n",
    "\n",
    "- `--only-source` : Only process the source language\n",
    "- `srcdict` allows forcing the tokenization dictionary.\n",
    "- `-s \"spm.review\"` for source language , `-t \"\"` for an undefined target language.\n",
    "\n",
    "Note on compression\n",
    "> - train.review `1102kb`\n",
    "> - train.spm `1741kb`  (added extra characters to get ready for binarization)\n",
    "> - binarized : `569kb` (file appears compressed, sentences transfers to GPU memory will be lighter).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DICT = \"../models/RoBERTa_small_fr/dict.txt\"\n",
    "DESTINATION_FOLDER = \"../data/cls-books-bin/input0\"\n",
    "CORPUS_TRAIN, CORPUS_VALID, CORPUS_TEST = [f\"../data/cls.books/{dataset_split}\" for dataset_split in ['train', 'valid', 'test']]\n",
    "# binarize the tokenized reviews\n",
    "!(python ~/fairseq/fairseq_cli/preprocess.py \\\n",
    "            --only-source \\\n",
    "            --workers 8 \\\n",
    "            --srcdict \"$SRC_DICT\" \\\n",
    "            --destdir \"$DESTINATION_FOLDER\"\\\n",
    "            --trainpref \"$CORPUS_TRAIN\" \\\n",
    "            --validpref \"$CORPUS_VALID\" \\\n",
    "            --testpref \"$CORPUS_TEST\" \\\n",
    "            -s \"spm.review\" \\\n",
    "            -t \"\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
