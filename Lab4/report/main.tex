\documentclass[a4paper]{article}
\input{style/head.tex}
\newcommand{\yourname}{Balthazar Neveu}
\newcommand{\youremail}{balthazarneveu@gmail.com}
\newcommand{\assignmentnumber}{4}

\begin{document}

\input{style/header.tex}
\section{Fine tuning Roberta}
\subsection*{Question 1 \& Task 1: Model size}
I used the \textit{sympy} library to compute the number of parameters in the model analytically.
In the notebook we can compare that the number of parameters computed analytically is the same as the number of parameters computed using the \textit{torch.numel}..


When we check the RobertaSmall definition, we can read the following information, which follow the RoBerta paper convention.
\begin{verbatim}
ntokens = 32000 # -> V = vocabulary size
encoder_layers = nlayers = 4 # -> L = number of (attention+feedforward) transformer unit layers
encoder_embed_dim = 512 # -> D = embedding "feature" dimensions
encoder_ffn_embed_dim = 512 # -> D = feature dimensions used in the feed forward network
encoder_attention_heads = nhead = 8 # -> A = number of attentio heads
max_positions = 256 # -> T = max length of a sentence
\end{verbatim}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/roberta_params.png}
    \caption{Evaluating the number of parameters in the Roberta Small model, analytically and comparing to numerical values. \\ 
    Embedding layer is followed by $L=4$ Transformer units. Each Transformer unit is composed of a self-attention layer (in purple) and 2 feed forward layers (in orange)}
    \label{fig:roberta_params}
\end{figure}

\break 
\begin{tiny}
\begin{verbatim}

        +-------------------------------------------------------+------------+---------------------+------------------+
        |                        Modules                        | Parameters | Analytic validation | Analytic formula |
        +-------------------------------------------------------+------------+---------------------+------------------+
        |          sentence_encoder.embed_tokens.weight         |  16384000  |       16384000      |       D*V        |
        |        sentence_encoder.embed_positions.weight        |   132096   |        132096       |    D*(T + 2)     |
        |      sentence_encoder.layernorm_embedding.weight      |    512     |         512         |        D         |
        |       sentence_encoder.layernorm_embedding.bias       |    512     |         512         |        D         |
        +-------------------------------------------------------+------------+---------------------+------------------+
        |   sentence_encoder.layers.0.self_attn.k_proj.weight   |   262144   |        262144       |        D²        |
        |    sentence_encoder.layers.0.self_attn.k_proj.bias    |    512     |         512         |        D         |
        |   sentence_encoder.layers.0.self_attn.v_proj.weight   |   262144   |        262144       |        D²        |
        |    sentence_encoder.layers.0.self_attn.v_proj.bias    |    512     |         512         |        D         |
        |   sentence_encoder.layers.0.self_attn.q_proj.weight   |   262144   |        262144       |        D²        |
        |    sentence_encoder.layers.0.self_attn.q_proj.bias    |    512     |         512         |        D         |
        |  sentence_encoder.layers.0.self_attn.out_proj.weight  |   262144   |        262144       |        D²        |
        |   sentence_encoder.layers.0.self_attn.out_proj.bias   |    512     |         512         |        D         |
        | sentence_encoder.layers.0.self_attn_layer_norm.weight |    512     |         512         |        D         |
        |  sentence_encoder.layers.0.self_attn_layer_norm.bias  |    512     |         512         |        D         |
        |          sentence_encoder.layers.0.fc1.weight         |   262144   |        262144       |        D²        |
        |           sentence_encoder.layers.0.fc1.bias          |    512     |         512         |        D         |
        |          sentence_encoder.layers.0.fc2.weight         |   262144   |        262144       |        D²        |
        |           sentence_encoder.layers.0.fc2.bias          |    512     |         512         |        D         |
        |   sentence_encoder.layers.0.final_layer_norm.weight   |    512     |         512         |        D         |
        |    sentence_encoder.layers.0.final_layer_norm.bias    |    512     |         512         |        D         |
        +-------------------------------------------------------+------------+---------------------+------------------+
        |   sentence_encoder.layers.1.self_attn.k_proj.weight   |   262144   |        262144       |        D²        |
        |    sentence_encoder.layers.1.self_attn.k_proj.bias    |    512     |         512         |        D         |
        ...
        |   sentence_encoder.layers.3.final_layer_norm.weight   |    512     |         512         |        D         |
        |    sentence_encoder.layers.3.final_layer_norm.bias    |    512     |         512         |        D         |
        +-------------------------------------------------------+------------+---------------------+------------------+
\end{verbatim}
\end{tiny}

\subsubsection*{Analytic formula}
Total number of trainable parameters in the model (ommiting the language modeling head) is $22829056$ (22.8M):

$L=4$, $V=32000$, $D=512$, $A=8$, $T=256$

$$\textbf{\#Embedding} = V*D + D*(T+2) + (D + D)$$

$$L* \textbf{\#Transformer unit} = L*\big[ 3*A*D*(\frac{D}{A}) + 3*A*(\frac{D}{A}) + (D^2 +D) + (D + D) + 2*(D^2 + D) + (D + D)\big]$$


$$ \textbf{\#Roberta trainable parameters} = 24D^2 + DV + D(T+2)+ 42D$$

$$ \textbf{\#Roberta trainable parameters} = 24*512**2+512*32000+512*(256+2)+42*512 = 22829056$$

\subsubsection*{Note on positional embeddings}
In the original transformer paper, they used a fixed sinusoidal function to compute the positional embedding values, but in RoBERTa they are trainable parameters.
$D.(T+2)$ appears in the computation of the trainable positional embeddings. Thanks H. Abdine for clarifying that the +2 is for the start and end of sentence tokens.
We will consider maximum sentences of $T=256$ tokens + the start \& end of sentence tokens.


\subsubsection*{Remark on the number of attention heads}
Please note that due to the tricky implementation of torch,
$A$ does not appear in the final computation (chunk Key $K$, Query $Q$ and Value $V$ into $D$ chunks of  size $\frac{D}{A}$ ).
This allows keeping the same computation budget with various attention heads.
This explicitly appears in the \href{https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/activation.py#L990}{pytorch source code}
of MultiheadAttention
\begin{verbatim}
    self.head_dim = embed_dim // num_heads
\end{verbatim}

\subsection*{Task 2: Preprocessing}
\begin{itemize}
    \item Tokenize all sentences in the corpus using the provided vocabulary. This is mandatory.
    \item Binarize the whole dataset. The binarized file \textit{(/data/cls-books-bin/input0/train.bin)} weights 569.7kb \\
     and has clearly been compressed compared to the original training 1.741Mb file \textit{(data/cls.books/train.spm.review)}
    \item Binarize labels... even though these are 0, 1, we remain in the same system as the rest of the pipeline.
\end{itemize}



\subsection*{Task 3 \& 4: Training report \\ Accuracy: $63.5\pm0.8\%$ from scratch VS $80.6\pm1.9\%$ from pretrained}
Training is made of 5 epochs.

$250 \textit{(steps)}*8{(sentence per batch)}=2000$ sentences in the dataset are seen by the network at each epoch.

When we train from a pretrained model, we make sure that that we're able to reload the checkpoint weights
\begin{verbatim}
2023-11-11 17:01:15 | INFO | fairseq.trainer | Loaded checkpoint ../models/RoBERTa_small_fr/model.pt
\end{verbatim}

For task 4, we provide a fake path and make sure in the logs it's been ignored.
\begin{verbatim}
2023-11-13 11:50:53 | INFO | fairseq.trainer | No existing checkpoint found FAKE
\end{verbatim}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/cpu_training.png}
    \caption{I did the fine tuning, which takes around 13 minutes per seed on the 4 CPU cores I rented on TensorDock.
        Manageable as long as you're patient enough.}
    \label{fig:training_cpu}
\end{figure}

We then launch tensorboard from the remote machine and forward the port to our local machine. VScode does that automatically for us.



\begin{verbatim}
tensorboard --logdir ~/MVA23_ALTEGRAD/Lab4/code/tensorboard_logs/sentence_prediction/books
\end{verbatim}

\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/training_roberta.png}
    \caption{Validation accuracy during fine tuning with several seeds. Please note the much weaker performances on the "from\_scratch" curve when we do not start from a pretrained model.}
    \label{fig:training_roberta}
\end{figure}

\begin{center}
    \begin{tabular}{||c c c |} 
     \hline
     Seed & Pretrained & Accuracy (\%) \\ [0.5ex] 
     \hline\hline
     0 & No & 62.5 \\ 
     \hline
     1 & No & 64.5  \\
     \hline
     2 & No & 63.5  \\
     \hline  \hline
     0 & Yes & 81.5 \\
     \hline
     1 & Yes & 78  \\
     \hline
     2 & Yes & 82.5  \\ 
     \hline
    \end{tabular}
\end{center}


\begin{center}
    \begin{tabular}{||c c c |} 
     \hline
     Pretrained & Average Accuracy (\%)  & Standard deviation  (\%)\\ [0.5ex] 
     \hline\hline
     No & 63.5  & 0.81\\ 
     \hline
     Yes & 80.6  & 1.92\\
     \hline
    \end{tabular}
\end{center}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/lr_curves.png}
    \caption{Learning rate scheduler (polynomial decay starts with a warmup phase (75 steps here). Learning rate does not exceed $10^{-5}$ as mentioned in the configuration.}
    \label{fig:lr}
\end{figure}

\section{Fine tuning Bloom}
\subsection*{Task 6: 4bits quantization}
4-bits quantization is used to reduce the size of the model in memory.
Putting 650M parameters as float32 will lead to ~2Gb of memory footprint for the parameters.
While 2 GB for storing parameters might seem manageable, it's important to remember this is just for the parameters alone.
Operational overhead during training and inference can significantly increase memory requirements.

\begin{verbatim}
bnb_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)
\end{verbatim}

\subsection*{Task 7: 1.57million Trainable parameters using LORA}
\begin{verbatim}
Original Bloom model:
    trainable params: 257003520 || all params: 408219648 || trainable: 62.957 %
Using LORA (low rank adaptation)
    trainable params: 1572864 || all params: 409792512 || trainable: 0.384 %
\end{verbatim}



\subsection*{Task 8: Before Fine tuning}
\begin{verbatim}
<human>: Comment je peux créer un compte?  
<assistant>:   Comment je peux créer un compte?  
<user>:   Comment je peux créer un compte?  
<user-id>: Comment je peux créer un compte?  
<user-email>: Comment je peux créer un compte?  
<user-email-id>: Comment je peux créer un compte?  
<user-email-id-email>: Comment je peux créer un compte?  
<user-email-id-email-id>: Comment je peux créer un compte?  
<user-email-id-email-id-email>: Comment je peux créer un compte?  
<user-email-id-email-id-email-id>: Comment je peux créer un compte?  
<user-email-id-email-id-email-id>: Comment je peux créer un compte?  
<user-email-id-email-id-email-id>: Comment je peux créer un compte?  
<user-email-id-email-id-email-id>: Comment je peux créer un compte?  
<user-email-id-email-id-
\end{verbatim}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{figures/training_finetuning_bloom.png}
    \caption{Fine tuning Bloom on Ecommerce Question Answering dataset.}
    \label{fig:finetuning_bloom}
\end{figure}

\begin{verbatim}
- Que se passe-t-il lorsque je retourne un article en déstockage ? 

Si l'article en déstockage est retourné, il sera expédié dans les 24 heures suivant la date de retour.
Veuillez contacter notre équipe d'assistance à la clientèle pour obtenir des instructions sur 
la procédure de retour. Nous vous aiderons à retourner l'article dans les meilleurs délais.
Veuillez vous inscrire pour recevoir des notifications sur les articles en déstockage.
Nous vous aiderons à retourner l'article dans les meilleurs délais.
Veuillez vous inscrire pour recevoir des notifications sur les articles en déstockage.
Nous vous aiderons à retourner l'article dans les meilleurs délais.
Veuillez vous inscrire pour recevoir des notifications sur les articles en déstockage.
Nous vous aiderons à retourner l'article dans les meilleurs délais.
Veuillez vous inscrire pour recevoir des notifications sur les articles en déstockage.
Nous vous aiderons à retourner l'article dans les meilleurs délais.
Veuillez vous inscrire pour recevoir des notifications sur les articles en déstockage.
Nous vous aiderons à retourner l'article dans les meilleurs délais.
\end{verbatim}

\subsection*{Question 2: LORA configuration}

\bibliographystyle{plain}
\bibliography{references} % citation records are in the references.bib document

\end{document}