\documentclass[a4paper]{article} 
\input{style/head.tex}
\newcommand{\yourname}{Balthazar Neveu}
\newcommand{\youremail}{balthazarneveu@gmail.com}
\newcommand{\assignmentnumber}{3}

\begin{document}

\input{style/header.tex}
Assume a sequence of $L$ words (and a batch size fof $N=1$ for simplified understanding).
\section{Question 1: Role of attention and positional encoding}
\subsection*{Attention square mask}
The square mask has 2 purposes:
\begin{itemize}
\item \textbf{Causality of the prediction}: The mask serves to mask the future tokens. So the current predicted word cannot see what's coming next.
defined as a square matrix.
\item \textbf{Performances} Save some computation time during training
    \begin{itemize} 
        \item The attention scores are not computed for one word at a time. We directly let the whole sequence vector attend to the whole sequence vector, resulting
    in a $(L, L)$ attention score matrix.
        \item \textit{Note: otherwise you'd have to loop $L$ times for all the words, assume you're treating  word $l$, you have to compute an attention vector of size $l$ to the previous word.
        But tensor operations are not friendly with varying shapes. So better do a bit of extra attention computation and be able to process a whole sequence at once (actually $\frac{L*(L-1)}{2})$ useless attention scalars are computed.} 
        \item In the end, you compute a $(L,L)$ attention score and pytorch adds the mask.
        \item Adding $-\infty$ to the score will result in a weight of $0$ after the softmax operations. Basically, we do not take the future words into account when multiplying with the values.
        \item Adding $0$ to the score will leave the original weights untouched.
    \end{itemize} 


\begin{verbatim}
    Attention mask for a sequence of 5 tokens.
    10 useless computations for a sequence of 5 tokens
       [[0., -inf, -inf, -inf, -inf],
        [0.,   0., -inf, -inf, -inf],
        [0.,   0.,   0., -inf, -inf],
        [0.,   0.,   0.,   0., -inf],
        [0.,   0.,   0.,   0.,   0.]]
\end{verbatim}
\end{itemize} 


\subsection*{Positional encoding}
The role of positional encoding is to provide the order of the words in the sentence to the attention mechanism.
\begin{itemize}
\item Observation 1: \textit{Order in a sequence matters: "Le chat chasse la souris" has a totally different meaning that "La souris chasse le chat".}
\item Observation 2: \textit{Unlike recurrent neural networks (RNNs) like the GRU-based architecture we used in lab 1 for translation, the Transformer \cite{vaswani2017} architecture \cite{vaswani2017}
doesn't process source sentences in a step-by-step manner. Instead, it processes all tokens in the sequence simultaneously.
This parallel processing capability gives Transformer \cite{vaswani2017}s their efficiency but also means that they lack an inherent sense of position or order in a sequence.}
\item To solve the issue mentioned above, positional encoding vector is added to the embeddings of tokens before they're fed into the Transformer \cite{vaswani2017}.
The positional encoding is a set of values that's designed to give the model information about the position of each token within the sequence.
The goal is to ensure that the resulting combined embeddings (token embedding + positional encoding) will be unique for each position, even if the token is the same.
\item This also lets the Transformer \cite{vaswani2017} architecture (in its internal computations) behave differently depending on the "distance" between words.
\end{itemize} 

\section{Question 2: Classification head}
\subsection*{Language modeling}
During pre-training, the Transformer \cite{vaswani2017} model transforms each token into a representative feature vector supposed to model the next word.

The classification head used for this purpose maps that vector into a logit/probability of getting the next word.
This allows to train on a corpus of un-annotated
sentences. If you want to generate new text or complete a sequence, you can use this classifier. \newline
But \textbf{if you want to do any other downstream task} (e.g sentiment classification or spam classification)
\textbf{the original classifier becomes useless.}

\subsection*{Downstream task(like classification)}
Instead, you can replace the original classification head dedicated to language modeling by a new classification head for a specific downstream task. 

To fine-tune, we can basilly freeze the Transformer \cite{vaswani2017} pretrained weigths and leverage the fact that the representation outputs of the Transformer \cite{vaswani2017}
will model the tokens in a very sensitive feature space. A powerful feature space which models many inherent things in the sentence.
Therefore, with a few annotated data samples for your downstream classification task (e.g Google maps comments on restaurants annotated with their star ratings, spam/not spam examples), you can simply retrain a new classifier head.
\textit{Freezing the Transformer \cite{vaswani2017} weights is not mandatory, you could also fine tune the Transformer \cite{vaswani2017} layers... but it seems a bit risky in my opinion if there's not much data
to perform the downstream task training.} 

\pagebreak
\section{Question 3: Trainable parameters}

\begin{table}[]
    \begin{tabular}{|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{Number of trainable parameter}              & \multicolumn{1}{c|}{Language modeling task} & \multicolumn{1}{c|}{Classification task}                                                       \\ \hline
    \begin{tabular}[c]{@{}l@{}}Transformer \cite{vaswani2017}\\ embeddings\end{tabular} & Trainable                                   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0 trainable parameters\\ = Frozen\end{tabular}} \\ \hline
    Transformer \cite{vaswani2017} layers                                               & Trainable                                   & \begin{tabular}[c]{@{}l@{}}0 trainable parameters\\ = Frozen\end{tabular}                      \\ \hline
    Classifier head                                                  & Trainable                                   & Trainable                                                                                      \\ \hline
    \end{tabular}
\end{table}


\section*{Task 3}
\begin{verbatim}
    <sos> Comment montrer que je suis autonome .
    Comment montrer que je suis autonome . <eos>
\end{verbatim}

Training logs. Seems like everything is going fine.
On a Nvidia T500, there is a memory limitation, we have to limit the sentence length to avoid out of memory issues.


\begin{itemize}
    \item $batch_{size} = 16$
    \item $max_{len} = 64$
\end{itemize}

\begin{verbatim}
    | epoch   1 |   500/ 3125 steps | loss 7.29929 | ppl 1479.254
    | epoch   1 |  1000/ 3125 steps | loss 6.47710 | ppl  650.081
    | epoch   1 |  1500/ 3125 steps | loss 6.19202 | ppl  488.834
    | epoch   1 |  2000/ 3125 steps | loss 6.05407 | ppl  425.842
    | epoch   1 |  2500/ 3125 steps | loss 5.93240 | ppl  377.057
    | epoch   1 |  3000/ 3125 steps | loss 5.82976 | ppl  340.279
    | epoch   2 |   500/ 3125 steps | loss 5.50992 | ppl  247.131
    | epoch   2 |  1000/ 3125 steps | loss 5.46325 | ppl  235.864
    | epoch   2 |  1500/ 3125 steps | loss 5.42948 | ppl  228.032
    | epoch   2 |  2000/ 3125 steps | loss 5.40475 | ppl  222.461
    | epoch   2 |  2500/ 3125 steps | loss 5.37446 | ppl  215.824
    | epoch   2 |  3000/ 3125 steps | loss 5.35723 | ppl  212.136
\end{verbatim}



Here are the prediction results compared to the provided pretrained model.

\begin{verbatim}
Epoch 2:
--------
Bonjour les plus tard, il a été mis à la fin de la région de la Commission,
mais aussi de la loi de la région de la Commission

Pretrained model:
-----------------
Bonjour les gens qui ont été très accueillants et sympathiques.
\end{verbatim}


\section{Question 4: Results interpretation}



\section{Question 5: Language modeling limitations}
\begin{itemize}
\item The proposed approach to language modeling is next word prediction which implies causality. It is also known as autoregressive language modeling.
Example: "Salut les copains, comment ???"
\item BERT \cite{devlin2019bert} introduces a different training objective which is masked language modeling. 
Example: "Salut les copains, ??? allez vous?"
\end{itemize}

In the causal LM objective, each token can only attend to previous tokens in the sequence, it's unidirectional. 
This is because the model is designed to predict the next token in a sequence. 
While this setup is great for generative tasks, it doesn't always capture the context from both sides (before and after) 
of a word or phrase, which can be beneficial for understanding its meaning in certain situations.

A comment might begin negatively but end positively or vice versa. Being able to see and consider the entire sentence at once can help in better grasping its overall sentiment.
Example: "L'ensemble du livre est médiocre. Mais les dix premières pages m'ont vendu tellement de rêve que je l'ai lu jusqu'au bout."
The classifier based on the autoregressive language model may simply forget the begining of the sentence and classify the review as a "good review".
The BERT model will actually have to focus on both the negative and positive sentences to actually understand the "Mais" word.

\subsection*{Note}
\textit{
For the specific downstream task considered in this notebook, sentiment classification, 
it didn't seem straigthforward at first that we'd need the bidirectional advantage
of BERT to simply grasp whether or not the recommendation was good or not. It doesn't seem like a very complete.
BERT\cite{devlin2019bert} shines on much more complex reasoning benchmarks examples mentioned in the GLUE \cite{wang2019glue} benchmark like
the Winograd canonical example:} \\
Who does the pronoun \textbf{They} refer to in the 2 following sentences?.

\begin{itemize}
\item \textbf{The city councilmen} refused the demonstrators a permit because \textbf{they} feared violence.
\item The city councilmen refused \textbf{the demonstrators} a permit because \textbf{they} advocated violence.
\end{itemize}

Answering this question definitely requires the context before and after the word \textbf{they} and seems much more
complex than feeling what's negative or positive in a review.
%------------------------------------------------

\bibliographystyle{plain}
\bibliography{references} % citation records are in the references.bib document

\end{document}