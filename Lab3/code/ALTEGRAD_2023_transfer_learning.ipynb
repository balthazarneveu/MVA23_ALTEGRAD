{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 3: Transfer learning for NLP</h2> 24 / 10 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Balthazar Neveu\n",
        "\n",
        "</center>\n",
        "\n",
        "<br><br>\n",
        "In this lab we will:\n",
        "* Implement and pretrain a language model with transformer architecture.\n",
        "* Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.\n",
        "* Compare the performance of the pretrained model to a model trained from scratch.\n",
        " <br>\n",
        "\n",
        "<b>The deadline for this lab is October 31, 2023 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "!head -5 dict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_vocab = Path(\"dict.txt\")\n",
        "assert path_vocab.exists()\n",
        "pretraining_path_data_train = Path(\"pretraining_subset.txt\")\n",
        "assert pretraining_path_data_train.exists()\n",
        "\n",
        "downstream_path_data_train = Path(\"train.review.spm\")\n",
        "assert downstream_path_data_train.exists()\n",
        "downstream_path_labels_train = Path(\"train.label\")\n",
        "assert downstream_path_labels_train.exists()\n",
        "downstream_path_data_valid = Path(\"test.review.spm\")\n",
        "assert downstream_path_data_valid.exists()\n",
        "downstream_path_labels_valid = Path(\"test.label\")\n",
        "assert downstream_path_labels_valid.exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_model = Path(\"pretrained_model_4layers.pt\")\n",
        "assert pretrained_model.exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensor convention for NLP\n",
        "`[L, N, D]`\n",
        "- L sequence length\n",
        "- N batch size\n",
        "- V vocabulary dimension `ntokens`\n",
        "- E embeddings dimension `embedding_dim`\n",
        "- D hidden dimension\n",
        "\n",
        "### Simplification:\n",
        "- `E=D` hidden dimension set equal to th embedding dimension for simplicity in the following code `nhid = embedding_dim`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add fixed precomputed positional encoding to the embeddings\n",
        "    Add means (=literally addition)\n",
        "    \"\"\"\n",
        "    def __init__(self, embdeddings_dim: int , dropout: float =0.1, max_len: int =5000):\n",
        "        \"\"\"Precompute a positional encoding vector of length `max_len`\n",
        "\n",
        "        Args:\n",
        "            embdeddings_dim (int): dimension of word embeddings. Note th\n",
        "            dropout (float, optional): dropout ratio. Defaults to 0.1.\n",
        "            max_len (int, optional): maximum sequence length. Defaults to 5000.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, embdeddings_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embdeddings_dim, 2).float() * (-math.log(10000.0) / embdeddings_dim)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Add positional encoding to the word embeddings.\n",
        "        Simply add the pre\n",
        "\n",
        "        Args:\n",
        "            x (torch.FloatTensor): embeddings tensor [L, N, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: Enhanced embeddings tensor, ready to go straight to the transformer blocks. \n",
        "        \"\"\"\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Transformer base model \n",
        "    =========================\n",
        "    - embedding from word to vectors\n",
        "    - add positional encoding\n",
        "    - `nlayers` * transformer blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, ntokens:int, nhead:int, nhid:int, nlayers:int, dropout=0.5):\n",
        "        \"\"\"Transformer base model\n",
        "\n",
        "        Args:\n",
        "            ntokens (int): the size of vocabulary\n",
        "            nhead (int): number of heads in each of the MHA models\n",
        "            nhid (int): hidden dimension of the model. assume `embedding_dim` = `nhid`\n",
        "            nlayers (int): number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "            dropout (float, optional): dropout value. Defaults to 0.5.\n",
        "        \"\"\"\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = \"Transformer\"\n",
        "        embedding_dim = nhid # use the same embedding & hidden dimensions\n",
        "        self.encoder = nn.Embedding(ntokens, embedding_dim) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid, dropout=dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        \n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=nhid, # input dimension to the transformer encoder layer\n",
        "            nhead=nhead, # number of heads for MHA (Multi-head attention)\n",
        "            dim_feedforward=nhid, # output dimension of the MLP on top of the transformer.\n",
        "            dropout=dropout\n",
        "        ) # we assume nhid = d_model = dim_feedforward\n",
        "        \n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=nlayers\n",
        "        )\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(sz: int) -> torch.FloatTensor:\n",
        "        \"\"\"Generate causality mask = mask future tokens for next word prediction\n",
        "\n",
        "        Args:\n",
        "            sz (int): mask size M\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: squares matrix [M, M] to mask the attention matrix.\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(\n",
        "            self, src: torch.LongTensor,\n",
        "            src_mask: torch.FloatTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Embdeddings, positional encoders, go trough `nlayers` of residual {multi (`nhead`) attention heads + MLP}.\n",
        "\n",
        "        Args:\n",
        "            src (torch.LongTensor): [L, N, V] sequence of tokens , V=vocabu\n",
        "            src_mask (torch.FloatTensor): [L, L] squared mask\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: encoded sequence [L, N, D]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) #embed [L, N, V] -> [L, N, E]\n",
        "        src = self.pos_encoder(src) # [L, N, E]  - add positional encoding\n",
        "        output = self.transformer_encoder(src, mask=src_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid: int, nclasses: int):\n",
        "        \"\"\"Linear classification head -> returns logits (not probabilities)\n",
        "\n",
        "        Args:\n",
        "            nhid (int): hidden dimension\n",
        "            nclasses (int): number of classes.\n",
        "        \"\"\"\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Classify encoded feature vectors\n",
        "\n",
        "        Args:\n",
        "            src (torch.FloatTensor): Encoded feature vectors [L, N, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: Logits (no softmax applied)\n",
        "        \"\"\"\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken: int, nhead: int, nhid: int, nlayers: int, nclasses: int, dropout: float=0.5):\n",
        "        \"\"\"TransformerModel+ClassificationHead\n",
        "        \n",
        "        This allows defining a model for next word prediction (classification with ntoken classes)\n",
        "        Or other downstream tasks if the base `TransformerModel` is pretrained\n",
        "\n",
        "        Args:\n",
        "        \n",
        "            ntoken (int): size of vocabulary for (`TransformerModel`)\n",
        "            nhead (int): number of heads in each of the MHA models (`TransformerModel`)\n",
        "            nhid (int): hidden dimension of the model. assume `embedding_dim` = `nhid`\n",
        "            nlayers (int):  number of nn.TransformerEncoderLayer in nn.TransformerEncoder (`TransformerModel`)\n",
        "            nclasses (int): number of output classes in the classifier `ClassificationHead`\n",
        "                - =size of vocabulary for next word prediction\n",
        "                - other for downstream tasks like sentiment analyzis.\n",
        "            dropout (float, optional): _description_. Defaults to 0.5.  (`TransformerModel`)\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers, dropout=dropout)\n",
        "        self.classifier = ClassificationHead(nhid, nclasses)\n",
        "\n",
        "    def forward(self, src:torch.LongTensor, src_mask: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Encoder + linear classifier\n",
        "\n",
        "        Args:\n",
        "            src (torch.LongTensor): sequence of tokens [L, N, V]\n",
        "            src_mask (torch.FloatTensor): [L, L] squared mask.\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: [N, C]\n",
        "        \"\"\"\n",
        "        # base model\n",
        "        x = self.base(src, src_mask)\n",
        "        # classifier model\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Causal attention mask & useless computations (question 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 useless computations for a sequence of 5 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_length_test = 5\n",
        "src_mask = TransformerModel.generate_square_subsequent_mask(sentence_length_test)\n",
        "useless_computations = sentence_length_test*(sentence_length_test-1)//2\n",
        "assert int( ((-src_mask).isinf()).sum()) == useless_computations\n",
        "print(f\"{useless_computations} useless computations for a sequence of {sentence_length_test} tokens\")\n",
        "src_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unit test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rhb2gkUhJMR0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bneveu/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:255: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 100])\n"
          ]
        }
      ],
      "source": [
        "def test_transformer_based_classifier():\n",
        "    ntokens = 100 #  V the size of vocabulary\n",
        "    nhid = 200  # hidden dimension\n",
        "    nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    nhead = 2  # the number of heads in the multiheadattention models\n",
        "    dropout = 0  # the dropout value\n",
        "    nclasses = ntokens # classification to get output words in the same language\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, nclasses, dropout).to(device)\n",
        "    dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21], [8, 5, 3, 42, 43, 21]]).to(device)\n",
        "\n",
        "    sequence_length = dummy_input.shape[0] #L\n",
        "    batch_size = dummy_input.shape[1] #N\n",
        "\n",
        "    src_mask = TransformerModel.generate_square_subsequent_mask(sequence_length).to(device)\n",
        "    assert list(src_mask.shape) == [sequence_length,sequence_length]\n",
        "    # batch dimension N is not involved in the mask computation! We assume all sequences in the batch has the same sequence length L\n",
        "    out = model.forward(dummy_input, src_mask)\n",
        "    expected_size = [sequence_length, batch_size, nclasses]\n",
        "    assert list(out.shape) == expected_size, f\"{out.shape}, {expected_size}\"\n",
        "    print(out.shape)\n",
        "test_transformer_based_classifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vFdH_-JeFbGA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁trop <sos>\n"
          ]
        }
      ],
      "source": [
        "SRC = \"source_sequence\"\n",
        "TGT = \"target\"\n",
        "SOS = \"<sos>\"\n",
        "PAD = \"<pad>\"\n",
        "EOS = \"<eos>\"\n",
        "OOV = \"<oov>\"\n",
        "LM_TASK = \"language_modeling\"\n",
        "DS_TASK = \"classification\"\n",
        "token2ind = {SOS: 0, PAD : 1, EOS: 2, OOV: 3} # the 4 first indices are reserved to special tokens\n",
        "offset = max(token2ind.values())+1\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] = idx+offset\n",
        "ind2token = {index: token for token, index in token2ind.items()}\n",
        "print(ind2token[1111], ind2token[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents: Path,\n",
        "        path_labels: Path = None,\n",
        "        token2ind: Dict[str, int]={},\n",
        "        max_len: int=512,\n",
        "        task: str=LM_TASK,\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "        self.oov_index = self.token2ind[OOV]\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1] \n",
        "        \n",
        "        source_sequence = [self.token2ind.get(token, self.oov_index) for token in sequence]\n",
        "        source_sequence.insert(0, self.token2ind[SOS]) \n",
        "        # (constract the input sequence using token2ind, sequence and special tokens)\n",
        "        if self.task == LM_TASK:\n",
        "            target = source_sequence[1:] # offset the sequence by one\n",
        "            # A, B , C, D , <EOS>\n",
        "            target.append(self.token2ind[EOS])\n",
        "            assert len(target) == len(source_sequence)\n",
        "        elif self.task == DS_TASK:\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            SRC: torch.tensor(source_sequence),\n",
        "            TGT: torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def collate_sentences_keep_dim(batch: List[Dict[str, torch.LongTensor]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "    \"\"\"Uniformize batches (have the same sentence length with padding for all sentences across the batch)\n",
        "\n",
        "    Args:\n",
        "        batch (List[Dict[str, torch.LongTensor]]): List of dict samples containing \n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.LongTensor, torch.LongTensor]: \n",
        "            - source [L, N, V]\n",
        "            where L is the maximum length along all sentences in the batch\n",
        "            - target \n",
        "                - [L, N, V] for language modeling task\n",
        "                - [N, C] for classification with C the number of classes\n",
        "            \n",
        "    \"\"\"\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[SRC] for sample in batch], padding_value=token2ind[PAD]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[TGT] for sample in batch], padding_value=token2ind[PAD]\n",
        "    )\n",
        "    return source_sequences, target\n",
        "\n",
        "\n",
        "def collate_sentences(batch: List[Dict[str, torch.LongTensor]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "    source_sequences, target = collate_sentences_keep_dim(batch)\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "def get_loader(\n",
        "    path_documents :Path,\n",
        "    path_labels: Path = None,\n",
        "    token2ind : Dict[str, int]={},\n",
        "    max_len: int =512,\n",
        "    batch_size: int = 32,\n",
        "    task: str=LM_TASK,\n",
        "    collate_fn = collate_sentences,\n",
        "    shuffle=True\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=max_len,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sanity check on data loader and collate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> Les chauds et les froids d ' un déplacement présidentiel ( M itter rand ) ou encore la prise en otage d ' un air bus ministériel ( cinq ministres , vingt députés et .\n",
            "Les chauds et les froids d ' un déplacement présidentiel ( M itter rand ) ou encore la prise en otage d ' un air bus ministériel ( cinq ministres , vingt députés et . <eos>\n",
            "<sos> Les hommes sont encore en train de mesurer leur pouce .\n",
            "Les hommes sont encore en train de mesurer leur pouce . <eos>\n",
            "<sos> La recherche est possible directement sur la carte , ou bien en rentrant une adresse de son choix .\n",
            "La recherche est possible directement sur la carte , ou bien en rentrant une adresse de son choix . <eos>\n",
            "<sos> Tous les jours nous cherchons pour vous sur le web les articles , vidéos et documentaires qui nous paraissent les plus pertinents et utiles à tous .\n",
            "Tous les jours nous cherchons pour vous sur le web les articles , vidéos et documentaires qui nous paraissent les plus pertinents et utiles à tous . <eos>\n",
            "<sos> D ' abord , le Hezbollah a toujours eu les mains sales et donne la naus ée lorsqu ' on voit le nombre d ' activités illégales qui participent de son financement : tous les trafics , armes , cigarettes , contrefaçon , drogues , passent par ses mains , qui ne sont plus innoc entes depuis longtemps .\n",
            "D ' abord , le Hezbollah a toujours eu les mains sales et donne la naus ée lorsqu ' on voit le nombre d ' activités illégales qui participent de son financement : tous les trafics , armes , cigarettes , contrefaçon , drogues , passent par ses mains , qui ne sont plus innoc entes depuis longtemps . <eos>\n"
          ]
        }
      ],
      "source": [
        "N = 32\n",
        "data_loader = get_loader(\n",
        "    pretraining_path_data_train,\n",
        "    token2ind=token2ind,\n",
        "    batch_size=N,\n",
        "    task=LM_TASK,\n",
        "    collate_fn=collate_sentences_keep_dim\n",
        ")\n",
        "token2ind[OOV]\n",
        "it = iter(data_loader)\n",
        "\n",
        "def tensor_to_sentence(tensor_sentence):\n",
        "        return \" \".join([ind2token.get(el, \"not found\").replace(\"▁\", \"\") for el in tensor_sentence.numpy() if el != token2ind[PAD]])\n",
        "\n",
        "for u in range(5):\n",
        "    sampled_batch = next(it)\n",
        "    # first[0].shape, first[1].shape\n",
        "    # first[0][:, 0] # first sentence # L, N\n",
        "    first_sentence = sampled_batch[0][:, 0]\n",
        "    def tensor_to_sentence(tensor_sentence):\n",
        "        return \" \".join([ind2token.get(el, \"not found\").replace(\"▁\", \"\") for el in tensor_sentence.numpy() if el != token2ind[PAD]])\n",
        "    print(tensor_to_sentence(first_sentence))\n",
        "    print(tensor_to_sentence(sampled_batch[1][:, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50001\n"
          ]
        }
      ],
      "source": [
        "ntokens = len(ind2token) # the size of vocabulary\n",
        "print(len(ind2token))\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model_pretraining = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimization parameters\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model_pretraining.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    path_data_train: Path,\n",
        "    path_labels_train: Path =None,\n",
        "    path_data_valid: Path = None,\n",
        "    save_interval: int =-1,\n",
        "    log_interval: int=5,\n",
        "    task: str=LM_TASK,\n",
        "    batch_size: int =32,\n",
        "    max_len=512,\n",
        "    epoch=0,\n",
        "    name=\"\"\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        max_len=max_len,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): # get a batch of samples\n",
        "        # reset gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        src_mask = TransformerModel.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        # forward pass\n",
        "        output = model(input, src_mask) \n",
        "        if task == DS_TASK:\n",
        "            output = output[-1:, :]  # last vector only\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = data[1]\n",
        "        target = target.to(device)\n",
        "        loss =  criterion(output, target) # CROSS ENTROPY\n",
        "        loss.backward() # compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                f\"| epoch {epoch:3d} | {idx:5d}/{len(data_loader):5d} steps | \"+\n",
        "                f\"loss {cur_loss:5.5f} | ppl {math.exp(cur_loss):8.3f}\"\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    if epoch % save_interval == 0 and save_interval>=1:\n",
        "        torch.save(model, f\"weights_{task}{name}_{epoch:02d}.pt\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# EMPTY THE GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m11g4ScjZaR"
      },
      "outputs": [],
      "source": [
        "#pretraining on a tiny subset\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        pretraining_path_data_train,\n",
        "        save_interval=-1,\n",
        "        task=LM_TASK,\n",
        "        batch_size=16,\n",
        "        max_len=64,\n",
        "        log_interval=log_interval,\n",
        "        epoch=epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-BcBC6FSkMH3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bneveu/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:255: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_pretraining = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device)\n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt')\n",
        "#load state dict\n",
        "model_pretraining.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tBRRVsWqlIoQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour les gens qui ont été très accueillants et sympathiques.\n"
          ]
        }
      ],
      "source": [
        "def infer_next_token(sent, model):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind =  int(torch.argmax(torch.nn.Softmax(dim=-1)(out[-1, 0, :])).detach()) #FORCE N=1\n",
        "    return next_token_ind, out\n",
        "\n",
        "def infer_next_tokens(sent, model, max_len=50):\n",
        "    next_token_list = []\n",
        "    next_token_index=-1\n",
        "    iter = 0\n",
        "    while next_token_index!=token2ind[EOS]:\n",
        "        if iter>max_len:\n",
        "            break\n",
        "        next_token_index, out = infer_next_token(sent, model)\n",
        "        next_token = ind2token.get(next_token_index)\n",
        "        sent+= next_token\n",
        "        next_token_list.append(next_token)\n",
        "    return next_token_list\n",
        "\n",
        "\n",
        "sent = \"Bonjour les\"\n",
        "out = infer_next_tokens(sent, model_pretraining)\n",
        "print(sent + \" \" + s.decode_pieces(out[:-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000 sentences in the validation set\n"
          ]
        }
      ],
      "source": [
        "data_loader_validation = get_loader(\n",
        "    downstream_path_data_valid,\n",
        "    downstream_path_labels_valid,\n",
        "    token2ind=token2ind,\n",
        "    # batch_size=20,\n",
        "    batch_size=1, # Let's use a batch size of 1 so a sentence always has maximum length\n",
        "    task='classification',\n",
        "    shuffle=False # AVOID SHUFFLING FOR OBVIOUS REASONS - even if we're going through the whole validation set - let's do it in the same order everytime (batches will always be the same, no matter the configuration)\n",
        ")\n",
        "print(f\"{len(data_loader_validation.dataset)} sentences in the validation set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> J ' ai lu ce livre car dans ma ville , tout le monde s ' en sert et le commande . C ' est ma pharmaci enne qui me l ' a conseillé , elle a t ellement maig ri que je lui ai demandé ce qu ' elle avait fait et au lieu de me vendre du per li mp in pin en gé lules , elle m ' a conseillé ce livre à 5 euros . Bien sur , il faut faire un effort pour perdre 25 kilos mais avec le livre , j ' avais un compagnon de route . L ' auteur a su me parler simplement avec des arguments très forts et surtout j ' ai senti qu ' il connaissait bien des cas comme le mien . Il y a dans son texte de l ' expérience , de la simplicité et de la compassion pour ceux qui comme moi vivait avec tout ce poids qui me col lait au corps sans jamais vouloir partir . Je ne crois pas qu ' il existe un régime miracle qui surp asse les autres mais je crois vraiment qu ' il y a des personnes qui savent parler aux autres et faire n aitre des déclic s . Je croyais être faible mais ce livre m ' a rendu forte , je l ' ai t ellement ann oté que j ' en suis à mon troisième . Quand on est très grosse comme je l ' ai été , les non - gros ne vous comprennent pas ou ont peur de vous frois ser en vous en parlant , alors ce livre a été comme un compagnon - journal . Je suis pé dic ure et je l ' ai conseillé à tous mes clients gros dont je lis la souffrance sur les pieds déform és et gonf lés . je rends aux autres le service que m ' a rendu ma pharmaci enne . Je le conseille à tous ceux qui souffrent car après avoir maig ri c ' est un tel bonheur que j ' ai accepté de passer à la phase 3 de ce plan qui impose 10 jours de consolidation pour chaque kilo perdu en s ' ouvrant progressivement à tout . Maintenant , je suis en phase 4, c ' est à dire que je mange de tout sauf le jeudi où je contrôle . Je remercier ai jamais assez l ' auteur de ce livre .\n",
            "True\n",
            "<sos> Recettes appréciées de toute la famille ( petits et grands ) de plus on peut faire son régime en ayant des invités , ils n ' y voient que du feu . Pour la vinaigrette il ne faut surtout pas dire quelle est faite avec de l ' huile de par af ine alors elle est excellente , s inon .... Le régime est super efficace il ne fatigue pas du tout . J ' encourage ceux qui ont des kilos en trop , à le faire , il ne faut pas beaucoup de volonté car on mange toujours à sa faim .\n",
            "True\n",
            "<sos> Be ig be der se dra pe de mystère . Il pose avec des airs sombres , en rou lé dans une cape noire , façon Z oz o ( com mentaire écrit sur fond musical : \" Z orro est arrivé \" de Henri Salvador ). Et pourtant , le seul mystère réservé aux ni ais , serait celui du \" mi ra cule ux \" succès obtenu sans aucun talent . La publicité a montré que le talent n ' était pas une condition nécessaire pour réussir . Un plat de nouilles a du succès s ' il est bien médi atisé . Les motoc ro ttes firent beaucoup pour la popularité du maire de Paris , Jacques C . \" Un roman français \" est un plat sans saveur , composé d ' ingrédients sans relief , qui n ' é veille ni l ' appétit , ni l ' esprit . Vous en doute z ? Au chapitre de l ' éducation sexuelle , cette autobiographie , nous conte les premiers ém ois à 13 ans de l ' auteur qui embrasse sur la bouche avec la langue dehors . Les goûts littéraires sont présentés à la manière d ' un feu d ' artifice Internet . Vous n ' avez rien lu de ce dont vous parlez . Pas grave , c itez p êle - mêle des noms d ' écriv ains aux cons on ances exotiques , anglo - sax onnes . Une pause . Qu ' il est fort ce Z oz o ! Be ig be der , dans la lignée de l ' émission \" lit té raire \" qu ' il avait \" anim ée \" nous apprend que San Antonio ( sic !) est un auteur de droite , comme Rab elais ( !). Rab elais , un écrivain de droite ! Le délire , l ' in culture et l ' absence de syntaxe soutiennent pénible ment la démonstration : Be ig be der appréci ait ces écrivains de droite car ils sont rigol os ( je cite ) alors que les auteurs de gauche , Sartre , Camus ne le sont pas ... à l ' exception précise - t - il des \" M ots \" et de \" La Ch ute \". Ces ouvrages seraient donc \" rig olos \". En quoi Céline , \" écrivain de droite \" peut - il être rigolo ? Trou bles ( ment aux ). Ce dro gué , coc aï nom ane , cra che sur la justice , les mesures de salubrité publique prises pour le sauver de lui - même . Z oz o était p af . Z oz o était malade . Z oz o était en infraction . Ah , ces 17 heures de mise \" en prison \" ( au poste de police seulement ) valent les années de goul ag de Sol j én its yne , celles du camp d ' extermination de Prim o Levi , celles de D\n",
            "False\n",
            "<sos> Un petit livre si facile à lire et si puissant . Le racisme ne peut pas résister longtemps devant une telle mise en perspective . Les \" ra ces \" soi - disant dominantes d ' aujourd ' hui ne seront pas celles de demain ... Un excellent livre à lire et offrir aussi souvent que l ' on peut\n",
            "True\n",
            "<sos> Saint - Ex upéry réalise à travers l ' histoire du petit prince l ' un des plus grands chefs - d ' oeuvres littéraires , poétiques et révolu tionnaires de tous les temps . La morale magique \" l ' essentiel est invisible pour les yeux \" semble rés onner partout où notre regard se porte , vibrant dans l ' harmonie retrouvée des premiers matins du monde . Change ton regard , là est la clé ! semble nous indiquer Saint - Ex . Ange d ' espoir , reviens ! nous lance - t - il en re tenant des larmes d ' enfant . Ce conte philosophique est une ode à la vie , au bonheur , à la compréhension et à la fraternité humaine . Il s ' agit d ' un testament poétique , d ' une oeuvre intemp orelle et universelle , d ' une tentative pour restaurer le lien secret entre l ' Homme et l ' Univers , entre l ' Homme et Lui - même . Mais la portée véritable et finale est d ' un autre ordre : la Fleur est destinée ...\n",
            "True\n",
            "<sos> je suis tres de çue de la taille de ce livre ( il tiens dans le creux de la main ... et les recettes y sont N ULES ( il n ' y a rien de nouveau et d ' in ven tif ) je ne le recommande a personne !!!\n",
            "False\n",
            "<sos> Ce livre n ' est pas cher et il comporte un grand nombre de recettes originales . Par contre , il est bourré de fautes : soit vous avez des ingrédients qui sont oubliés dans la recette ( quand les mettre ? ), ou la recette demandent des ingrédients mais ils ne sont pas dans la liste des ingrédients ( quelle quantité mettre ?) A acheter en connaissance de cause ...\n",
            "False\n",
            "<sos> Un roman de rentrée de la fille de B HL , a - t - on vraiment besoin d ' en dire plus pour vous convaincre que la lecture de cet ouvrage , au style aussi peu inspiré que lourd , vous fera perdre de précieuses heures ? A mon humble avis , le seul fait d ' être la fille d ' un philosophe milliardaire ne donne pas automatiquement toutes les qualités requises pour faire un bon écrivain . Pour ma part j ' ai assez rapidement laissé tomber ce bouquin pour me plonger dans .\n",
            "False\n",
            "<sos> Contrairement au commentaire de Seb , je trouve ce livre vraiment très pratique . Il y a certes quelques erreurs comme l ' IG du cass oulet effectivement non nul . Mais le reste de la ligne montre qu ' il y a bien de n bre uses glucides et plus important qu ' il est à limiter pour les diabétiques . Les pages sont bien dans le bon ordre ( c ' est ton exemplaire qui est mauvais Seb !) et surtout il n ' y a pas d ' erreurs sur les quantités de G , L ou P car le n bre noté est pour la quantité indiquée . Donc pour le N ute lla pas d ' erreur à noter puisqu ' il s ' agit de 15 g rs de produit étudié et pas la valeur pour 100 g rs comme sur le pot . Donc monsieur Seb devrait chercher à mieux comprendre le tableau avant de critiquer ce fabuleux outils qui a tout de même le mérite , en format poche , de regrouper 11 40 aliments ! Trés utile pour les diab étique .\n",
            "True\n",
            "<sos> j ' ai acheté ce bouquin sur le seul nom de Gran gé . On ne m ' y reprendra plus ! On n ' y croit pas un seul instant , c ' est creux , comme l ' héroïne . Je me suis forcée à le finir , en croyant au miracle . Que n enni !\n",
            "False\n",
            "<sos> Je dis non à la facilité ..... et non à l ' auteur qui n ' a même pas fait l ' effort de rencontrer la famille avant d ' écrire son bouquin , qui ne connait rien au personnage de Gregory et encore moins à l ' artiste !! ..... Comment parler correctement de quelqu ' un qu ' on a jamais rencontré de son vivant , et dont on ne s ' est intéressé qu ' après sa mort ??? Franchement à la place de l ' auteur j ' aurais vraiment honte de vouloir profiter de la douleur des fans !! Si vous voulez lire un bon livre sur GRE G OR Y LE MAR CH AL , achetez plutôt celui de sa soeur Leslie \" Mon Frère , l ' Ar tiste \" et en plus vous fer rez une bonne action , puisque tout est entièrement re versé à l ' association GRE G OR Y LE MAR CH AL !!\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence_batch in enumerate(data_loader_validation):\n",
        "    if idx>10:\n",
        "        break\n",
        "    first_sentence = sentence_batch[0][:, 0]\n",
        "    print(tensor_to_sentence(first_sentence))\n",
        "    print(bool(sentence_batch[1][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "outputs": [],
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader: DataLoader, model):\n",
        "    accuracy = 0\n",
        "    for _idx, sentence_batch in enumerate(data_loader):\n",
        "        input_sentences = sentence_batch[0].to(device)\n",
        "        target_labels = sentence_batch[1].to(device)\n",
        "        logits = model(input_sentences, src_mask=None)[-1, ...]\n",
        "        predicted_class = torch.argmax(logits)\n",
        "        accuracy += torch.sum(predicted_class==target_labels).detach()\n",
        "        # print(accuracy)\n",
        "        # print(logits.shape, sentence_batch[1].shape)\n",
        "        # torch.argmax(logits)\n",
        "    accuracy = float(accuracy)/len(data_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "def test_evaluate_accuracy():\n",
        "    model_ds = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    model_ds.eval()\n",
        "    acc = evaluate_accuracy(\n",
        "        data_loader_validation,\n",
        "        model_ds\n",
        "    )\n",
        "    print(f\"accuracy of a randomly initalized classifier {acc} ~ shall be around 0.5\")\n",
        "\n",
        "# test_evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "outputs": [],
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model_pretraining.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "i-xclMCpnVpw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bneveu/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:255: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/ 1600 steps | loss 0.80170 | ppl    2.229\n",
            "| epoch   1 |   100/ 1600 steps | loss 1.89372 | ppl    6.644\n",
            "| epoch   1 |   150/ 1600 steps | loss 1.51040 | ppl    4.529\n",
            "| epoch   1 |   200/ 1600 steps | loss 1.09961 | ppl    3.003\n",
            "| epoch   1 |   250/ 1600 steps | loss 1.62145 | ppl    5.060\n",
            "| epoch   1 |   300/ 1600 steps | loss 1.94498 | ppl    6.994\n",
            "| epoch   1 |   350/ 1600 steps | loss 1.23516 | ppl    3.439\n",
            "| epoch   1 |   400/ 1600 steps | loss 1.00855 | ppl    2.742\n",
            "| epoch   1 |   450/ 1600 steps | loss 1.88957 | ppl    6.617\n",
            "| epoch   1 |   500/ 1600 steps | loss 1.60197 | ppl    4.963\n",
            "| epoch   1 |   550/ 1600 steps | loss 1.90519 | ppl    6.721\n",
            "| epoch   1 |   600/ 1600 steps | loss 0.80170 | ppl    2.229\n",
            "| epoch   1 |   650/ 1600 steps | loss 1.52158 | ppl    4.579\n",
            "| epoch   1 |   700/ 1600 steps | loss 1.11996 | ppl    3.065\n",
            "| epoch   1 |   750/ 1600 steps | loss 1.41844 | ppl    4.131\n",
            "| epoch   1 |   800/ 1600 steps | loss 1.40640 | ppl    4.081\n",
            "| epoch   1 |   850/ 1600 steps | loss 1.52994 | ppl    4.618\n",
            "| epoch   1 |   900/ 1600 steps | loss 1.51956 | ppl    4.570\n",
            "| epoch   1 |   950/ 1600 steps | loss 1.10344 | ppl    3.015\n",
            "| epoch   1 |  1000/ 1600 steps | loss 1.79648 | ppl    6.028\n",
            "| epoch   1 |  1050/ 1600 steps | loss 0.94176 | ppl    2.564\n",
            "| epoch   1 |  1100/ 1600 steps | loss 1.32788 | ppl    3.773\n",
            "| epoch   1 |  1150/ 1600 steps | loss 1.12048 | ppl    3.066\n",
            "| epoch   1 |  1200/ 1600 steps | loss 1.60444 | ppl    4.975\n",
            "| epoch   1 |  1250/ 1600 steps | loss 1.06432 | ppl    2.899\n",
            "| epoch   1 |  1300/ 1600 steps | loss 1.05312 | ppl    2.867\n",
            "| epoch   1 |  1350/ 1600 steps | loss 1.35903 | ppl    3.892\n",
            "| epoch   1 |  1400/ 1600 steps | loss 1.81934 | ppl    6.168\n",
            "| epoch   1 |  1450/ 1600 steps | loss 1.21204 | ppl    3.360\n",
            "| epoch   1 |  1500/ 1600 steps | loss 1.35993 | ppl    3.896\n",
            "| epoch   1 |  1550/ 1600 steps | loss 1.69906 | ppl    5.469\n",
            "from_scratch=True - VALIDATION ACCURACY 0.6365\n",
            "| epoch   2 |    50/ 1600 steps | loss 1.15035 | ppl    3.159\n",
            "| epoch   2 |   100/ 1600 steps | loss 1.82475 | ppl    6.201\n",
            "| epoch   2 |   150/ 1600 steps | loss 1.64020 | ppl    5.156\n",
            "| epoch   2 |   200/ 1600 steps | loss 1.13063 | ppl    3.098\n",
            "| epoch   2 |   250/ 1600 steps | loss 1.51983 | ppl    4.571\n",
            "| epoch   2 |   300/ 1600 steps | loss 1.29656 | ppl    3.657\n",
            "| epoch   2 |   350/ 1600 steps | loss 0.98758 | ppl    2.685\n",
            "| epoch   2 |   400/ 1600 steps | loss 1.74231 | ppl    5.710\n",
            "| epoch   2 |   450/ 1600 steps | loss 1.19719 | ppl    3.311\n",
            "| epoch   2 |   500/ 1600 steps | loss 1.40211 | ppl    4.064\n",
            "| epoch   2 |   550/ 1600 steps | loss 1.48371 | ppl    4.409\n",
            "| epoch   2 |   600/ 1600 steps | loss 1.35642 | ppl    3.882\n",
            "| epoch   2 |   650/ 1600 steps | loss 1.50366 | ppl    4.498\n",
            "| epoch   2 |   700/ 1600 steps | loss 0.90169 | ppl    2.464\n",
            "| epoch   2 |   750/ 1600 steps | loss 1.87350 | ppl    6.511\n",
            "| epoch   2 |   800/ 1600 steps | loss 1.10800 | ppl    3.028\n",
            "| epoch   2 |   850/ 1600 steps | loss 1.19660 | ppl    3.309\n",
            "| epoch   2 |   900/ 1600 steps | loss 1.18388 | ppl    3.267\n",
            "| epoch   2 |   950/ 1600 steps | loss 1.53103 | ppl    4.623\n",
            "| epoch   2 |  1000/ 1600 steps | loss 1.53214 | ppl    4.628\n",
            "| epoch   2 |  1050/ 1600 steps | loss 1.02525 | ppl    2.788\n",
            "| epoch   2 |  1100/ 1600 steps | loss 0.83298 | ppl    2.300\n",
            "| epoch   2 |  1150/ 1600 steps | loss 1.43816 | ppl    4.213\n",
            "| epoch   2 |  1200/ 1600 steps | loss 1.23259 | ppl    3.430\n",
            "| epoch   2 |  1250/ 1600 steps | loss 1.29873 | ppl    3.665\n",
            "| epoch   2 |  1300/ 1600 steps | loss 1.58351 | ppl    4.872\n",
            "| epoch   2 |  1350/ 1600 steps | loss 0.92585 | ppl    2.524\n",
            "| epoch   2 |  1400/ 1600 steps | loss 1.09664 | ppl    2.994\n",
            "| epoch   2 |  1450/ 1600 steps | loss 1.02457 | ppl    2.786\n",
            "| epoch   2 |  1500/ 1600 steps | loss 1.37348 | ppl    3.949\n",
            "| epoch   2 |  1550/ 1600 steps | loss 1.71461 | ppl    5.554\n",
            "from_scratch=True - VALIDATION ACCURACY 0.671\n",
            "| epoch   3 |    50/ 1600 steps | loss 1.39475 | ppl    4.034\n",
            "| epoch   3 |   100/ 1600 steps | loss 0.93103 | ppl    2.537\n",
            "| epoch   3 |   150/ 1600 steps | loss 0.62288 | ppl    1.864\n",
            "| epoch   3 |   200/ 1600 steps | loss 1.23740 | ppl    3.447\n",
            "| epoch   3 |   250/ 1600 steps | loss 0.50551 | ppl    1.658\n",
            "| epoch   3 |   300/ 1600 steps | loss 1.04945 | ppl    2.856\n",
            "| epoch   3 |   350/ 1600 steps | loss 0.64481 | ppl    1.906\n",
            "| epoch   3 |   400/ 1600 steps | loss 0.69972 | ppl    2.013\n",
            "| epoch   3 |   450/ 1600 steps | loss 0.81183 | ppl    2.252\n",
            "| epoch   3 |   500/ 1600 steps | loss 0.59927 | ppl    1.821\n",
            "| epoch   3 |   550/ 1600 steps | loss 1.16921 | ppl    3.219\n",
            "| epoch   3 |   600/ 1600 steps | loss 0.69736 | ppl    2.008\n",
            "| epoch   3 |   650/ 1600 steps | loss 0.94151 | ppl    2.564\n",
            "| epoch   3 |   700/ 1600 steps | loss 0.93945 | ppl    2.559\n",
            "| epoch   3 |   750/ 1600 steps | loss 1.39674 | ppl    4.042\n",
            "| epoch   3 |   800/ 1600 steps | loss 0.31714 | ppl    1.373\n",
            "| epoch   3 |   850/ 1600 steps | loss 0.64717 | ppl    1.910\n",
            "| epoch   3 |   900/ 1600 steps | loss 0.82309 | ppl    2.278\n",
            "| epoch   3 |   950/ 1600 steps | loss 0.86654 | ppl    2.379\n",
            "| epoch   3 |  1000/ 1600 steps | loss 0.48112 | ppl    1.618\n",
            "| epoch   3 |  1050/ 1600 steps | loss 0.45245 | ppl    1.572\n",
            "| epoch   3 |  1100/ 1600 steps | loss 0.90455 | ppl    2.471\n",
            "| epoch   3 |  1150/ 1600 steps | loss 0.71840 | ppl    2.051\n",
            "| epoch   3 |  1200/ 1600 steps | loss 0.64279 | ppl    1.902\n",
            "| epoch   3 |  1250/ 1600 steps | loss 0.82977 | ppl    2.293\n",
            "| epoch   3 |  1300/ 1600 steps | loss 0.52096 | ppl    1.684\n",
            "| epoch   3 |  1350/ 1600 steps | loss 0.89811 | ppl    2.455\n",
            "| epoch   3 |  1400/ 1600 steps | loss 0.53450 | ppl    1.707\n",
            "| epoch   3 |  1450/ 1600 steps | loss 0.80865 | ppl    2.245\n",
            "| epoch   3 |  1500/ 1600 steps | loss 0.57764 | ppl    1.782\n",
            "| epoch   3 |  1550/ 1600 steps | loss 0.68723 | ppl    1.988\n",
            "from_scratch=True - VALIDATION ACCURACY 0.6765\n",
            "| epoch   4 |    50/ 1600 steps | loss 0.30858 | ppl    1.361\n",
            "| epoch   4 |   100/ 1600 steps | loss 0.20520 | ppl    1.228\n",
            "| epoch   4 |   150/ 1600 steps | loss 0.23759 | ppl    1.268\n",
            "| epoch   4 |   200/ 1600 steps | loss 0.23142 | ppl    1.260\n",
            "| epoch   4 |   250/ 1600 steps | loss 0.59961 | ppl    1.821\n",
            "| epoch   4 |   300/ 1600 steps | loss 0.27215 | ppl    1.313\n",
            "| epoch   4 |   350/ 1600 steps | loss 0.07003 | ppl    1.073\n",
            "| epoch   4 |   400/ 1600 steps | loss 0.00587 | ppl    1.006\n",
            "| epoch   4 |   450/ 1600 steps | loss 0.09514 | ppl    1.100\n",
            "| epoch   4 |   500/ 1600 steps | loss 0.13260 | ppl    1.142\n",
            "| epoch   4 |   550/ 1600 steps | loss 0.14672 | ppl    1.158\n",
            "| epoch   4 |   600/ 1600 steps | loss 0.88831 | ppl    2.431\n",
            "| epoch   4 |   650/ 1600 steps | loss 0.41103 | ppl    1.508\n",
            "| epoch   4 |   700/ 1600 steps | loss 0.14061 | ppl    1.151\n",
            "| epoch   4 |   750/ 1600 steps | loss 0.19409 | ppl    1.214\n",
            "| epoch   4 |   800/ 1600 steps | loss 0.23527 | ppl    1.265\n",
            "| epoch   4 |   850/ 1600 steps | loss 0.15717 | ppl    1.170\n",
            "| epoch   4 |   900/ 1600 steps | loss 0.44362 | ppl    1.558\n",
            "| epoch   4 |   950/ 1600 steps | loss 0.04215 | ppl    1.043\n",
            "| epoch   4 |  1000/ 1600 steps | loss 0.26444 | ppl    1.303\n",
            "| epoch   4 |  1050/ 1600 steps | loss 0.45983 | ppl    1.584\n",
            "| epoch   4 |  1100/ 1600 steps | loss 0.19812 | ppl    1.219\n",
            "| epoch   4 |  1150/ 1600 steps | loss 0.55967 | ppl    1.750\n",
            "| epoch   4 |  1200/ 1600 steps | loss 0.13444 | ppl    1.144\n",
            "| epoch   4 |  1250/ 1600 steps | loss 0.00053 | ppl    1.001\n",
            "| epoch   4 |  1300/ 1600 steps | loss 0.10040 | ppl    1.106\n",
            "| epoch   4 |  1350/ 1600 steps | loss 0.20894 | ppl    1.232\n",
            "| epoch   4 |  1400/ 1600 steps | loss 0.19374 | ppl    1.214\n",
            "| epoch   4 |  1450/ 1600 steps | loss 0.10019 | ppl    1.105\n",
            "| epoch   4 |  1500/ 1600 steps | loss 0.41360 | ppl    1.512\n",
            "| epoch   4 |  1550/ 1600 steps | loss 0.21293 | ppl    1.237\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7325\n",
            "| epoch   5 |    50/ 1600 steps | loss 0.00022 | ppl    1.000\n",
            "| epoch   5 |   100/ 1600 steps | loss 0.00032 | ppl    1.000\n",
            "| epoch   5 |   150/ 1600 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch   5 |   200/ 1600 steps | loss 0.12896 | ppl    1.138\n",
            "| epoch   5 |   250/ 1600 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch   5 |   300/ 1600 steps | loss 0.00016 | ppl    1.000\n",
            "| epoch   5 |   350/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |   400/ 1600 steps | loss 0.23200 | ppl    1.261\n",
            "| epoch   5 |   450/ 1600 steps | loss 0.08419 | ppl    1.088\n",
            "| epoch   5 |   500/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   5 |   550/ 1600 steps | loss 0.20381 | ppl    1.226\n",
            "| epoch   5 |   600/ 1600 steps | loss 0.00010 | ppl    1.000\n",
            "| epoch   5 |   650/ 1600 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch   5 |   700/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   5 |   750/ 1600 steps | loss 0.00016 | ppl    1.000\n",
            "| epoch   5 |   800/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   5 |   850/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   900/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   950/ 1600 steps | loss 0.18506 | ppl    1.203\n",
            "| epoch   5 |  1000/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |  1050/ 1600 steps | loss 0.00181 | ppl    1.002\n",
            "| epoch   5 |  1100/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1150/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1200/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1250/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1300/ 1600 steps | loss 0.14746 | ppl    1.159\n",
            "| epoch   5 |  1350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1400/ 1600 steps | loss 0.24048 | ppl    1.272\n",
            "| epoch   5 |  1450/ 1600 steps | loss 0.40202 | ppl    1.495\n",
            "| epoch   5 |  1500/ 1600 steps | loss 0.09575 | ppl    1.100\n",
            "| epoch   5 |  1550/ 1600 steps | loss 0.31654 | ppl    1.372\n",
            "from_scratch=True - VALIDATION ACCURACY 0.6925\n",
            "| epoch   6 |    50/ 1600 steps | loss 0.00016 | ppl    1.000\n",
            "| epoch   6 |   100/ 1600 steps | loss 0.40454 | ppl    1.499\n",
            "| epoch   6 |   150/ 1600 steps | loss 0.34228 | ppl    1.408\n",
            "| epoch   6 |   200/ 1600 steps | loss 0.00018 | ppl    1.000\n",
            "| epoch   6 |   250/ 1600 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch   6 |   300/ 1600 steps | loss 0.04230 | ppl    1.043\n",
            "| epoch   6 |   350/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   6 |   400/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   6 |   450/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   6 |   500/ 1600 steps | loss 0.11989 | ppl    1.127\n",
            "| epoch   6 |   550/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   6 |   600/ 1600 steps | loss 0.14454 | ppl    1.156\n",
            "| epoch   6 |   650/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |   700/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |   750/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |   800/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |   850/ 1600 steps | loss 0.19131 | ppl    1.211\n",
            "| epoch   6 |   900/ 1600 steps | loss 0.00020 | ppl    1.000\n",
            "| epoch   6 |   950/ 1600 steps | loss 0.00012 | ppl    1.000\n",
            "| epoch   6 |  1000/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |  1050/ 1600 steps | loss 0.20719 | ppl    1.230\n",
            "| epoch   6 |  1100/ 1600 steps | loss 0.12093 | ppl    1.129\n",
            "| epoch   6 |  1150/ 1600 steps | loss 0.00035 | ppl    1.000\n",
            "| epoch   6 |  1200/ 1600 steps | loss 0.24181 | ppl    1.274\n",
            "| epoch   6 |  1250/ 1600 steps | loss 0.14811 | ppl    1.160\n",
            "| epoch   6 |  1300/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   6 |  1350/ 1600 steps | loss 0.00021 | ppl    1.000\n",
            "| epoch   6 |  1400/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   6 |  1450/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   6 |  1500/ 1600 steps | loss 0.43777 | ppl    1.549\n",
            "| epoch   6 |  1550/ 1600 steps | loss 0.23578 | ppl    1.266\n",
            "from_scratch=True - VALIDATION ACCURACY 0.73\n",
            "| epoch   7 |    50/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   7 |   100/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   7 |   150/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   200/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   250/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   300/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   400/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   650/ 1600 steps | loss 0.15106 | ppl    1.163\n",
            "| epoch   7 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   750/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   850/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   950/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |  1000/ 1600 steps | loss 0.48727 | ppl    1.628\n",
            "| epoch   7 |  1050/ 1600 steps | loss 0.16416 | ppl    1.178\n",
            "| epoch   7 |  1100/ 1600 steps | loss 0.64638 | ppl    1.909\n",
            "| epoch   7 |  1150/ 1600 steps | loss 0.00063 | ppl    1.001\n",
            "| epoch   7 |  1200/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   7 |  1250/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   7 |  1300/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |  1350/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |  1400/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |  1450/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |  1500/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |  1550/ 1600 steps | loss 0.28486 | ppl    1.330\n",
            "from_scratch=True - VALIDATION ACCURACY 0.732\n",
            "| epoch   8 |    50/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |   100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   350/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   400/ 1600 steps | loss 0.30637 | ppl    1.358\n",
            "| epoch   8 |   450/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   750/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   850/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1000/ 1600 steps | loss 0.05278 | ppl    1.054\n",
            "| epoch   8 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1100/ 1600 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch   8 |  1150/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   8 |  1200/ 1600 steps | loss 0.22779 | ppl    1.256\n",
            "| epoch   8 |  1250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1300/ 1600 steps | loss 0.36604 | ppl    1.442\n",
            "| epoch   8 |  1350/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1450/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |  1500/ 1600 steps | loss 0.12114 | ppl    1.129\n",
            "| epoch   8 |  1550/ 1600 steps | loss 0.15117 | ppl    1.163\n",
            "from_scratch=True - VALIDATION ACCURACY 0.666\n",
            "| epoch   9 |    50/ 1600 steps | loss 0.18140 | ppl    1.199\n",
            "| epoch   9 |   100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   300/ 1600 steps | loss 0.12020 | ppl    1.128\n",
            "| epoch   9 |   350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   650/ 1600 steps | loss 0.00310 | ppl    1.003\n",
            "| epoch   9 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   800/ 1600 steps | loss 0.16200 | ppl    1.176\n",
            "| epoch   9 |   850/ 1600 steps | loss 0.05523 | ppl    1.057\n",
            "| epoch   9 |   900/ 1600 steps | loss 0.04325 | ppl    1.044\n",
            "| epoch   9 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1000/ 1600 steps | loss 0.24017 | ppl    1.271\n",
            "| epoch   9 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1450/ 1600 steps | loss 0.01010 | ppl    1.010\n",
            "| epoch   9 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7405\n",
            "| epoch  10 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   350/ 1600 steps | loss 0.15550 | ppl    1.168\n",
            "| epoch  10 |   400/ 1600 steps | loss 0.00016 | ppl    1.000\n",
            "| epoch  10 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   650/ 1600 steps | loss 0.33450 | ppl    1.397\n",
            "| epoch  10 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   750/ 1600 steps | loss 0.04100 | ppl    1.042\n",
            "| epoch  10 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   950/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch  10 |  1000/ 1600 steps | loss 0.18471 | ppl    1.203\n",
            "| epoch  10 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1300/ 1600 steps | loss 0.09645 | ppl    1.101\n",
            "| epoch  10 |  1350/ 1600 steps | loss 0.18445 | ppl    1.203\n",
            "| epoch  10 |  1400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1450/ 1600 steps | loss 0.14156 | ppl    1.152\n",
            "| epoch  10 |  1500/ 1600 steps | loss 0.21155 | ppl    1.236\n",
            "| epoch  10 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.715\n",
            "| epoch  11 |    50/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1100/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch  11 |  1150/ 1600 steps | loss 0.24405 | ppl    1.276\n",
            "| epoch  11 |  1200/ 1600 steps | loss 0.18906 | ppl    1.208\n",
            "| epoch  11 |  1250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1350/ 1600 steps | loss 0.06020 | ppl    1.062\n",
            "| epoch  11 |  1400/ 1600 steps | loss 0.01619 | ppl    1.016\n",
            "| epoch  11 |  1450/ 1600 steps | loss 0.65648 | ppl    1.928\n",
            "| epoch  11 |  1500/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch  11 |  1550/ 1600 steps | loss 0.00699 | ppl    1.007\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7475\n",
            "| epoch  12 |    50/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  12 |   100/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  12 |   150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   350/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   400/ 1600 steps | loss 0.00010 | ppl    1.000\n",
            "| epoch  12 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   550/ 1600 steps | loss 0.29546 | ppl    1.344\n",
            "| epoch  12 |   600/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  12 |   650/ 1600 steps | loss 0.06442 | ppl    1.067\n",
            "| epoch  12 |   700/ 1600 steps | loss 0.08036 | ppl    1.084\n",
            "| epoch  12 |   750/ 1600 steps | loss 0.01719 | ppl    1.017\n",
            "| epoch  12 |   800/ 1600 steps | loss 0.13549 | ppl    1.145\n",
            "| epoch  12 |   850/ 1600 steps | loss 0.00540 | ppl    1.005\n",
            "| epoch  12 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1000/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  12 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1200/ 1600 steps | loss 0.07775 | ppl    1.081\n",
            "| epoch  12 |  1250/ 1600 steps | loss 0.18112 | ppl    1.199\n",
            "| epoch  12 |  1300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.738\n",
            "| epoch  13 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1350/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  13 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.753\n",
            "| epoch  14 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.75\n",
            "| epoch  15 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7505\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/ 1600 steps | loss 2.71881 | ppl   15.162\n",
            "| epoch   1 |   100/ 1600 steps | loss 1.25289 | ppl    3.500\n",
            "| epoch   1 |   150/ 1600 steps | loss 1.74467 | ppl    5.724\n",
            "| epoch   1 |   200/ 1600 steps | loss 1.65929 | ppl    5.256\n",
            "| epoch   1 |   250/ 1600 steps | loss 1.30780 | ppl    3.698\n",
            "| epoch   1 |   300/ 1600 steps | loss 1.01788 | ppl    2.767\n",
            "| epoch   1 |   350/ 1600 steps | loss 1.27821 | ppl    3.590\n",
            "| epoch   1 |   400/ 1600 steps | loss 1.29064 | ppl    3.635\n",
            "| epoch   1 |   450/ 1600 steps | loss 1.15958 | ppl    3.189\n",
            "| epoch   1 |   500/ 1600 steps | loss 0.71722 | ppl    2.049\n",
            "| epoch   1 |   550/ 1600 steps | loss 0.98508 | ppl    2.678\n",
            "| epoch   1 |   600/ 1600 steps | loss 0.72585 | ppl    2.066\n",
            "| epoch   1 |   650/ 1600 steps | loss 1.50067 | ppl    4.485\n",
            "| epoch   1 |   700/ 1600 steps | loss 1.89259 | ppl    6.637\n",
            "| epoch   1 |   750/ 1600 steps | loss 0.73233 | ppl    2.080\n",
            "| epoch   1 |   800/ 1600 steps | loss 0.94550 | ppl    2.574\n",
            "| epoch   1 |   850/ 1600 steps | loss 1.00434 | ppl    2.730\n",
            "| epoch   1 |   900/ 1600 steps | loss 1.30444 | ppl    3.686\n",
            "| epoch   1 |   950/ 1600 steps | loss 0.81870 | ppl    2.268\n",
            "| epoch   1 |  1000/ 1600 steps | loss 0.94788 | ppl    2.580\n",
            "| epoch   1 |  1050/ 1600 steps | loss 1.44969 | ppl    4.262\n",
            "| epoch   1 |  1100/ 1600 steps | loss 0.93656 | ppl    2.551\n",
            "| epoch   1 |  1150/ 1600 steps | loss 1.87523 | ppl    6.522\n",
            "| epoch   1 |  1200/ 1600 steps | loss 1.43954 | ppl    4.219\n",
            "| epoch   1 |  1250/ 1600 steps | loss 1.00450 | ppl    2.731\n",
            "| epoch   1 |  1300/ 1600 steps | loss 1.53006 | ppl    4.618\n",
            "| epoch   1 |  1350/ 1600 steps | loss 1.41914 | ppl    4.134\n",
            "| epoch   1 |  1400/ 1600 steps | loss 1.78471 | ppl    5.958\n",
            "| epoch   1 |  1450/ 1600 steps | loss 0.98482 | ppl    2.677\n",
            "| epoch   1 |  1500/ 1600 steps | loss 0.56103 | ppl    1.752\n",
            "| epoch   1 |  1550/ 1600 steps | loss 0.92914 | ppl    2.532\n",
            "from_scratch=False - VALIDATION ACCURACY 0.755\n",
            "| epoch   2 |    50/ 1600 steps | loss 0.35081 | ppl    1.420\n",
            "| epoch   2 |   100/ 1600 steps | loss 0.64518 | ppl    1.906\n",
            "| epoch   2 |   150/ 1600 steps | loss 0.38713 | ppl    1.473\n",
            "| epoch   2 |   200/ 1600 steps | loss 0.72491 | ppl    2.065\n",
            "| epoch   2 |   250/ 1600 steps | loss 0.62415 | ppl    1.867\n",
            "| epoch   2 |   300/ 1600 steps | loss 0.69415 | ppl    2.002\n",
            "| epoch   2 |   350/ 1600 steps | loss 1.14606 | ppl    3.146\n",
            "| epoch   2 |   400/ 1600 steps | loss 0.42133 | ppl    1.524\n",
            "| epoch   2 |   450/ 1600 steps | loss 0.68587 | ppl    1.985\n",
            "| epoch   2 |   500/ 1600 steps | loss 0.61652 | ppl    1.852\n",
            "| epoch   2 |   550/ 1600 steps | loss 0.60642 | ppl    1.834\n",
            "| epoch   2 |   600/ 1600 steps | loss 0.80675 | ppl    2.241\n",
            "| epoch   2 |   650/ 1600 steps | loss 0.08188 | ppl    1.085\n",
            "| epoch   2 |   700/ 1600 steps | loss 0.94698 | ppl    2.578\n",
            "| epoch   2 |   750/ 1600 steps | loss 1.55982 | ppl    4.758\n",
            "| epoch   2 |   800/ 1600 steps | loss 0.55740 | ppl    1.746\n",
            "| epoch   2 |   850/ 1600 steps | loss 0.71227 | ppl    2.039\n",
            "| epoch   2 |   900/ 1600 steps | loss 0.58478 | ppl    1.795\n",
            "| epoch   2 |   950/ 1600 steps | loss 0.78617 | ppl    2.195\n",
            "| epoch   2 |  1000/ 1600 steps | loss 0.76329 | ppl    2.145\n",
            "| epoch   2 |  1050/ 1600 steps | loss 0.84736 | ppl    2.333\n",
            "| epoch   2 |  1100/ 1600 steps | loss 0.52431 | ppl    1.689\n",
            "| epoch   2 |  1150/ 1600 steps | loss 0.40571 | ppl    1.500\n",
            "| epoch   2 |  1200/ 1600 steps | loss 1.11610 | ppl    3.053\n",
            "| epoch   2 |  1250/ 1600 steps | loss 0.68141 | ppl    1.977\n",
            "| epoch   2 |  1300/ 1600 steps | loss 0.31315 | ppl    1.368\n",
            "| epoch   2 |  1350/ 1600 steps | loss 0.65768 | ppl    1.930\n",
            "| epoch   2 |  1400/ 1600 steps | loss 1.30245 | ppl    3.678\n",
            "| epoch   2 |  1450/ 1600 steps | loss 0.27716 | ppl    1.319\n",
            "| epoch   2 |  1500/ 1600 steps | loss 0.43001 | ppl    1.537\n",
            "| epoch   2 |  1550/ 1600 steps | loss 1.32077 | ppl    3.746\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7485\n",
            "| epoch   3 |    50/ 1600 steps | loss 0.31749 | ppl    1.374\n",
            "| epoch   3 |   100/ 1600 steps | loss 0.37663 | ppl    1.457\n",
            "| epoch   3 |   150/ 1600 steps | loss 0.00093 | ppl    1.001\n",
            "| epoch   3 |   200/ 1600 steps | loss 0.57475 | ppl    1.777\n",
            "| epoch   3 |   250/ 1600 steps | loss 0.47108 | ppl    1.602\n",
            "| epoch   3 |   300/ 1600 steps | loss 0.31983 | ppl    1.377\n",
            "| epoch   3 |   350/ 1600 steps | loss 0.16060 | ppl    1.174\n",
            "| epoch   3 |   400/ 1600 steps | loss 0.66188 | ppl    1.938\n",
            "| epoch   3 |   450/ 1600 steps | loss 0.20194 | ppl    1.224\n",
            "| epoch   3 |   500/ 1600 steps | loss 0.30270 | ppl    1.354\n",
            "| epoch   3 |   550/ 1600 steps | loss 0.48535 | ppl    1.625\n",
            "| epoch   3 |   600/ 1600 steps | loss 0.25634 | ppl    1.292\n",
            "| epoch   3 |   650/ 1600 steps | loss 0.38197 | ppl    1.465\n",
            "| epoch   3 |   700/ 1600 steps | loss 0.84009 | ppl    2.317\n",
            "| epoch   3 |   750/ 1600 steps | loss 0.09429 | ppl    1.099\n",
            "| epoch   3 |   800/ 1600 steps | loss 0.10829 | ppl    1.114\n",
            "| epoch   3 |   850/ 1600 steps | loss 0.43598 | ppl    1.546\n",
            "| epoch   3 |   900/ 1600 steps | loss 0.19196 | ppl    1.212\n",
            "| epoch   3 |   950/ 1600 steps | loss 0.39989 | ppl    1.492\n",
            "| epoch   3 |  1000/ 1600 steps | loss 0.46227 | ppl    1.588\n",
            "| epoch   3 |  1050/ 1600 steps | loss 0.42946 | ppl    1.536\n",
            "| epoch   3 |  1100/ 1600 steps | loss 0.26893 | ppl    1.309\n",
            "| epoch   3 |  1150/ 1600 steps | loss 0.09058 | ppl    1.095\n",
            "| epoch   3 |  1200/ 1600 steps | loss 0.21039 | ppl    1.234\n",
            "| epoch   3 |  1250/ 1600 steps | loss 0.00090 | ppl    1.001\n",
            "| epoch   3 |  1300/ 1600 steps | loss 0.36849 | ppl    1.446\n",
            "| epoch   3 |  1350/ 1600 steps | loss 0.23400 | ppl    1.264\n",
            "| epoch   3 |  1400/ 1600 steps | loss 0.43520 | ppl    1.545\n",
            "| epoch   3 |  1450/ 1600 steps | loss 0.30182 | ppl    1.352\n",
            "| epoch   3 |  1500/ 1600 steps | loss 0.00821 | ppl    1.008\n",
            "| epoch   3 |  1550/ 1600 steps | loss 0.00535 | ppl    1.005\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7675\n",
            "| epoch   4 |    50/ 1600 steps | loss 0.01188 | ppl    1.012\n",
            "| epoch   4 |   100/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   4 |   150/ 1600 steps | loss 0.15616 | ppl    1.169\n",
            "| epoch   4 |   200/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   4 |   250/ 1600 steps | loss 0.00258 | ppl    1.003\n",
            "| epoch   4 |   300/ 1600 steps | loss 0.23046 | ppl    1.259\n",
            "| epoch   4 |   350/ 1600 steps | loss 0.02636 | ppl    1.027\n",
            "| epoch   4 |   400/ 1600 steps | loss 0.21446 | ppl    1.239\n",
            "| epoch   4 |   450/ 1600 steps | loss 0.00059 | ppl    1.001\n",
            "| epoch   4 |   500/ 1600 steps | loss 0.06399 | ppl    1.066\n",
            "| epoch   4 |   550/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   4 |   600/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   4 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   4 |   700/ 1600 steps | loss 0.40384 | ppl    1.498\n",
            "| epoch   4 |   750/ 1600 steps | loss 0.09013 | ppl    1.094\n",
            "| epoch   4 |   800/ 1600 steps | loss 0.22216 | ppl    1.249\n",
            "| epoch   4 |   850/ 1600 steps | loss 0.00295 | ppl    1.003\n",
            "| epoch   4 |   900/ 1600 steps | loss 0.01723 | ppl    1.017\n",
            "| epoch   4 |   950/ 1600 steps | loss 0.00024 | ppl    1.000\n",
            "| epoch   4 |  1000/ 1600 steps | loss 0.14095 | ppl    1.151\n",
            "| epoch   4 |  1050/ 1600 steps | loss 0.00347 | ppl    1.003\n",
            "| epoch   4 |  1100/ 1600 steps | loss 0.01703 | ppl    1.017\n",
            "| epoch   4 |  1150/ 1600 steps | loss 0.22274 | ppl    1.249\n",
            "| epoch   4 |  1200/ 1600 steps | loss 0.47852 | ppl    1.614\n",
            "| epoch   4 |  1250/ 1600 steps | loss 0.25984 | ppl    1.297\n",
            "| epoch   4 |  1300/ 1600 steps | loss 0.00775 | ppl    1.008\n",
            "| epoch   4 |  1350/ 1600 steps | loss 0.09871 | ppl    1.104\n",
            "| epoch   4 |  1400/ 1600 steps | loss 0.09981 | ppl    1.105\n",
            "| epoch   4 |  1450/ 1600 steps | loss 0.03654 | ppl    1.037\n",
            "| epoch   4 |  1500/ 1600 steps | loss 0.22897 | ppl    1.257\n",
            "| epoch   4 |  1550/ 1600 steps | loss 0.16002 | ppl    1.174\n",
            "from_scratch=False - VALIDATION ACCURACY 0.724\n",
            "| epoch   5 |    50/ 1600 steps | loss 0.19570 | ppl    1.216\n",
            "| epoch   5 |   100/ 1600 steps | loss 0.00018 | ppl    1.000\n",
            "| epoch   5 |   150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |   350/ 1600 steps | loss 0.13789 | ppl    1.148\n",
            "| epoch   5 |   400/ 1600 steps | loss 0.14837 | ppl    1.160\n",
            "| epoch   5 |   450/ 1600 steps | loss 0.13460 | ppl    1.144\n",
            "| epoch   5 |   500/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   5 |   550/ 1600 steps | loss 0.27330 | ppl    1.314\n",
            "| epoch   5 |   600/ 1600 steps | loss 0.20933 | ppl    1.233\n",
            "| epoch   5 |   650/ 1600 steps | loss 0.19616 | ppl    1.217\n",
            "| epoch   5 |   700/ 1600 steps | loss 0.26774 | ppl    1.307\n",
            "| epoch   5 |   750/ 1600 steps | loss 0.26190 | ppl    1.299\n",
            "| epoch   5 |   800/ 1600 steps | loss 0.01486 | ppl    1.015\n",
            "| epoch   5 |   850/ 1600 steps | loss 0.00173 | ppl    1.002\n",
            "| epoch   5 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |   950/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |  1000/ 1600 steps | loss 0.00313 | ppl    1.003\n",
            "| epoch   5 |  1050/ 1600 steps | loss 0.00023 | ppl    1.000\n",
            "| epoch   5 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   5 |  1150/ 1600 steps | loss 0.15084 | ppl    1.163\n",
            "| epoch   5 |  1200/ 1600 steps | loss 0.00745 | ppl    1.007\n",
            "| epoch   5 |  1250/ 1600 steps | loss 0.22826 | ppl    1.256\n",
            "| epoch   5 |  1300/ 1600 steps | loss 0.05647 | ppl    1.058\n",
            "| epoch   5 |  1350/ 1600 steps | loss 0.05892 | ppl    1.061\n",
            "| epoch   5 |  1400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |  1450/ 1600 steps | loss 0.00470 | ppl    1.005\n",
            "| epoch   5 |  1500/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   5 |  1550/ 1600 steps | loss 0.06094 | ppl    1.063\n",
            "from_scratch=False - VALIDATION ACCURACY 0.77\n",
            "| epoch   6 |    50/ 1600 steps | loss 0.01657 | ppl    1.017\n",
            "| epoch   6 |   100/ 1600 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch   6 |   150/ 1600 steps | loss 0.17376 | ppl    1.190\n",
            "| epoch   6 |   200/ 1600 steps | loss 0.07458 | ppl    1.077\n",
            "| epoch   6 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   350/ 1600 steps | loss 0.13943 | ppl    1.150\n",
            "| epoch   6 |   400/ 1600 steps | loss 0.22289 | ppl    1.250\n",
            "| epoch   6 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   500/ 1600 steps | loss 0.15033 | ppl    1.162\n",
            "| epoch   6 |   550/ 1600 steps | loss 0.01711 | ppl    1.017\n",
            "| epoch   6 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   650/ 1600 steps | loss 0.00286 | ppl    1.003\n",
            "| epoch   6 |   700/ 1600 steps | loss 0.00010 | ppl    1.000\n",
            "| epoch   6 |   750/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   6 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1000/ 1600 steps | loss 0.02126 | ppl    1.021\n",
            "| epoch   6 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1150/ 1600 steps | loss 0.04196 | ppl    1.043\n",
            "| epoch   6 |  1200/ 1600 steps | loss 0.01483 | ppl    1.015\n",
            "| epoch   6 |  1250/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1400/ 1600 steps | loss 0.05338 | ppl    1.055\n",
            "| epoch   6 |  1450/ 1600 steps | loss 0.15891 | ppl    1.172\n",
            "| epoch   6 |  1500/ 1600 steps | loss 0.00454 | ppl    1.005\n",
            "| epoch   6 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7665\n",
            "| epoch   7 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   150/ 1600 steps | loss 0.04464 | ppl    1.046\n",
            "| epoch   7 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   250/ 1600 steps | loss 0.31764 | ppl    1.374\n",
            "| epoch   7 |   300/ 1600 steps | loss 0.23244 | ppl    1.262\n",
            "| epoch   7 |   350/ 1600 steps | loss 0.00026 | ppl    1.000\n",
            "| epoch   7 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   450/ 1600 steps | loss 0.00011 | ppl    1.000\n",
            "| epoch   7 |   500/ 1600 steps | loss 0.07822 | ppl    1.081\n",
            "| epoch   7 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   650/ 1600 steps | loss 0.08198 | ppl    1.085\n",
            "| epoch   7 |   700/ 1600 steps | loss 0.20165 | ppl    1.223\n",
            "| epoch   7 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   800/ 1600 steps | loss 0.04312 | ppl    1.044\n",
            "| epoch   7 |   850/ 1600 steps | loss 0.00010 | ppl    1.000\n",
            "| epoch   7 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   950/ 1600 steps | loss 0.26433 | ppl    1.303\n",
            "| epoch   7 |  1000/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |  1150/ 1600 steps | loss 0.33538 | ppl    1.398\n",
            "| epoch   7 |  1200/ 1600 steps | loss 0.01412 | ppl    1.014\n",
            "| epoch   7 |  1250/ 1600 steps | loss 0.09708 | ppl    1.102\n",
            "| epoch   7 |  1300/ 1600 steps | loss 0.00074 | ppl    1.001\n",
            "| epoch   7 |  1350/ 1600 steps | loss 0.00121 | ppl    1.001\n",
            "| epoch   7 |  1400/ 1600 steps | loss 0.49960 | ppl    1.648\n",
            "| epoch   7 |  1450/ 1600 steps | loss 0.18030 | ppl    1.198\n",
            "| epoch   7 |  1500/ 1600 steps | loss 0.00553 | ppl    1.006\n",
            "| epoch   7 |  1550/ 1600 steps | loss 0.22017 | ppl    1.246\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7875\n",
            "| epoch   8 |    50/ 1600 steps | loss 0.09966 | ppl    1.105\n",
            "| epoch   8 |   100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   150/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   8 |   200/ 1600 steps | loss 0.30866 | ppl    1.362\n",
            "| epoch   8 |   250/ 1600 steps | loss 0.40746 | ppl    1.503\n",
            "| epoch   8 |   300/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   8 |   350/ 1600 steps | loss 0.01471 | ppl    1.015\n",
            "| epoch   8 |   400/ 1600 steps | loss 0.00047 | ppl    1.000\n",
            "| epoch   8 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   500/ 1600 steps | loss 0.27838 | ppl    1.321\n",
            "| epoch   8 |   550/ 1600 steps | loss 0.00142 | ppl    1.001\n",
            "| epoch   8 |   600/ 1600 steps | loss 0.50573 | ppl    1.658\n",
            "| epoch   8 |   650/ 1600 steps | loss 0.37768 | ppl    1.459\n",
            "| epoch   8 |   700/ 1600 steps | loss 0.21934 | ppl    1.245\n",
            "| epoch   8 |   750/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   850/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   8 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |   950/ 1600 steps | loss 0.13544 | ppl    1.145\n",
            "| epoch   8 |  1000/ 1600 steps | loss 0.23464 | ppl    1.264\n",
            "| epoch   8 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1150/ 1600 steps | loss 0.08296 | ppl    1.086\n",
            "| epoch   8 |  1200/ 1600 steps | loss 0.27052 | ppl    1.311\n",
            "| epoch   8 |  1250/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   8 |  1300/ 1600 steps | loss 0.00846 | ppl    1.008\n",
            "| epoch   8 |  1350/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1400/ 1600 steps | loss 0.04196 | ppl    1.043\n",
            "| epoch   8 |  1450/ 1600 steps | loss 0.00020 | ppl    1.000\n",
            "| epoch   8 |  1500/ 1600 steps | loss 0.00012 | ppl    1.000\n",
            "| epoch   8 |  1550/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7775\n",
            "| epoch   9 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   500/ 1600 steps | loss 0.00040 | ppl    1.000\n",
            "| epoch   9 |   550/ 1600 steps | loss 0.21102 | ppl    1.235\n",
            "| epoch   9 |   600/ 1600 steps | loss 0.00042 | ppl    1.000\n",
            "| epoch   9 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |   700/ 1600 steps | loss 0.75670 | ppl    2.131\n",
            "| epoch   9 |   750/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   850/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   950/ 1600 steps | loss 0.22817 | ppl    1.256\n",
            "| epoch   9 |  1000/ 1600 steps | loss 0.11687 | ppl    1.124\n",
            "| epoch   9 |  1050/ 1600 steps | loss 0.28666 | ppl    1.332\n",
            "| epoch   9 |  1100/ 1600 steps | loss 0.17185 | ppl    1.188\n",
            "| epoch   9 |  1150/ 1600 steps | loss 0.38518 | ppl    1.470\n",
            "| epoch   9 |  1200/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |  1250/ 1600 steps | loss 0.24016 | ppl    1.271\n",
            "| epoch   9 |  1300/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   9 |  1350/ 1600 steps | loss 0.02175 | ppl    1.022\n",
            "| epoch   9 |  1400/ 1600 steps | loss 0.00026 | ppl    1.000\n",
            "| epoch   9 |  1450/ 1600 steps | loss 0.00066 | ppl    1.001\n",
            "| epoch   9 |  1500/ 1600 steps | loss 0.44257 | ppl    1.557\n",
            "| epoch   9 |  1550/ 1600 steps | loss 0.00038 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7765\n",
            "| epoch  10 |    50/ 1600 steps | loss 0.18406 | ppl    1.202\n",
            "| epoch  10 |   100/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  10 |   150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   200/ 1600 steps | loss 0.13441 | ppl    1.144\n",
            "| epoch  10 |   250/ 1600 steps | loss 0.00013 | ppl    1.000\n",
            "| epoch  10 |   300/ 1600 steps | loss 0.05456 | ppl    1.056\n",
            "| epoch  10 |   350/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch  10 |   400/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch  10 |   450/ 1600 steps | loss 0.00014 | ppl    1.000\n",
            "| epoch  10 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   550/ 1600 steps | loss 0.04690 | ppl    1.048\n",
            "| epoch  10 |   600/ 1600 steps | loss 0.22335 | ppl    1.250\n",
            "| epoch  10 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   700/ 1600 steps | loss 0.18742 | ppl    1.206\n",
            "| epoch  10 |   750/ 1600 steps | loss 0.29698 | ppl    1.346\n",
            "| epoch  10 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   850/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1500/ 1600 steps | loss 0.04847 | ppl    1.050\n",
            "| epoch  10 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7975\n",
            "| epoch  11 |    50/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  11 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   150/ 1600 steps | loss 0.06097 | ppl    1.063\n",
            "| epoch  11 |   200/ 1600 steps | loss 0.33792 | ppl    1.402\n",
            "| epoch  11 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   300/ 1600 steps | loss 0.07302 | ppl    1.076\n",
            "| epoch  11 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   600/ 1600 steps | loss 0.22814 | ppl    1.256\n",
            "| epoch  11 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1050/ 1600 steps | loss 0.12872 | ppl    1.137\n",
            "| epoch  11 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1150/ 1600 steps | loss 0.00231 | ppl    1.002\n",
            "| epoch  11 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1500/ 1600 steps | loss 0.00098 | ppl    1.001\n",
            "| epoch  11 |  1550/ 1600 steps | loss 0.11816 | ppl    1.125\n",
            "from_scratch=False - VALIDATION ACCURACY 0.763\n",
            "| epoch  12 |    50/ 1600 steps | loss 0.00260 | ppl    1.003\n",
            "| epoch  12 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   150/ 1600 steps | loss 0.22941 | ppl    1.258\n",
            "| epoch  12 |   200/ 1600 steps | loss 0.30563 | ppl    1.357\n",
            "| epoch  12 |   250/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  12 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   350/ 1600 steps | loss 0.15475 | ppl    1.167\n",
            "| epoch  12 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   500/ 1600 steps | loss 0.00060 | ppl    1.001\n",
            "| epoch  12 |   550/ 1600 steps | loss 0.00100 | ppl    1.001\n",
            "| epoch  12 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   700/ 1600 steps | loss 0.00194 | ppl    1.002\n",
            "| epoch  12 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   850/ 1600 steps | loss 0.08927 | ppl    1.093\n",
            "| epoch  12 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1100/ 1600 steps | loss 0.09188 | ppl    1.096\n",
            "| epoch  12 |  1150/ 1600 steps | loss 0.11212 | ppl    1.119\n",
            "| epoch  12 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1350/ 1600 steps | loss 0.54976 | ppl    1.733\n",
            "| epoch  12 |  1400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |  1450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1550/ 1600 steps | loss 0.28566 | ppl    1.331\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7795\n",
            "| epoch  13 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   100/ 1600 steps | loss 0.07114 | ppl    1.074\n",
            "| epoch  13 |   150/ 1600 steps | loss 0.00205 | ppl    1.002\n",
            "| epoch  13 |   200/ 1600 steps | loss 0.31836 | ppl    1.375\n",
            "| epoch  13 |   250/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  13 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   400/ 1600 steps | loss 0.30152 | ppl    1.352\n",
            "| epoch  13 |   450/ 1600 steps | loss 0.18503 | ppl    1.203\n",
            "| epoch  13 |   500/ 1600 steps | loss 0.00060 | ppl    1.001\n",
            "| epoch  13 |   550/ 1600 steps | loss 0.00019 | ppl    1.000\n",
            "| epoch  13 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   800/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  13 |   850/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch  13 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1000/ 1600 steps | loss 0.18205 | ppl    1.200\n",
            "| epoch  13 |  1050/ 1600 steps | loss 0.00025 | ppl    1.000\n",
            "| epoch  13 |  1100/ 1600 steps | loss 0.39312 | ppl    1.482\n",
            "| epoch  13 |  1150/ 1600 steps | loss 0.32088 | ppl    1.378\n",
            "| epoch  13 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |  1300/ 1600 steps | loss 0.20530 | ppl    1.228\n",
            "| epoch  13 |  1350/ 1600 steps | loss 0.12228 | ppl    1.130\n",
            "| epoch  13 |  1400/ 1600 steps | loss 0.18699 | ppl    1.206\n",
            "| epoch  13 |  1450/ 1600 steps | loss 0.16279 | ppl    1.177\n",
            "| epoch  13 |  1500/ 1600 steps | loss 0.18193 | ppl    1.200\n",
            "| epoch  13 |  1550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.782\n",
            "| epoch  14 |    50/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   300/ 1600 steps | loss 0.00090 | ppl    1.001\n",
            "| epoch  14 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   400/ 1600 steps | loss 0.00138 | ppl    1.001\n",
            "| epoch  14 |   450/ 1600 steps | loss 0.10553 | ppl    1.111\n",
            "| epoch  14 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1050/ 1600 steps | loss 0.09546 | ppl    1.100\n",
            "| epoch  14 |  1100/ 1600 steps | loss 0.25527 | ppl    1.291\n",
            "| epoch  14 |  1150/ 1600 steps | loss 0.03145 | ppl    1.032\n",
            "| epoch  14 |  1200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1300/ 1600 steps | loss 0.02169 | ppl    1.022\n",
            "| epoch  14 |  1350/ 1600 steps | loss 0.00225 | ppl    1.002\n",
            "| epoch  14 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1450/ 1600 steps | loss 0.57441 | ppl    1.776\n",
            "| epoch  14 |  1500/ 1600 steps | loss 0.14143 | ppl    1.152\n",
            "| epoch  14 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.8035\n",
            "| epoch  15 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/ 1600 steps | loss 0.00025 | ppl    1.000\n",
            "| epoch  15 |   200/ 1600 steps | loss 0.15020 | ppl    1.162\n",
            "| epoch  15 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   300/ 1600 steps | loss 0.07535 | ppl    1.078\n",
            "| epoch  15 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   450/ 1600 steps | loss 0.28050 | ppl    1.324\n",
            "| epoch  15 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   800/ 1600 steps | loss 0.03546 | ppl    1.036\n",
            "| epoch  15 |   850/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch  15 |   900/ 1600 steps | loss 0.00060 | ppl    1.001\n",
            "| epoch  15 |   950/ 1600 steps | loss 0.25439 | ppl    1.290\n",
            "| epoch  15 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1400/ 1600 steps | loss 0.22529 | ppl    1.253\n",
            "| epoch  15 |  1450/ 1600 steps | loss 0.15812 | ppl    1.171\n",
            "| epoch  15 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7535\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model_ds = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model_ds.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model_ds.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    # @TODO : freeze transformer weights!\n",
        "    # @TODO: check batch size subtlety on the last element (we want to classify the last feature of the sentence!)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            model_ds,\n",
        "            downstream_path_data_train,\n",
        "            downstream_path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task=DS_TASK,\n",
        "            # batch_size=8,\n",
        "            batch_size=1, # TO AVOID THE NEED TO RETRIEVE THE RIGHT LAST TOKEN IN A BATCH\n",
        "            log_interval=50,\n",
        "            epoch=epoch,\n",
        "            name=\"_from_scratch\" if from_scratch else \"_pretrained\"\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            data_loader_validation,\n",
        "            model_ds\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "        print(f\"{from_scratch=} - VALIDATION ACCURACY {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RCpBIdTHojm6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(100.*np.array(from_scratch_valid_acc), \"-+r\", label=\"training from scratch\")\n",
        "plt.plot(100.*np.array(pretrained_valid_acc), \"-+b\", label=\"pretrained model\")\n",
        "plt.grid()\n",
        "plt.title(\"Classifier accuracy on a downstream task [Sentiment analyzis]\")\n",
        "plt.legend()\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.15 ('altegrad')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f3cfdeab8dd8f9900bd16266619de191cf0f5e09365d74b1fba1714dce58066"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
