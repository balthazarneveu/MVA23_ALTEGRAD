{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 3: Transfer learning for NLP</h2> 24 / 10 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Balthazar Neveu\n",
        "\n",
        "</center>\n",
        "\n",
        "<br><br>\n",
        "In this lab we will:\n",
        "* Implement and pretrain a language model with transformer architecture.\n",
        "* Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.\n",
        "* Compare the performance of the pretrained model to a model trained from scratch.\n",
        " <br>\n",
        "\n",
        "<b>The deadline for this lab is October 31, 2023 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "!head -5 dict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_vocab = Path(\"dict.txt\")\n",
        "assert path_vocab.exists()\n",
        "pretraining_path_data_train = Path(\"pretraining_subset.txt\")\n",
        "assert pretraining_path_data_train.exists()\n",
        "\n",
        "downstream_path_data_train = Path(\"train.review.spm\")\n",
        "assert downstream_path_data_train.exists()\n",
        "downstream_path_labels_train = Path(\"train.label\")\n",
        "assert downstream_path_labels_train.exists()\n",
        "downstream_path_data_valid = Path(\"test.review.spm\")\n",
        "assert downstream_path_data_valid.exists()\n",
        "downstream_path_labels_valid = Path(\"test.label\")\n",
        "assert downstream_path_labels_valid.exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_model = Path(\"pretrained_model_4layers.pt\")\n",
        "assert pretrained_model.exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensor convention for NLP\n",
        "`[L, N, D]`\n",
        "- L sequence length\n",
        "- N batch size\n",
        "- V vocabulary dimension `ntokens`\n",
        "- E embeddings dimension `embedding_dim`\n",
        "- D hidden dimension\n",
        "\n",
        "### Simplification:\n",
        "- `E=D` hidden dimension set equal to th embedding dimension for simplicity in the following code `nhid = embedding_dim`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add fixed precomputed positional encoding to the embeddings\n",
        "    Add means (=literally addition)\n",
        "    \"\"\"\n",
        "    def __init__(self, embdeddings_dim: int , dropout: float =0.1, max_len: int =5000):\n",
        "        \"\"\"Precompute a positional encoding vector of length `max_len`\n",
        "\n",
        "        Args:\n",
        "            embdeddings_dim (int): dimension of word embeddings. Note th\n",
        "            dropout (float, optional): dropout ratio. Defaults to 0.1.\n",
        "            max_len (int, optional): maximum sequence length. Defaults to 5000.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, embdeddings_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embdeddings_dim, 2).float() * (-math.log(10000.0) / embdeddings_dim)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Add positional encoding to the word embeddings.\n",
        "        Simply add the pre\n",
        "\n",
        "        Args:\n",
        "            x (torch.FloatTensor): embeddings tensor [L, N, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: Enhanced embeddings tensor, ready to go straight to the transformer blocks. \n",
        "        \"\"\"\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Transformer base model \n",
        "    =========================\n",
        "    - embedding from word to vectors\n",
        "    - add positional encoding\n",
        "    - `nlayers` * transformer blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, ntokens:int, nhead:int, nhid:int, nlayers:int, dropout=0.5):\n",
        "        \"\"\"Transformer base model\n",
        "\n",
        "        Args:\n",
        "            ntokens (int): the size of vocabulary\n",
        "            nhead (int): number of heads in each of the MHA models\n",
        "            nhid (int): hidden dimension of the model. assume `embedding_dim` = `nhid`\n",
        "            nlayers (int): number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "            dropout (float, optional): dropout value. Defaults to 0.5.\n",
        "        \"\"\"\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = \"Transformer\"\n",
        "        embedding_dim = nhid # use the same embedding & hidden dimensions\n",
        "        self.encoder = nn.Embedding(ntokens, embedding_dim) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid, dropout=dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        \n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=nhid, # input dimension to the transformer encoder layer\n",
        "            nhead=nhead, # number of heads for MHA (Multi-head attention)\n",
        "            dim_feedforward=nhid, # output dimension of the MLP on top of the transformer.\n",
        "            dropout=dropout\n",
        "        ) # we assume nhid = d_model = dim_feedforward\n",
        "        \n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=nlayers\n",
        "        )\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(sz: int) -> torch.FloatTensor:\n",
        "        \"\"\"Generate causality mask = mask future tokens for next word prediction\n",
        "\n",
        "        Args:\n",
        "            sz (int): mask size M\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: squares matrix [M, M] to mask the attention matrix.\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(\n",
        "            self, src: torch.LongTensor,\n",
        "            src_mask: torch.FloatTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Embdeddings, positional encoders, go trough `nlayers` of residual {multi (`nhead`) attention heads + MLP}.\n",
        "\n",
        "        Args:\n",
        "            src (torch.LongTensor): [L, N, V] sequence of tokens , V=vocabu\n",
        "            src_mask (torch.FloatTensor): [L, L] squared mask\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: encoded sequence [L, N, D]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) #embed [L, N, V] -> [L, N, E]\n",
        "        src = self.pos_encoder(src) # [L, N, E]  - add positional encoding\n",
        "        output = self.transformer_encoder(src, mask=src_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid: int, nclasses: int):\n",
        "        \"\"\"Linear classification head -> returns logits (not probabilities)\n",
        "\n",
        "        Args:\n",
        "            nhid (int): hidden dimension\n",
        "            nclasses (int): number of classes.\n",
        "        \"\"\"\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Classify encoded feature vectors\n",
        "\n",
        "        Args:\n",
        "            src (torch.FloatTensor): Encoded feature vectors [L, N, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: Logits (no softmax applied)\n",
        "        \"\"\"\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken: int, nhead: int, nhid: int, nlayers: int, nclasses: int, dropout: float=0.5):\n",
        "        \"\"\"TransformerModel+ClassificationHead\n",
        "        \n",
        "        This allows defining a model for next word prediction (classification with ntoken classes)\n",
        "        Or other downstream tasks if the base `TransformerModel` is pretrained\n",
        "\n",
        "        Args:\n",
        "        \n",
        "            ntoken (int): size of vocabulary for (`TransformerModel`)\n",
        "            nhead (int): number of heads in each of the MHA models (`TransformerModel`)\n",
        "            nhid (int): hidden dimension of the model. assume `embedding_dim` = `nhid`\n",
        "            nlayers (int):  number of nn.TransformerEncoderLayer in nn.TransformerEncoder (`TransformerModel`)\n",
        "            nclasses (int): number of output classes in the classifier `ClassificationHead`\n",
        "                - =size of vocabulary for next word prediction\n",
        "                - other for downstream tasks like sentiment analyzis.\n",
        "            dropout (float, optional): _description_. Defaults to 0.5.  (`TransformerModel`)\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers, dropout=dropout)\n",
        "        self.classifier = ClassificationHead(nhid, nclasses)\n",
        "\n",
        "    def forward(self, src:torch.LongTensor, src_mask: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Encoder + linear classifier\n",
        "\n",
        "        Args:\n",
        "            src (torch.LongTensor): sequence of tokens [L, N, V]\n",
        "            src_mask (torch.FloatTensor): [L, L] squared mask.\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: [N, C]\n",
        "        \"\"\"\n",
        "        # base model\n",
        "        x = self.base(src, src_mask)\n",
        "        # classifier model\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Causal attention mask & useless computations (question 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 useless computations for a sequence of 5 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_length_test = 5\n",
        "src_mask = TransformerModel.generate_square_subsequent_mask(sentence_length_test)\n",
        "useless_computations = sentence_length_test*(sentence_length_test-1)//2\n",
        "assert int( ((-src_mask).isinf()).sum()) == useless_computations\n",
        "print(f\"{useless_computations} useless computations for a sequence of {sentence_length_test} tokens\")\n",
        "src_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unit test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rhb2gkUhJMR0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bneveu/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:255: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 100])\n"
          ]
        }
      ],
      "source": [
        "def test_transformer_based_classifier():\n",
        "    ntokens = 100 #  V the size of vocabulary\n",
        "    nhid = 200  # hidden dimension\n",
        "    nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    nhead = 2  # the number of heads in the multiheadattention models\n",
        "    dropout = 0  # the dropout value\n",
        "    nclasses = ntokens # classification to get output words in the same language\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, nclasses, dropout).to(device)\n",
        "    dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21], [8, 5, 3, 42, 43, 21]]).to(device)\n",
        "\n",
        "    sequence_length = dummy_input.shape[0] #L\n",
        "    batch_size = dummy_input.shape[1] #N\n",
        "\n",
        "    src_mask = TransformerModel.generate_square_subsequent_mask(sequence_length).to(device)\n",
        "    assert list(src_mask.shape) == [sequence_length,sequence_length]\n",
        "    # batch dimension N is not involved in the mask computation! We assume all sequences in the batch has the same sequence length L\n",
        "    out = model.forward(dummy_input, src_mask)\n",
        "    expected_size = [sequence_length, batch_size, nclasses]\n",
        "    assert list(out.shape) == expected_size, f\"{out.shape}, {expected_size}\"\n",
        "    print(out.shape)\n",
        "test_transformer_based_classifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vFdH_-JeFbGA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁trop <sos>\n"
          ]
        }
      ],
      "source": [
        "SRC = \"source_sequence\"\n",
        "TGT = \"target\"\n",
        "SOS = \"<sos>\"\n",
        "PAD = \"<pad>\"\n",
        "EOS = \"<eos>\"\n",
        "OOV = \"<oov>\"\n",
        "LM_TASK = \"language_modeling\"\n",
        "DS_TASK = \"classification\"\n",
        "token2ind = {SOS: 0, PAD : 1, EOS: 2, OOV: 3} # the 4 first indices are reserved to special tokens\n",
        "offset = max(token2ind.values())+1\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] = idx+offset\n",
        "ind2token = {index: token for token, index in token2ind.items()}\n",
        "print(ind2token[1111], ind2token[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents: Path,\n",
        "        path_labels: Path = None,\n",
        "        token2ind: Dict[str, int]={},\n",
        "        max_len: int=512,\n",
        "        task: str=LM_TASK,\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "        self.oov_index = self.token2ind[OOV]\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1] \n",
        "        \n",
        "        source_sequence = [self.token2ind.get(token, self.oov_index) for token in sequence]\n",
        "        source_sequence.insert(0, self.token2ind[SOS]) \n",
        "        # (constract the input sequence using token2ind, sequence and special tokens)\n",
        "        if self.task == LM_TASK:\n",
        "            target = source_sequence[1:] # offset the sequence by one\n",
        "            # A, B , C, D , <EOS>\n",
        "            target.append(self.token2ind[EOS])\n",
        "            assert len(target) == len(source_sequence)\n",
        "        elif self.task == DS_TASK:\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            SRC: torch.tensor(source_sequence),\n",
        "            TGT: torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def collate_sentences_keep_dim(batch: List[Dict[str, torch.LongTensor]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "    \"\"\"Uniformize batches (have the same sentence length with padding for all sentences across the batch)\n",
        "\n",
        "    Args:\n",
        "        batch (List[Dict[str, torch.LongTensor]]): List of dict samples containing \n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.LongTensor, torch.LongTensor]: \n",
        "            - source [L, N, V]\n",
        "            where L is the maximum length along all sentences in the batch\n",
        "            - target \n",
        "                - [L, N, V] for language modeling task\n",
        "                - [N, C] for classification with C the number of classes\n",
        "            \n",
        "    \"\"\"\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[SRC] for sample in batch], padding_value=token2ind[PAD]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[TGT] for sample in batch], padding_value=token2ind[PAD]\n",
        "    )\n",
        "    return source_sequences, target\n",
        "\n",
        "\n",
        "def collate_sentences(batch: List[Dict[str, torch.LongTensor]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "    source_sequences, target = collate_sentences_keep_dim(batch)\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "def get_loader(\n",
        "    path_documents :Path,\n",
        "    path_labels: Path = None,\n",
        "    token2ind : Dict[str, int]={},\n",
        "    max_len: int =512,\n",
        "    batch_size: int = 32,\n",
        "    task: str=LM_TASK,\n",
        "    collate_fn = collate_sentences,\n",
        "    shuffle=True\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=max_len,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sanity check on data loader and collate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> Les chauds et les froids d ' un déplacement présidentiel ( M itter rand ) ou encore la prise en otage d ' un air bus ministériel ( cinq ministres , vingt députés et .\n",
            "Les chauds et les froids d ' un déplacement présidentiel ( M itter rand ) ou encore la prise en otage d ' un air bus ministériel ( cinq ministres , vingt députés et . <eos>\n",
            "<sos> Les hommes sont encore en train de mesurer leur pouce .\n",
            "Les hommes sont encore en train de mesurer leur pouce . <eos>\n",
            "<sos> La recherche est possible directement sur la carte , ou bien en rentrant une adresse de son choix .\n",
            "La recherche est possible directement sur la carte , ou bien en rentrant une adresse de son choix . <eos>\n",
            "<sos> Tous les jours nous cherchons pour vous sur le web les articles , vidéos et documentaires qui nous paraissent les plus pertinents et utiles à tous .\n",
            "Tous les jours nous cherchons pour vous sur le web les articles , vidéos et documentaires qui nous paraissent les plus pertinents et utiles à tous . <eos>\n",
            "<sos> D ' abord , le Hezbollah a toujours eu les mains sales et donne la naus ée lorsqu ' on voit le nombre d ' activités illégales qui participent de son financement : tous les trafics , armes , cigarettes , contrefaçon , drogues , passent par ses mains , qui ne sont plus innoc entes depuis longtemps .\n",
            "D ' abord , le Hezbollah a toujours eu les mains sales et donne la naus ée lorsqu ' on voit le nombre d ' activités illégales qui participent de son financement : tous les trafics , armes , cigarettes , contrefaçon , drogues , passent par ses mains , qui ne sont plus innoc entes depuis longtemps . <eos>\n"
          ]
        }
      ],
      "source": [
        "N = 32\n",
        "data_loader = get_loader(\n",
        "    pretraining_path_data_train,\n",
        "    token2ind=token2ind,\n",
        "    batch_size=N,\n",
        "    task=LM_TASK,\n",
        "    collate_fn=collate_sentences_keep_dim\n",
        ")\n",
        "token2ind[OOV]\n",
        "it = iter(data_loader)\n",
        "\n",
        "def tensor_to_sentence(tensor_sentence):\n",
        "        return \" \".join([ind2token.get(el, \"not found\").replace(\"▁\", \"\") for el in tensor_sentence.numpy() if el != token2ind[PAD]])\n",
        "\n",
        "for u in range(5):\n",
        "    sampled_batch = next(it)\n",
        "    # first[0].shape, first[1].shape\n",
        "    # first[0][:, 0] # first sentence # L, N\n",
        "    first_sentence = sampled_batch[0][:, 0]\n",
        "    def tensor_to_sentence(tensor_sentence):\n",
        "        return \" \".join([ind2token.get(el, \"not found\").replace(\"▁\", \"\") for el in tensor_sentence.numpy() if el != token2ind[PAD]])\n",
        "    print(tensor_to_sentence(first_sentence))\n",
        "    print(tensor_to_sentence(sampled_batch[1][:, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50001\n"
          ]
        }
      ],
      "source": [
        "ntokens = len(ind2token) # the size of vocabulary\n",
        "print(len(ind2token))\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model_pretraining = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimization parameters\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model_pretraining.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: Model,\n",
        "    path_data_train: Path,\n",
        "    path_labels_train: Path =None,\n",
        "    path_data_valid: Path = None,\n",
        "    save_interval: int =-1,\n",
        "    log_interval: int=5,\n",
        "    task: str=LM_TASK,\n",
        "    batch_size: int =32,\n",
        "    max_len=512,\n",
        "    epoch=0,\n",
        "    name=\"\",\n",
        "    freeze_backbone=False\n",
        "):\n",
        "    model.train()\n",
        "    if freeze_backbone:\n",
        "        print(\"FREEZE TRANSFORMER BACKBONE\")\n",
        "        for param in model.base.parameters():\n",
        "            param.requires_grad = False\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        max_len=max_len,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): # get a batch of samples\n",
        "        # reset gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        src_mask = TransformerModel.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        # forward pass\n",
        "        output = model(input, src_mask) \n",
        "        if task == DS_TASK:\n",
        "            output = output[-1:, :]  # last vector only\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = data[1]\n",
        "        target = target.to(device)\n",
        "        loss =  criterion(output, target) # CROSS ENTROPY\n",
        "        loss.backward() # compute gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                f\"| epoch {epoch:3d} | {idx:5d}/{len(data_loader):5d} steps | \"+\n",
        "                f\"loss {cur_loss:5.5f} | ppl {math.exp(cur_loss):8.3f}\"\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    if epoch % save_interval == 0 and save_interval>=1:\n",
        "        torch.save(model, f\"weights_{task}{name}_{epoch:02d}.pt\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# EMPTY THE GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m11g4ScjZaR"
      },
      "outputs": [],
      "source": [
        "#pretraining on a tiny subset\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        pretraining_path_data_train,\n",
        "        save_interval=-1,\n",
        "        task=LM_TASK,\n",
        "        batch_size=16,\n",
        "        max_len=64,\n",
        "        log_interval=log_interval,\n",
        "        epoch=epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-BcBC6FSkMH3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_pretraining = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device)\n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt')\n",
        "#load state dict\n",
        "model_pretraining.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tBRRVsWqlIoQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour les gens qui ont été très accueillants et sympathiques.\n"
          ]
        }
      ],
      "source": [
        "def infer_next_token(sent, model):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind =  int(torch.argmax(torch.nn.Softmax(dim=-1)(out[-1, 0, :])).detach()) #FORCE N=1\n",
        "    return next_token_ind, out\n",
        "\n",
        "def infer_next_tokens(sent, model, max_len=50):\n",
        "    next_token_list = []\n",
        "    next_token_index=-1\n",
        "    iter = 0\n",
        "    while next_token_index!=token2ind[EOS]:\n",
        "        if iter>max_len:\n",
        "            break\n",
        "        next_token_index, out = infer_next_token(sent, model)\n",
        "        next_token = ind2token.get(next_token_index)\n",
        "        sent+= next_token\n",
        "        next_token_list.append(next_token)\n",
        "    return next_token_list\n",
        "\n",
        "\n",
        "sent = \"Bonjour les\"\n",
        "out = infer_next_tokens(sent, model_pretraining)\n",
        "print(sent + \" \" + s.decode_pieces(out[:-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000 sentences in the validation set\n"
          ]
        }
      ],
      "source": [
        "data_loader_validation = get_loader(\n",
        "    downstream_path_data_valid,\n",
        "    downstream_path_labels_valid,\n",
        "    token2ind=token2ind,\n",
        "    # batch_size=20,\n",
        "    batch_size=1, # Let's use a batch size of 1 so a sentence always has maximum length\n",
        "    task='classification',\n",
        "    shuffle=False # AVOID SHUFFLING FOR OBVIOUS REASONS - even if we're going through the whole validation set - let's do it in the same order everytime (batches will always be the same, no matter the configuration)\n",
        ")\n",
        "print(f\"{len(data_loader_validation.dataset)} sentences in the validation set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> J ' ai lu ce livre car dans ma ville , tout le monde s ' en sert et le commande . C ' est ma pharmaci enne qui me l ' a conseillé , elle a t ellement maig ri que je lui ai demandé ce qu ' elle avait fait et au lieu de me vendre du per li mp in pin en gé lules , elle m ' a conseillé ce livre à 5 euros . Bien sur , il faut faire un effort pour perdre 25 kilos mais avec le livre , j ' avais un compagnon de route . L ' auteur a su me parler simplement avec des arguments très forts et surtout j ' ai senti qu ' il connaissait bien des cas comme le mien . Il y a dans son texte de l ' expérience , de la simplicité et de la compassion pour ceux qui comme moi vivait avec tout ce poids qui me col lait au corps sans jamais vouloir partir . Je ne crois pas qu ' il existe un régime miracle qui surp asse les autres mais je crois vraiment qu ' il y a des personnes qui savent parler aux autres et faire n aitre des déclic s . Je croyais être faible mais ce livre m ' a rendu forte , je l ' ai t ellement ann oté que j ' en suis à mon troisième . Quand on est très grosse comme je l ' ai été , les non - gros ne vous comprennent pas ou ont peur de vous frois ser en vous en parlant , alors ce livre a été comme un compagnon - journal . Je suis pé dic ure et je l ' ai conseillé à tous mes clients gros dont je lis la souffrance sur les pieds déform és et gonf lés . je rends aux autres le service que m ' a rendu ma pharmaci enne . Je le conseille à tous ceux qui souffrent car après avoir maig ri c ' est un tel bonheur que j ' ai accepté de passer à la phase 3 de ce plan qui impose 10 jours de consolidation pour chaque kilo perdu en s ' ouvrant progressivement à tout . Maintenant , je suis en phase 4, c ' est à dire que je mange de tout sauf le jeudi où je contrôle . Je remercier ai jamais assez l ' auteur de ce livre .\n",
            "True\n",
            "<sos> Recettes appréciées de toute la famille ( petits et grands ) de plus on peut faire son régime en ayant des invités , ils n ' y voient que du feu . Pour la vinaigrette il ne faut surtout pas dire quelle est faite avec de l ' huile de par af ine alors elle est excellente , s inon .... Le régime est super efficace il ne fatigue pas du tout . J ' encourage ceux qui ont des kilos en trop , à le faire , il ne faut pas beaucoup de volonté car on mange toujours à sa faim .\n",
            "True\n",
            "<sos> Be ig be der se dra pe de mystère . Il pose avec des airs sombres , en rou lé dans une cape noire , façon Z oz o ( com mentaire écrit sur fond musical : \" Z orro est arrivé \" de Henri Salvador ). Et pourtant , le seul mystère réservé aux ni ais , serait celui du \" mi ra cule ux \" succès obtenu sans aucun talent . La publicité a montré que le talent n ' était pas une condition nécessaire pour réussir . Un plat de nouilles a du succès s ' il est bien médi atisé . Les motoc ro ttes firent beaucoup pour la popularité du maire de Paris , Jacques C . \" Un roman français \" est un plat sans saveur , composé d ' ingrédients sans relief , qui n ' é veille ni l ' appétit , ni l ' esprit . Vous en doute z ? Au chapitre de l ' éducation sexuelle , cette autobiographie , nous conte les premiers ém ois à 13 ans de l ' auteur qui embrasse sur la bouche avec la langue dehors . Les goûts littéraires sont présentés à la manière d ' un feu d ' artifice Internet . Vous n ' avez rien lu de ce dont vous parlez . Pas grave , c itez p êle - mêle des noms d ' écriv ains aux cons on ances exotiques , anglo - sax onnes . Une pause . Qu ' il est fort ce Z oz o ! Be ig be der , dans la lignée de l ' émission \" lit té raire \" qu ' il avait \" anim ée \" nous apprend que San Antonio ( sic !) est un auteur de droite , comme Rab elais ( !). Rab elais , un écrivain de droite ! Le délire , l ' in culture et l ' absence de syntaxe soutiennent pénible ment la démonstration : Be ig be der appréci ait ces écrivains de droite car ils sont rigol os ( je cite ) alors que les auteurs de gauche , Sartre , Camus ne le sont pas ... à l ' exception précise - t - il des \" M ots \" et de \" La Ch ute \". Ces ouvrages seraient donc \" rig olos \". En quoi Céline , \" écrivain de droite \" peut - il être rigolo ? Trou bles ( ment aux ). Ce dro gué , coc aï nom ane , cra che sur la justice , les mesures de salubrité publique prises pour le sauver de lui - même . Z oz o était p af . Z oz o était malade . Z oz o était en infraction . Ah , ces 17 heures de mise \" en prison \" ( au poste de police seulement ) valent les années de goul ag de Sol j én its yne , celles du camp d ' extermination de Prim o Levi , celles de D\n",
            "False\n",
            "<sos> Un petit livre si facile à lire et si puissant . Le racisme ne peut pas résister longtemps devant une telle mise en perspective . Les \" ra ces \" soi - disant dominantes d ' aujourd ' hui ne seront pas celles de demain ... Un excellent livre à lire et offrir aussi souvent que l ' on peut\n",
            "True\n",
            "<sos> Saint - Ex upéry réalise à travers l ' histoire du petit prince l ' un des plus grands chefs - d ' oeuvres littéraires , poétiques et révolu tionnaires de tous les temps . La morale magique \" l ' essentiel est invisible pour les yeux \" semble rés onner partout où notre regard se porte , vibrant dans l ' harmonie retrouvée des premiers matins du monde . Change ton regard , là est la clé ! semble nous indiquer Saint - Ex . Ange d ' espoir , reviens ! nous lance - t - il en re tenant des larmes d ' enfant . Ce conte philosophique est une ode à la vie , au bonheur , à la compréhension et à la fraternité humaine . Il s ' agit d ' un testament poétique , d ' une oeuvre intemp orelle et universelle , d ' une tentative pour restaurer le lien secret entre l ' Homme et l ' Univers , entre l ' Homme et Lui - même . Mais la portée véritable et finale est d ' un autre ordre : la Fleur est destinée ...\n",
            "True\n",
            "<sos> je suis tres de çue de la taille de ce livre ( il tiens dans le creux de la main ... et les recettes y sont N ULES ( il n ' y a rien de nouveau et d ' in ven tif ) je ne le recommande a personne !!!\n",
            "False\n",
            "<sos> Ce livre n ' est pas cher et il comporte un grand nombre de recettes originales . Par contre , il est bourré de fautes : soit vous avez des ingrédients qui sont oubliés dans la recette ( quand les mettre ? ), ou la recette demandent des ingrédients mais ils ne sont pas dans la liste des ingrédients ( quelle quantité mettre ?) A acheter en connaissance de cause ...\n",
            "False\n",
            "<sos> Un roman de rentrée de la fille de B HL , a - t - on vraiment besoin d ' en dire plus pour vous convaincre que la lecture de cet ouvrage , au style aussi peu inspiré que lourd , vous fera perdre de précieuses heures ? A mon humble avis , le seul fait d ' être la fille d ' un philosophe milliardaire ne donne pas automatiquement toutes les qualités requises pour faire un bon écrivain . Pour ma part j ' ai assez rapidement laissé tomber ce bouquin pour me plonger dans .\n",
            "False\n",
            "<sos> Contrairement au commentaire de Seb , je trouve ce livre vraiment très pratique . Il y a certes quelques erreurs comme l ' IG du cass oulet effectivement non nul . Mais le reste de la ligne montre qu ' il y a bien de n bre uses glucides et plus important qu ' il est à limiter pour les diabétiques . Les pages sont bien dans le bon ordre ( c ' est ton exemplaire qui est mauvais Seb !) et surtout il n ' y a pas d ' erreurs sur les quantités de G , L ou P car le n bre noté est pour la quantité indiquée . Donc pour le N ute lla pas d ' erreur à noter puisqu ' il s ' agit de 15 g rs de produit étudié et pas la valeur pour 100 g rs comme sur le pot . Donc monsieur Seb devrait chercher à mieux comprendre le tableau avant de critiquer ce fabuleux outils qui a tout de même le mérite , en format poche , de regrouper 11 40 aliments ! Trés utile pour les diab étique .\n",
            "True\n",
            "<sos> j ' ai acheté ce bouquin sur le seul nom de Gran gé . On ne m ' y reprendra plus ! On n ' y croit pas un seul instant , c ' est creux , comme l ' héroïne . Je me suis forcée à le finir , en croyant au miracle . Que n enni !\n",
            "False\n",
            "<sos> Je dis non à la facilité ..... et non à l ' auteur qui n ' a même pas fait l ' effort de rencontrer la famille avant d ' écrire son bouquin , qui ne connait rien au personnage de Gregory et encore moins à l ' artiste !! ..... Comment parler correctement de quelqu ' un qu ' on a jamais rencontré de son vivant , et dont on ne s ' est intéressé qu ' après sa mort ??? Franchement à la place de l ' auteur j ' aurais vraiment honte de vouloir profiter de la douleur des fans !! Si vous voulez lire un bon livre sur GRE G OR Y LE MAR CH AL , achetez plutôt celui de sa soeur Leslie \" Mon Frère , l ' Ar tiste \" et en plus vous fer rez une bonne action , puisque tout est entièrement re versé à l ' association GRE G OR Y LE MAR CH AL !!\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence_batch in enumerate(data_loader_validation):\n",
        "    if idx>10:\n",
        "        break\n",
        "    first_sentence = sentence_batch[0][:, 0]\n",
        "    print(tensor_to_sentence(first_sentence))\n",
        "    print(bool(sentence_batch[1][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "outputs": [],
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader: DataLoader, model):\n",
        "    accuracy = 0\n",
        "    for _idx, sentence_batch in enumerate(data_loader):\n",
        "        input_sentences = sentence_batch[0].to(device)\n",
        "        target_labels = sentence_batch[1].to(device)\n",
        "        logits = model(input_sentences, src_mask=None)[-1, ...]\n",
        "        predicted_class = torch.argmax(logits)\n",
        "        accuracy += torch.sum(predicted_class==target_labels).detach()\n",
        "    accuracy = float(accuracy)/len(data_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "def test_evaluate_accuracy():\n",
        "    model_ds = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    model_ds.eval()\n",
        "    acc = evaluate_accuracy(\n",
        "        data_loader_validation,\n",
        "        model_ds\n",
        "    )\n",
        "    print(f\"accuracy of a randomly initalized classifier {acc} ~ shall be around 0.5\")\n",
        "\n",
        "# test_evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "outputs": [],
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model_pretraining.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====PRETRAINED MODEL======\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   1 |    50/ 1600 steps | loss 0.80629 | ppl    2.240\n",
            "| epoch   1 |   100/ 1600 steps | loss 0.83552 | ppl    2.306\n",
            "| epoch   1 |   150/ 1600 steps | loss 0.78159 | ppl    2.185\n",
            "| epoch   1 |   200/ 1600 steps | loss 0.77289 | ppl    2.166\n",
            "| epoch   1 |   250/ 1600 steps | loss 0.72881 | ppl    2.073\n",
            "| epoch   1 |   300/ 1600 steps | loss 0.77162 | ppl    2.163\n",
            "| epoch   1 |   350/ 1600 steps | loss 0.71900 | ppl    2.052\n",
            "| epoch   1 |   400/ 1600 steps | loss 0.73108 | ppl    2.077\n",
            "| epoch   1 |   450/ 1600 steps | loss 0.68333 | ppl    1.980\n",
            "| epoch   1 |   500/ 1600 steps | loss 0.86753 | ppl    2.381\n",
            "| epoch   1 |   550/ 1600 steps | loss 0.89081 | ppl    2.437\n",
            "| epoch   1 |   600/ 1600 steps | loss 0.76824 | ppl    2.156\n",
            "| epoch   1 |   650/ 1600 steps | loss 0.70084 | ppl    2.015\n",
            "| epoch   1 |   700/ 1600 steps | loss 0.87753 | ppl    2.405\n",
            "| epoch   1 |   750/ 1600 steps | loss 0.83862 | ppl    2.313\n",
            "| epoch   1 |   800/ 1600 steps | loss 0.92922 | ppl    2.533\n",
            "| epoch   1 |   850/ 1600 steps | loss 0.69227 | ppl    1.998\n",
            "| epoch   1 |   900/ 1600 steps | loss 0.66193 | ppl    1.939\n",
            "| epoch   1 |   950/ 1600 steps | loss 0.70259 | ppl    2.019\n",
            "| epoch   1 |  1000/ 1600 steps | loss 0.83228 | ppl    2.299\n",
            "| epoch   1 |  1050/ 1600 steps | loss 0.76381 | ppl    2.146\n",
            "| epoch   1 |  1100/ 1600 steps | loss 0.79639 | ppl    2.218\n",
            "| epoch   1 |  1150/ 1600 steps | loss 0.75324 | ppl    2.124\n",
            "| epoch   1 |  1200/ 1600 steps | loss 0.75705 | ppl    2.132\n",
            "| epoch   1 |  1250/ 1600 steps | loss 0.76992 | ppl    2.160\n",
            "| epoch   1 |  1300/ 1600 steps | loss 0.70842 | ppl    2.031\n",
            "| epoch   1 |  1350/ 1600 steps | loss 0.81241 | ppl    2.253\n",
            "| epoch   1 |  1400/ 1600 steps | loss 0.88223 | ppl    2.416\n",
            "| epoch   1 |  1450/ 1600 steps | loss 0.78611 | ppl    2.195\n",
            "| epoch   1 |  1500/ 1600 steps | loss 0.75353 | ppl    2.124\n",
            "| epoch   1 |  1550/ 1600 steps | loss 0.86124 | ppl    2.366\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6295\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   2 |    50/ 1600 steps | loss 0.80883 | ppl    2.245\n",
            "| epoch   2 |   100/ 1600 steps | loss 0.61103 | ppl    1.842\n",
            "| epoch   2 |   150/ 1600 steps | loss 0.73852 | ppl    2.093\n",
            "| epoch   2 |   200/ 1600 steps | loss 0.59959 | ppl    1.821\n",
            "| epoch   2 |   250/ 1600 steps | loss 0.56904 | ppl    1.767\n",
            "| epoch   2 |   300/ 1600 steps | loss 0.85531 | ppl    2.352\n",
            "| epoch   2 |   350/ 1600 steps | loss 0.79384 | ppl    2.212\n",
            "| epoch   2 |   400/ 1600 steps | loss 0.73446 | ppl    2.084\n",
            "| epoch   2 |   450/ 1600 steps | loss 0.58342 | ppl    1.792\n",
            "| epoch   2 |   500/ 1600 steps | loss 0.65429 | ppl    1.924\n",
            "| epoch   2 |   550/ 1600 steps | loss 0.61485 | ppl    1.849\n",
            "| epoch   2 |   600/ 1600 steps | loss 0.64055 | ppl    1.898\n",
            "| epoch   2 |   650/ 1600 steps | loss 0.69528 | ppl    2.004\n",
            "| epoch   2 |   700/ 1600 steps | loss 0.68160 | ppl    1.977\n",
            "| epoch   2 |   750/ 1600 steps | loss 0.65787 | ppl    1.931\n",
            "| epoch   2 |   800/ 1600 steps | loss 0.84665 | ppl    2.332\n",
            "| epoch   2 |   850/ 1600 steps | loss 0.59998 | ppl    1.822\n",
            "| epoch   2 |   900/ 1600 steps | loss 0.61400 | ppl    1.848\n",
            "| epoch   2 |   950/ 1600 steps | loss 0.57681 | ppl    1.780\n",
            "| epoch   2 |  1000/ 1600 steps | loss 0.71529 | ppl    2.045\n",
            "| epoch   2 |  1050/ 1600 steps | loss 0.65232 | ppl    1.920\n",
            "| epoch   2 |  1100/ 1600 steps | loss 0.73439 | ppl    2.084\n",
            "| epoch   2 |  1150/ 1600 steps | loss 0.61994 | ppl    1.859\n",
            "| epoch   2 |  1200/ 1600 steps | loss 0.69803 | ppl    2.010\n",
            "| epoch   2 |  1250/ 1600 steps | loss 0.74384 | ppl    2.104\n",
            "| epoch   2 |  1300/ 1600 steps | loss 0.67494 | ppl    1.964\n",
            "| epoch   2 |  1350/ 1600 steps | loss 0.67092 | ppl    1.956\n",
            "| epoch   2 |  1400/ 1600 steps | loss 0.67211 | ppl    1.958\n",
            "| epoch   2 |  1450/ 1600 steps | loss 0.62298 | ppl    1.864\n",
            "| epoch   2 |  1500/ 1600 steps | loss 0.52483 | ppl    1.690\n",
            "| epoch   2 |  1550/ 1600 steps | loss 0.77914 | ppl    2.180\n",
            "from_scratch=False - VALIDATION ACCURACY 0.665\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   3 |    50/ 1600 steps | loss 0.63729 | ppl    1.891\n",
            "| epoch   3 |   100/ 1600 steps | loss 0.68631 | ppl    1.986\n",
            "| epoch   3 |   150/ 1600 steps | loss 0.70642 | ppl    2.027\n",
            "| epoch   3 |   200/ 1600 steps | loss 0.67318 | ppl    1.960\n",
            "| epoch   3 |   250/ 1600 steps | loss 0.70953 | ppl    2.033\n",
            "| epoch   3 |   300/ 1600 steps | loss 0.81637 | ppl    2.262\n",
            "| epoch   3 |   350/ 1600 steps | loss 0.64443 | ppl    1.905\n",
            "| epoch   3 |   400/ 1600 steps | loss 0.63941 | ppl    1.895\n",
            "| epoch   3 |   450/ 1600 steps | loss 0.61402 | ppl    1.848\n",
            "| epoch   3 |   500/ 1600 steps | loss 0.69766 | ppl    2.009\n",
            "| epoch   3 |   550/ 1600 steps | loss 0.68640 | ppl    1.987\n",
            "| epoch   3 |   600/ 1600 steps | loss 0.96840 | ppl    2.634\n",
            "| epoch   3 |   650/ 1600 steps | loss 0.86926 | ppl    2.385\n",
            "| epoch   3 |   700/ 1600 steps | loss 0.79919 | ppl    2.224\n",
            "| epoch   3 |   750/ 1600 steps | loss 0.70482 | ppl    2.023\n",
            "| epoch   3 |   800/ 1600 steps | loss 0.81915 | ppl    2.269\n",
            "| epoch   3 |   850/ 1600 steps | loss 0.62937 | ppl    1.876\n",
            "| epoch   3 |   900/ 1600 steps | loss 0.55966 | ppl    1.750\n",
            "| epoch   3 |   950/ 1600 steps | loss 0.82640 | ppl    2.285\n",
            "| epoch   3 |  1000/ 1600 steps | loss 0.70429 | ppl    2.022\n",
            "| epoch   3 |  1050/ 1600 steps | loss 0.46504 | ppl    1.592\n",
            "| epoch   3 |  1100/ 1600 steps | loss 0.58654 | ppl    1.798\n",
            "| epoch   3 |  1150/ 1600 steps | loss 0.70995 | ppl    2.034\n",
            "| epoch   3 |  1200/ 1600 steps | loss 0.77455 | ppl    2.170\n",
            "| epoch   3 |  1250/ 1600 steps | loss 0.79928 | ppl    2.224\n",
            "| epoch   3 |  1300/ 1600 steps | loss 0.70485 | ppl    2.024\n",
            "| epoch   3 |  1350/ 1600 steps | loss 0.62925 | ppl    1.876\n",
            "| epoch   3 |  1400/ 1600 steps | loss 0.73145 | ppl    2.078\n",
            "| epoch   3 |  1450/ 1600 steps | loss 0.57645 | ppl    1.780\n",
            "| epoch   3 |  1500/ 1600 steps | loss 0.93392 | ppl    2.544\n",
            "| epoch   3 |  1550/ 1600 steps | loss 0.90003 | ppl    2.460\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6805\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   4 |    50/ 1600 steps | loss 0.68301 | ppl    1.980\n",
            "| epoch   4 |   100/ 1600 steps | loss 0.68542 | ppl    1.985\n",
            "| epoch   4 |   150/ 1600 steps | loss 0.65587 | ppl    1.927\n",
            "| epoch   4 |   200/ 1600 steps | loss 0.75844 | ppl    2.135\n",
            "| epoch   4 |   250/ 1600 steps | loss 0.93127 | ppl    2.538\n",
            "| epoch   4 |   300/ 1600 steps | loss 0.71881 | ppl    2.052\n",
            "| epoch   4 |   350/ 1600 steps | loss 0.74906 | ppl    2.115\n",
            "| epoch   4 |   400/ 1600 steps | loss 0.54137 | ppl    1.718\n",
            "| epoch   4 |   450/ 1600 steps | loss 0.64778 | ppl    1.911\n",
            "| epoch   4 |   500/ 1600 steps | loss 0.88094 | ppl    2.413\n",
            "| epoch   4 |   550/ 1600 steps | loss 0.73502 | ppl    2.086\n",
            "| epoch   4 |   600/ 1600 steps | loss 0.75793 | ppl    2.134\n",
            "| epoch   4 |   650/ 1600 steps | loss 0.72214 | ppl    2.059\n",
            "| epoch   4 |   700/ 1600 steps | loss 0.77709 | ppl    2.175\n",
            "| epoch   4 |   750/ 1600 steps | loss 0.54876 | ppl    1.731\n",
            "| epoch   4 |   800/ 1600 steps | loss 0.67983 | ppl    1.974\n",
            "| epoch   4 |   850/ 1600 steps | loss 0.58481 | ppl    1.795\n",
            "| epoch   4 |   900/ 1600 steps | loss 0.84519 | ppl    2.328\n",
            "| epoch   4 |   950/ 1600 steps | loss 0.68123 | ppl    1.976\n",
            "| epoch   4 |  1000/ 1600 steps | loss 0.82744 | ppl    2.287\n",
            "| epoch   4 |  1050/ 1600 steps | loss 0.56758 | ppl    1.764\n",
            "| epoch   4 |  1100/ 1600 steps | loss 0.96987 | ppl    2.638\n",
            "| epoch   4 |  1150/ 1600 steps | loss 0.68023 | ppl    1.974\n",
            "| epoch   4 |  1200/ 1600 steps | loss 0.61262 | ppl    1.845\n",
            "| epoch   4 |  1250/ 1600 steps | loss 0.51187 | ppl    1.668\n",
            "| epoch   4 |  1300/ 1600 steps | loss 0.66336 | ppl    1.941\n",
            "| epoch   4 |  1350/ 1600 steps | loss 0.48248 | ppl    1.620\n",
            "| epoch   4 |  1400/ 1600 steps | loss 0.73502 | ppl    2.086\n",
            "| epoch   4 |  1450/ 1600 steps | loss 0.61020 | ppl    1.841\n",
            "| epoch   4 |  1500/ 1600 steps | loss 0.65376 | ppl    1.923\n",
            "| epoch   4 |  1550/ 1600 steps | loss 0.97703 | ppl    2.657\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6865\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   5 |    50/ 1600 steps | loss 0.59867 | ppl    1.820\n",
            "| epoch   5 |   100/ 1600 steps | loss 0.60922 | ppl    1.839\n",
            "| epoch   5 |   150/ 1600 steps | loss 0.78262 | ppl    2.187\n",
            "| epoch   5 |   200/ 1600 steps | loss 0.82328 | ppl    2.278\n",
            "| epoch   5 |   250/ 1600 steps | loss 0.53648 | ppl    1.710\n",
            "| epoch   5 |   300/ 1600 steps | loss 0.65236 | ppl    1.920\n",
            "| epoch   5 |   350/ 1600 steps | loss 0.57994 | ppl    1.786\n",
            "| epoch   5 |   400/ 1600 steps | loss 0.76779 | ppl    2.155\n",
            "| epoch   5 |   450/ 1600 steps | loss 0.67148 | ppl    1.957\n",
            "| epoch   5 |   500/ 1600 steps | loss 0.83745 | ppl    2.310\n",
            "| epoch   5 |   550/ 1600 steps | loss 0.63997 | ppl    1.896\n",
            "| epoch   5 |   600/ 1600 steps | loss 0.88432 | ppl    2.421\n",
            "| epoch   5 |   650/ 1600 steps | loss 0.68851 | ppl    1.991\n",
            "| epoch   5 |   700/ 1600 steps | loss 0.71005 | ppl    2.034\n",
            "| epoch   5 |   750/ 1600 steps | loss 0.70405 | ppl    2.022\n",
            "| epoch   5 |   800/ 1600 steps | loss 0.54748 | ppl    1.729\n",
            "| epoch   5 |   850/ 1600 steps | loss 0.70433 | ppl    2.022\n",
            "| epoch   5 |   900/ 1600 steps | loss 0.64333 | ppl    1.903\n",
            "| epoch   5 |   950/ 1600 steps | loss 0.50867 | ppl    1.663\n",
            "| epoch   5 |  1000/ 1600 steps | loss 0.50701 | ppl    1.660\n",
            "| epoch   5 |  1050/ 1600 steps | loss 0.74877 | ppl    2.114\n",
            "| epoch   5 |  1100/ 1600 steps | loss 0.83327 | ppl    2.301\n",
            "| epoch   5 |  1150/ 1600 steps | loss 1.06015 | ppl    2.887\n",
            "| epoch   5 |  1200/ 1600 steps | loss 0.74487 | ppl    2.106\n",
            "| epoch   5 |  1250/ 1600 steps | loss 0.77184 | ppl    2.164\n",
            "| epoch   5 |  1300/ 1600 steps | loss 0.78397 | ppl    2.190\n",
            "| epoch   5 |  1350/ 1600 steps | loss 0.55240 | ppl    1.737\n",
            "| epoch   5 |  1400/ 1600 steps | loss 0.61299 | ppl    1.846\n",
            "| epoch   5 |  1450/ 1600 steps | loss 0.60962 | ppl    1.840\n",
            "| epoch   5 |  1500/ 1600 steps | loss 0.69477 | ppl    2.003\n",
            "| epoch   5 |  1550/ 1600 steps | loss 0.85355 | ppl    2.348\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6855\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   6 |    50/ 1600 steps | loss 0.61065 | ppl    1.842\n",
            "| epoch   6 |   100/ 1600 steps | loss 0.82277 | ppl    2.277\n",
            "| epoch   6 |   150/ 1600 steps | loss 0.74902 | ppl    2.115\n",
            "| epoch   6 |   200/ 1600 steps | loss 0.69866 | ppl    2.011\n",
            "| epoch   6 |   250/ 1600 steps | loss 0.66991 | ppl    1.954\n",
            "| epoch   6 |   300/ 1600 steps | loss 0.57846 | ppl    1.783\n",
            "| epoch   6 |   350/ 1600 steps | loss 0.84261 | ppl    2.322\n",
            "| epoch   6 |   400/ 1600 steps | loss 0.68577 | ppl    1.985\n",
            "| epoch   6 |   450/ 1600 steps | loss 0.74986 | ppl    2.117\n",
            "| epoch   6 |   500/ 1600 steps | loss 0.62351 | ppl    1.865\n",
            "| epoch   6 |   550/ 1600 steps | loss 0.70267 | ppl    2.019\n",
            "| epoch   6 |   600/ 1600 steps | loss 0.70920 | ppl    2.032\n",
            "| epoch   6 |   650/ 1600 steps | loss 0.69568 | ppl    2.005\n",
            "| epoch   6 |   700/ 1600 steps | loss 0.84063 | ppl    2.318\n",
            "| epoch   6 |   750/ 1600 steps | loss 0.72952 | ppl    2.074\n",
            "| epoch   6 |   800/ 1600 steps | loss 0.85588 | ppl    2.353\n",
            "| epoch   6 |   850/ 1600 steps | loss 0.73633 | ppl    2.088\n",
            "| epoch   6 |   900/ 1600 steps | loss 0.64538 | ppl    1.907\n",
            "| epoch   6 |   950/ 1600 steps | loss 0.59107 | ppl    1.806\n",
            "| epoch   6 |  1000/ 1600 steps | loss 0.77482 | ppl    2.170\n",
            "| epoch   6 |  1050/ 1600 steps | loss 0.74129 | ppl    2.099\n",
            "| epoch   6 |  1100/ 1600 steps | loss 0.82073 | ppl    2.272\n",
            "| epoch   6 |  1150/ 1600 steps | loss 0.84079 | ppl    2.318\n",
            "| epoch   6 |  1200/ 1600 steps | loss 0.33145 | ppl    1.393\n",
            "| epoch   6 |  1250/ 1600 steps | loss 0.79254 | ppl    2.209\n",
            "| epoch   6 |  1300/ 1600 steps | loss 0.99664 | ppl    2.709\n",
            "| epoch   6 |  1350/ 1600 steps | loss 0.70675 | ppl    2.027\n",
            "| epoch   6 |  1400/ 1600 steps | loss 0.80273 | ppl    2.232\n",
            "| epoch   6 |  1450/ 1600 steps | loss 0.77440 | ppl    2.169\n",
            "| epoch   6 |  1500/ 1600 steps | loss 0.70715 | ppl    2.028\n",
            "| epoch   6 |  1550/ 1600 steps | loss 0.47373 | ppl    1.606\n",
            "from_scratch=False - VALIDATION ACCURACY 0.691\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   7 |    50/ 1600 steps | loss 0.98627 | ppl    2.681\n",
            "| epoch   7 |   100/ 1600 steps | loss 0.75200 | ppl    2.121\n",
            "| epoch   7 |   150/ 1600 steps | loss 0.65391 | ppl    1.923\n",
            "| epoch   7 |   200/ 1600 steps | loss 0.84052 | ppl    2.318\n",
            "| epoch   7 |   250/ 1600 steps | loss 0.85122 | ppl    2.342\n",
            "| epoch   7 |   300/ 1600 steps | loss 0.44191 | ppl    1.556\n",
            "| epoch   7 |   350/ 1600 steps | loss 1.06491 | ppl    2.901\n",
            "| epoch   7 |   400/ 1600 steps | loss 0.70805 | ppl    2.030\n",
            "| epoch   7 |   450/ 1600 steps | loss 0.62737 | ppl    1.873\n",
            "| epoch   7 |   500/ 1600 steps | loss 0.82348 | ppl    2.278\n",
            "| epoch   7 |   550/ 1600 steps | loss 0.88415 | ppl    2.421\n",
            "| epoch   7 |   600/ 1600 steps | loss 0.98334 | ppl    2.673\n",
            "| epoch   7 |   650/ 1600 steps | loss 0.27950 | ppl    1.322\n",
            "| epoch   7 |   700/ 1600 steps | loss 0.67234 | ppl    1.959\n",
            "| epoch   7 |   750/ 1600 steps | loss 0.72028 | ppl    2.055\n",
            "| epoch   7 |   800/ 1600 steps | loss 0.69826 | ppl    2.010\n",
            "| epoch   7 |   850/ 1600 steps | loss 0.64675 | ppl    1.909\n",
            "| epoch   7 |   900/ 1600 steps | loss 0.84205 | ppl    2.321\n",
            "| epoch   7 |   950/ 1600 steps | loss 0.50964 | ppl    1.665\n",
            "| epoch   7 |  1000/ 1600 steps | loss 0.68353 | ppl    1.981\n",
            "| epoch   7 |  1050/ 1600 steps | loss 0.91889 | ppl    2.507\n",
            "| epoch   7 |  1100/ 1600 steps | loss 0.64829 | ppl    1.912\n",
            "| epoch   7 |  1150/ 1600 steps | loss 0.69542 | ppl    2.005\n",
            "| epoch   7 |  1200/ 1600 steps | loss 0.92582 | ppl    2.524\n",
            "| epoch   7 |  1250/ 1600 steps | loss 0.66873 | ppl    1.952\n",
            "| epoch   7 |  1300/ 1600 steps | loss 0.89154 | ppl    2.439\n",
            "| epoch   7 |  1350/ 1600 steps | loss 0.78268 | ppl    2.187\n",
            "| epoch   7 |  1400/ 1600 steps | loss 0.66654 | ppl    1.947\n",
            "| epoch   7 |  1450/ 1600 steps | loss 0.62425 | ppl    1.867\n",
            "| epoch   7 |  1500/ 1600 steps | loss 0.69008 | ppl    1.994\n",
            "| epoch   7 |  1550/ 1600 steps | loss 0.62622 | ppl    1.871\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6955\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   8 |    50/ 1600 steps | loss 0.94418 | ppl    2.571\n",
            "| epoch   8 |   100/ 1600 steps | loss 0.79835 | ppl    2.222\n",
            "| epoch   8 |   150/ 1600 steps | loss 0.53756 | ppl    1.712\n",
            "| epoch   8 |   200/ 1600 steps | loss 0.83289 | ppl    2.300\n",
            "| epoch   8 |   250/ 1600 steps | loss 0.65846 | ppl    1.932\n",
            "| epoch   8 |   300/ 1600 steps | loss 0.87767 | ppl    2.405\n",
            "| epoch   8 |   350/ 1600 steps | loss 0.55638 | ppl    1.744\n",
            "| epoch   8 |   400/ 1600 steps | loss 0.47130 | ppl    1.602\n",
            "| epoch   8 |   450/ 1600 steps | loss 0.65225 | ppl    1.920\n",
            "| epoch   8 |   500/ 1600 steps | loss 0.63847 | ppl    1.894\n",
            "| epoch   8 |   550/ 1600 steps | loss 0.59368 | ppl    1.811\n",
            "| epoch   8 |   600/ 1600 steps | loss 0.81375 | ppl    2.256\n",
            "| epoch   8 |   650/ 1600 steps | loss 0.74075 | ppl    2.098\n",
            "| epoch   8 |   700/ 1600 steps | loss 0.89375 | ppl    2.444\n",
            "| epoch   8 |   750/ 1600 steps | loss 0.95293 | ppl    2.593\n",
            "| epoch   8 |   800/ 1600 steps | loss 0.76663 | ppl    2.153\n",
            "| epoch   8 |   850/ 1600 steps | loss 0.95845 | ppl    2.608\n",
            "| epoch   8 |   900/ 1600 steps | loss 0.92428 | ppl    2.520\n",
            "| epoch   8 |   950/ 1600 steps | loss 0.90023 | ppl    2.460\n",
            "| epoch   8 |  1000/ 1600 steps | loss 0.99986 | ppl    2.718\n",
            "| epoch   8 |  1050/ 1600 steps | loss 0.88578 | ppl    2.425\n",
            "| epoch   8 |  1100/ 1600 steps | loss 0.72776 | ppl    2.070\n",
            "| epoch   8 |  1150/ 1600 steps | loss 0.63909 | ppl    1.895\n",
            "| epoch   8 |  1200/ 1600 steps | loss 0.71983 | ppl    2.054\n",
            "| epoch   8 |  1250/ 1600 steps | loss 0.78908 | ppl    2.201\n",
            "| epoch   8 |  1300/ 1600 steps | loss 0.66435 | ppl    1.943\n",
            "| epoch   8 |  1350/ 1600 steps | loss 0.55524 | ppl    1.742\n",
            "| epoch   8 |  1400/ 1600 steps | loss 0.81717 | ppl    2.264\n",
            "| epoch   8 |  1450/ 1600 steps | loss 0.77784 | ppl    2.177\n",
            "| epoch   8 |  1500/ 1600 steps | loss 0.73833 | ppl    2.092\n",
            "| epoch   8 |  1550/ 1600 steps | loss 0.62473 | ppl    1.868\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7045\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   9 |    50/ 1600 steps | loss 0.86548 | ppl    2.376\n",
            "| epoch   9 |   100/ 1600 steps | loss 0.73923 | ppl    2.094\n",
            "| epoch   9 |   150/ 1600 steps | loss 0.50635 | ppl    1.659\n",
            "| epoch   9 |   200/ 1600 steps | loss 0.63782 | ppl    1.892\n",
            "| epoch   9 |   250/ 1600 steps | loss 0.98509 | ppl    2.678\n",
            "| epoch   9 |   300/ 1600 steps | loss 0.63764 | ppl    1.892\n",
            "| epoch   9 |   350/ 1600 steps | loss 0.53765 | ppl    1.712\n",
            "| epoch   9 |   400/ 1600 steps | loss 0.67191 | ppl    1.958\n",
            "| epoch   9 |   450/ 1600 steps | loss 0.60581 | ppl    1.833\n",
            "| epoch   9 |   500/ 1600 steps | loss 0.58997 | ppl    1.804\n",
            "| epoch   9 |   550/ 1600 steps | loss 0.65769 | ppl    1.930\n",
            "| epoch   9 |   600/ 1600 steps | loss 0.72364 | ppl    2.062\n",
            "| epoch   9 |   650/ 1600 steps | loss 0.81294 | ppl    2.255\n",
            "| epoch   9 |   700/ 1600 steps | loss 0.85620 | ppl    2.354\n",
            "| epoch   9 |   750/ 1600 steps | loss 0.85812 | ppl    2.359\n",
            "| epoch   9 |   800/ 1600 steps | loss 0.63370 | ppl    1.885\n",
            "| epoch   9 |   850/ 1600 steps | loss 0.70238 | ppl    2.019\n",
            "| epoch   9 |   900/ 1600 steps | loss 0.79844 | ppl    2.222\n",
            "| epoch   9 |   950/ 1600 steps | loss 0.90163 | ppl    2.464\n",
            "| epoch   9 |  1000/ 1600 steps | loss 0.71199 | ppl    2.038\n",
            "| epoch   9 |  1050/ 1600 steps | loss 0.85849 | ppl    2.360\n",
            "| epoch   9 |  1100/ 1600 steps | loss 0.51063 | ppl    1.666\n",
            "| epoch   9 |  1150/ 1600 steps | loss 0.95275 | ppl    2.593\n",
            "| epoch   9 |  1200/ 1600 steps | loss 0.96526 | ppl    2.625\n",
            "| epoch   9 |  1250/ 1600 steps | loss 0.50998 | ppl    1.665\n",
            "| epoch   9 |  1300/ 1600 steps | loss 1.19535 | ppl    3.305\n",
            "| epoch   9 |  1350/ 1600 steps | loss 0.80995 | ppl    2.248\n",
            "| epoch   9 |  1400/ 1600 steps | loss 0.82044 | ppl    2.271\n",
            "| epoch   9 |  1450/ 1600 steps | loss 0.50751 | ppl    1.661\n",
            "| epoch   9 |  1500/ 1600 steps | loss 0.49880 | ppl    1.647\n",
            "| epoch   9 |  1550/ 1600 steps | loss 0.85114 | ppl    2.342\n",
            "from_scratch=False - VALIDATION ACCURACY 0.705\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  10 |    50/ 1600 steps | loss 0.98250 | ppl    2.671\n",
            "| epoch  10 |   100/ 1600 steps | loss 0.56084 | ppl    1.752\n",
            "| epoch  10 |   150/ 1600 steps | loss 0.73000 | ppl    2.075\n",
            "| epoch  10 |   200/ 1600 steps | loss 0.88218 | ppl    2.416\n",
            "| epoch  10 |   250/ 1600 steps | loss 0.98686 | ppl    2.683\n",
            "| epoch  10 |   300/ 1600 steps | loss 0.61906 | ppl    1.857\n",
            "| epoch  10 |   350/ 1600 steps | loss 0.67041 | ppl    1.955\n",
            "| epoch  10 |   400/ 1600 steps | loss 0.86720 | ppl    2.380\n",
            "| epoch  10 |   450/ 1600 steps | loss 0.83563 | ppl    2.306\n",
            "| epoch  10 |   500/ 1600 steps | loss 0.56376 | ppl    1.757\n",
            "| epoch  10 |   550/ 1600 steps | loss 0.59976 | ppl    1.822\n",
            "| epoch  10 |   600/ 1600 steps | loss 0.81487 | ppl    2.259\n",
            "| epoch  10 |   650/ 1600 steps | loss 0.77694 | ppl    2.175\n",
            "| epoch  10 |   700/ 1600 steps | loss 0.64088 | ppl    1.898\n",
            "| epoch  10 |   750/ 1600 steps | loss 0.61945 | ppl    1.858\n",
            "| epoch  10 |   800/ 1600 steps | loss 0.72879 | ppl    2.073\n",
            "| epoch  10 |   850/ 1600 steps | loss 0.51674 | ppl    1.677\n",
            "| epoch  10 |   900/ 1600 steps | loss 0.93909 | ppl    2.558\n",
            "| epoch  10 |   950/ 1600 steps | loss 0.65171 | ppl    1.919\n",
            "| epoch  10 |  1000/ 1600 steps | loss 0.64795 | ppl    1.912\n",
            "| epoch  10 |  1050/ 1600 steps | loss 0.44457 | ppl    1.560\n",
            "| epoch  10 |  1100/ 1600 steps | loss 0.77943 | ppl    2.180\n",
            "| epoch  10 |  1150/ 1600 steps | loss 1.19499 | ppl    3.304\n",
            "| epoch  10 |  1200/ 1600 steps | loss 0.66851 | ppl    1.951\n",
            "| epoch  10 |  1250/ 1600 steps | loss 0.54073 | ppl    1.717\n",
            "| epoch  10 |  1300/ 1600 steps | loss 0.42562 | ppl    1.531\n",
            "| epoch  10 |  1350/ 1600 steps | loss 1.16771 | ppl    3.215\n",
            "| epoch  10 |  1400/ 1600 steps | loss 0.71045 | ppl    2.035\n",
            "| epoch  10 |  1450/ 1600 steps | loss 1.03565 | ppl    2.817\n",
            "| epoch  10 |  1500/ 1600 steps | loss 0.43877 | ppl    1.551\n",
            "| epoch  10 |  1550/ 1600 steps | loss 0.77202 | ppl    2.164\n",
            "from_scratch=False - VALIDATION ACCURACY 0.707\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  11 |    50/ 1600 steps | loss 1.02880 | ppl    2.798\n",
            "| epoch  11 |   100/ 1600 steps | loss 0.43157 | ppl    1.540\n",
            "| epoch  11 |   150/ 1600 steps | loss 0.82320 | ppl    2.278\n",
            "| epoch  11 |   200/ 1600 steps | loss 0.80989 | ppl    2.248\n",
            "| epoch  11 |   250/ 1600 steps | loss 0.56055 | ppl    1.752\n",
            "| epoch  11 |   300/ 1600 steps | loss 0.74229 | ppl    2.101\n",
            "| epoch  11 |   350/ 1600 steps | loss 1.09897 | ppl    3.001\n",
            "| epoch  11 |   400/ 1600 steps | loss 1.05078 | ppl    2.860\n",
            "| epoch  11 |   450/ 1600 steps | loss 0.58257 | ppl    1.791\n",
            "| epoch  11 |   500/ 1600 steps | loss 0.59067 | ppl    1.805\n",
            "| epoch  11 |   550/ 1600 steps | loss 0.85064 | ppl    2.341\n",
            "| epoch  11 |   600/ 1600 steps | loss 0.91849 | ppl    2.506\n",
            "| epoch  11 |   650/ 1600 steps | loss 0.85981 | ppl    2.363\n",
            "| epoch  11 |   700/ 1600 steps | loss 0.47282 | ppl    1.605\n",
            "| epoch  11 |   750/ 1600 steps | loss 1.07048 | ppl    2.917\n",
            "| epoch  11 |   800/ 1600 steps | loss 0.71745 | ppl    2.049\n",
            "| epoch  11 |   850/ 1600 steps | loss 0.50767 | ppl    1.661\n",
            "| epoch  11 |   900/ 1600 steps | loss 0.73255 | ppl    2.080\n",
            "| epoch  11 |   950/ 1600 steps | loss 0.92157 | ppl    2.513\n",
            "| epoch  11 |  1000/ 1600 steps | loss 0.55176 | ppl    1.736\n",
            "| epoch  11 |  1050/ 1600 steps | loss 0.89859 | ppl    2.456\n",
            "| epoch  11 |  1100/ 1600 steps | loss 0.53604 | ppl    1.709\n",
            "| epoch  11 |  1150/ 1600 steps | loss 0.75140 | ppl    2.120\n",
            "| epoch  11 |  1200/ 1600 steps | loss 0.91513 | ppl    2.497\n",
            "| epoch  11 |  1250/ 1600 steps | loss 0.71058 | ppl    2.035\n",
            "| epoch  11 |  1300/ 1600 steps | loss 0.87723 | ppl    2.404\n",
            "| epoch  11 |  1350/ 1600 steps | loss 0.60705 | ppl    1.835\n",
            "| epoch  11 |  1400/ 1600 steps | loss 0.46511 | ppl    1.592\n",
            "| epoch  11 |  1450/ 1600 steps | loss 0.69309 | ppl    2.000\n",
            "| epoch  11 |  1500/ 1600 steps | loss 0.67577 | ppl    1.966\n",
            "| epoch  11 |  1550/ 1600 steps | loss 0.96620 | ppl    2.628\n",
            "from_scratch=False - VALIDATION ACCURACY 0.707\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  12 |    50/ 1600 steps | loss 0.60639 | ppl    1.834\n",
            "| epoch  12 |   100/ 1600 steps | loss 0.71668 | ppl    2.048\n",
            "| epoch  12 |   150/ 1600 steps | loss 0.61098 | ppl    1.842\n",
            "| epoch  12 |   200/ 1600 steps | loss 0.75380 | ppl    2.125\n",
            "| epoch  12 |   250/ 1600 steps | loss 0.61606 | ppl    1.852\n",
            "| epoch  12 |   300/ 1600 steps | loss 1.18780 | ppl    3.280\n",
            "| epoch  12 |   350/ 1600 steps | loss 1.01225 | ppl    2.752\n",
            "| epoch  12 |   400/ 1600 steps | loss 0.61861 | ppl    1.856\n",
            "| epoch  12 |   450/ 1600 steps | loss 0.89861 | ppl    2.456\n",
            "| epoch  12 |   500/ 1600 steps | loss 0.75628 | ppl    2.130\n",
            "| epoch  12 |   550/ 1600 steps | loss 0.65684 | ppl    1.929\n",
            "| epoch  12 |   600/ 1600 steps | loss 0.64818 | ppl    1.912\n",
            "| epoch  12 |   650/ 1600 steps | loss 0.75663 | ppl    2.131\n",
            "| epoch  12 |   700/ 1600 steps | loss 1.04819 | ppl    2.852\n",
            "| epoch  12 |   750/ 1600 steps | loss 0.75905 | ppl    2.136\n",
            "| epoch  12 |   800/ 1600 steps | loss 0.72802 | ppl    2.071\n",
            "| epoch  12 |   850/ 1600 steps | loss 0.56082 | ppl    1.752\n",
            "| epoch  12 |   900/ 1600 steps | loss 0.57237 | ppl    1.772\n",
            "| epoch  12 |   950/ 1600 steps | loss 0.69038 | ppl    1.994\n",
            "| epoch  12 |  1000/ 1600 steps | loss 0.81702 | ppl    2.264\n",
            "| epoch  12 |  1050/ 1600 steps | loss 0.76678 | ppl    2.153\n",
            "| epoch  12 |  1100/ 1600 steps | loss 0.56072 | ppl    1.752\n",
            "| epoch  12 |  1150/ 1600 steps | loss 0.71621 | ppl    2.047\n",
            "| epoch  12 |  1200/ 1600 steps | loss 0.73549 | ppl    2.087\n",
            "| epoch  12 |  1250/ 1600 steps | loss 1.03776 | ppl    2.823\n",
            "| epoch  12 |  1300/ 1600 steps | loss 0.70502 | ppl    2.024\n",
            "| epoch  12 |  1350/ 1600 steps | loss 1.00036 | ppl    2.719\n",
            "| epoch  12 |  1400/ 1600 steps | loss 0.70956 | ppl    2.033\n",
            "| epoch  12 |  1450/ 1600 steps | loss 0.98360 | ppl    2.674\n",
            "| epoch  12 |  1500/ 1600 steps | loss 0.82315 | ppl    2.278\n",
            "| epoch  12 |  1550/ 1600 steps | loss 0.49090 | ppl    1.634\n",
            "from_scratch=False - VALIDATION ACCURACY 0.7115\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  13 |    50/ 1600 steps | loss 0.69681 | ppl    2.007\n",
            "| epoch  13 |   100/ 1600 steps | loss 0.75929 | ppl    2.137\n",
            "| epoch  13 |   150/ 1600 steps | loss 0.60150 | ppl    1.825\n",
            "| epoch  13 |   200/ 1600 steps | loss 0.86463 | ppl    2.374\n",
            "| epoch  13 |   250/ 1600 steps | loss 0.84753 | ppl    2.334\n",
            "| epoch  13 |   300/ 1600 steps | loss 0.72020 | ppl    2.055\n",
            "| epoch  13 |   350/ 1600 steps | loss 0.51041 | ppl    1.666\n",
            "| epoch  13 |   400/ 1600 steps | loss 0.66016 | ppl    1.935\n",
            "| epoch  13 |   450/ 1600 steps | loss 1.01044 | ppl    2.747\n",
            "| epoch  13 |   500/ 1600 steps | loss 0.86163 | ppl    2.367\n",
            "| epoch  13 |   550/ 1600 steps | loss 0.55672 | ppl    1.745\n",
            "| epoch  13 |   600/ 1600 steps | loss 0.71416 | ppl    2.042\n",
            "| epoch  13 |   650/ 1600 steps | loss 1.00043 | ppl    2.719\n",
            "| epoch  13 |   700/ 1600 steps | loss 0.55279 | ppl    1.738\n",
            "| epoch  13 |   750/ 1600 steps | loss 0.97413 | ppl    2.649\n",
            "| epoch  13 |   800/ 1600 steps | loss 0.74841 | ppl    2.114\n",
            "| epoch  13 |   850/ 1600 steps | loss 0.76607 | ppl    2.151\n",
            "| epoch  13 |   900/ 1600 steps | loss 0.57650 | ppl    1.780\n",
            "| epoch  13 |   950/ 1600 steps | loss 1.25756 | ppl    3.517\n",
            "| epoch  13 |  1000/ 1600 steps | loss 0.95961 | ppl    2.611\n",
            "| epoch  13 |  1050/ 1600 steps | loss 0.53103 | ppl    1.701\n",
            "| epoch  13 |  1100/ 1600 steps | loss 0.76070 | ppl    2.140\n",
            "| epoch  13 |  1150/ 1600 steps | loss 0.87037 | ppl    2.388\n",
            "| epoch  13 |  1200/ 1600 steps | loss 0.83616 | ppl    2.307\n",
            "| epoch  13 |  1250/ 1600 steps | loss 0.60400 | ppl    1.829\n",
            "| epoch  13 |  1300/ 1600 steps | loss 0.74319 | ppl    2.103\n",
            "| epoch  13 |  1350/ 1600 steps | loss 0.66010 | ppl    1.935\n",
            "| epoch  13 |  1400/ 1600 steps | loss 0.96667 | ppl    2.629\n",
            "| epoch  13 |  1450/ 1600 steps | loss 0.54143 | ppl    1.718\n",
            "| epoch  13 |  1500/ 1600 steps | loss 0.59999 | ppl    1.822\n",
            "| epoch  13 |  1550/ 1600 steps | loss 0.70421 | ppl    2.022\n",
            "from_scratch=False - VALIDATION ACCURACY 0.708\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  14 |    50/ 1600 steps | loss 0.96897 | ppl    2.635\n",
            "| epoch  14 |   100/ 1600 steps | loss 0.63762 | ppl    1.892\n",
            "| epoch  14 |   150/ 1600 steps | loss 0.41793 | ppl    1.519\n",
            "| epoch  14 |   200/ 1600 steps | loss 0.84764 | ppl    2.334\n",
            "| epoch  14 |   250/ 1600 steps | loss 0.76877 | ppl    2.157\n",
            "| epoch  14 |   300/ 1600 steps | loss 0.85925 | ppl    2.361\n",
            "| epoch  14 |   350/ 1600 steps | loss 0.77390 | ppl    2.168\n",
            "| epoch  14 |   400/ 1600 steps | loss 0.76407 | ppl    2.147\n",
            "| epoch  14 |   450/ 1600 steps | loss 0.49774 | ppl    1.645\n",
            "| epoch  14 |   500/ 1600 steps | loss 0.51750 | ppl    1.678\n",
            "| epoch  14 |   550/ 1600 steps | loss 0.75604 | ppl    2.130\n",
            "| epoch  14 |   600/ 1600 steps | loss 0.98199 | ppl    2.670\n",
            "| epoch  14 |   650/ 1600 steps | loss 1.04651 | ppl    2.848\n",
            "| epoch  14 |   700/ 1600 steps | loss 0.50445 | ppl    1.656\n",
            "| epoch  14 |   750/ 1600 steps | loss 0.58175 | ppl    1.789\n",
            "| epoch  14 |   800/ 1600 steps | loss 0.75674 | ppl    2.131\n",
            "| epoch  14 |   850/ 1600 steps | loss 0.78168 | ppl    2.185\n",
            "| epoch  14 |   900/ 1600 steps | loss 0.53660 | ppl    1.710\n",
            "| epoch  14 |   950/ 1600 steps | loss 0.97231 | ppl    2.644\n",
            "| epoch  14 |  1000/ 1600 steps | loss 0.71926 | ppl    2.053\n",
            "| epoch  14 |  1050/ 1600 steps | loss 0.62168 | ppl    1.862\n",
            "| epoch  14 |  1100/ 1600 steps | loss 0.89328 | ppl    2.443\n",
            "| epoch  14 |  1150/ 1600 steps | loss 0.83047 | ppl    2.294\n",
            "| epoch  14 |  1200/ 1600 steps | loss 0.58917 | ppl    1.802\n",
            "| epoch  14 |  1250/ 1600 steps | loss 0.44819 | ppl    1.565\n",
            "| epoch  14 |  1300/ 1600 steps | loss 0.71549 | ppl    2.045\n",
            "| epoch  14 |  1350/ 1600 steps | loss 0.90588 | ppl    2.474\n",
            "| epoch  14 |  1400/ 1600 steps | loss 0.99945 | ppl    2.717\n",
            "| epoch  14 |  1450/ 1600 steps | loss 0.75154 | ppl    2.120\n",
            "| epoch  14 |  1500/ 1600 steps | loss 0.96463 | ppl    2.624\n",
            "| epoch  14 |  1550/ 1600 steps | loss 0.80898 | ppl    2.246\n",
            "from_scratch=False - VALIDATION ACCURACY 0.708\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  15 |    50/ 1600 steps | loss 0.78818 | ppl    2.199\n",
            "| epoch  15 |   100/ 1600 steps | loss 0.77472 | ppl    2.170\n",
            "| epoch  15 |   150/ 1600 steps | loss 0.36839 | ppl    1.445\n",
            "| epoch  15 |   200/ 1600 steps | loss 0.79702 | ppl    2.219\n",
            "| epoch  15 |   250/ 1600 steps | loss 1.07059 | ppl    2.917\n",
            "| epoch  15 |   300/ 1600 steps | loss 0.58111 | ppl    1.788\n",
            "| epoch  15 |   350/ 1600 steps | loss 0.53441 | ppl    1.706\n",
            "| epoch  15 |   400/ 1600 steps | loss 0.67194 | ppl    1.958\n",
            "| epoch  15 |   450/ 1600 steps | loss 0.95906 | ppl    2.609\n",
            "| epoch  15 |   500/ 1600 steps | loss 1.03757 | ppl    2.822\n",
            "| epoch  15 |   550/ 1600 steps | loss 0.42509 | ppl    1.530\n",
            "| epoch  15 |   600/ 1600 steps | loss 0.90846 | ppl    2.480\n",
            "| epoch  15 |   650/ 1600 steps | loss 1.03411 | ppl    2.813\n",
            "| epoch  15 |   700/ 1600 steps | loss 1.09847 | ppl    3.000\n",
            "| epoch  15 |   750/ 1600 steps | loss 0.49212 | ppl    1.636\n",
            "| epoch  15 |   800/ 1600 steps | loss 0.58051 | ppl    1.787\n",
            "| epoch  15 |   850/ 1600 steps | loss 0.84189 | ppl    2.321\n",
            "| epoch  15 |   900/ 1600 steps | loss 0.36907 | ppl    1.446\n",
            "| epoch  15 |   950/ 1600 steps | loss 0.56558 | ppl    1.760\n",
            "| epoch  15 |  1000/ 1600 steps | loss 0.58157 | ppl    1.789\n",
            "| epoch  15 |  1050/ 1600 steps | loss 0.72744 | ppl    2.070\n",
            "| epoch  15 |  1100/ 1600 steps | loss 1.05054 | ppl    2.859\n",
            "| epoch  15 |  1150/ 1600 steps | loss 0.95264 | ppl    2.593\n",
            "| epoch  15 |  1200/ 1600 steps | loss 0.45179 | ppl    1.571\n",
            "| epoch  15 |  1250/ 1600 steps | loss 0.97959 | ppl    2.663\n",
            "| epoch  15 |  1300/ 1600 steps | loss 0.97879 | ppl    2.661\n",
            "| epoch  15 |  1350/ 1600 steps | loss 1.13193 | ppl    3.102\n",
            "| epoch  15 |  1400/ 1600 steps | loss 0.74133 | ppl    2.099\n",
            "| epoch  15 |  1450/ 1600 steps | loss 0.59961 | ppl    1.821\n",
            "| epoch  15 |  1500/ 1600 steps | loss 0.72986 | ppl    2.075\n",
            "| epoch  15 |  1550/ 1600 steps | loss 0.77781 | ppl    2.177\n",
            "from_scratch=False - VALIDATION ACCURACY 0.706\n",
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/ 1600 steps | loss 1.91900 | ppl    6.814\n",
            "| epoch   1 |   100/ 1600 steps | loss 1.37424 | ppl    3.952\n",
            "| epoch   1 |   150/ 1600 steps | loss 1.59077 | ppl    4.908\n",
            "| epoch   1 |   200/ 1600 steps | loss 1.60169 | ppl    4.961\n",
            "| epoch   1 |   250/ 1600 steps | loss 1.34759 | ppl    3.848\n",
            "| epoch   1 |   300/ 1600 steps | loss 1.48798 | ppl    4.428\n",
            "| epoch   1 |   350/ 1600 steps | loss 1.56815 | ppl    4.798\n",
            "| epoch   1 |   400/ 1600 steps | loss 1.42898 | ppl    4.174\n",
            "| epoch   1 |   450/ 1600 steps | loss 1.81977 | ppl    6.170\n",
            "| epoch   1 |   500/ 1600 steps | loss 1.48241 | ppl    4.404\n",
            "| epoch   1 |   550/ 1600 steps | loss 1.68783 | ppl    5.408\n",
            "| epoch   1 |   600/ 1600 steps | loss 1.27821 | ppl    3.590\n",
            "| epoch   1 |   650/ 1600 steps | loss 1.26030 | ppl    3.526\n",
            "| epoch   1 |   700/ 1600 steps | loss 0.97957 | ppl    2.663\n",
            "| epoch   1 |   750/ 1600 steps | loss 1.13044 | ppl    3.097\n",
            "| epoch   1 |   800/ 1600 steps | loss 1.61858 | ppl    5.046\n",
            "| epoch   1 |   850/ 1600 steps | loss 1.06909 | ppl    2.913\n",
            "| epoch   1 |   900/ 1600 steps | loss 1.81585 | ppl    6.146\n",
            "| epoch   1 |   950/ 1600 steps | loss 0.91360 | ppl    2.493\n",
            "| epoch   1 |  1000/ 1600 steps | loss 1.12557 | ppl    3.082\n",
            "| epoch   1 |  1050/ 1600 steps | loss 1.19890 | ppl    3.316\n",
            "| epoch   1 |  1100/ 1600 steps | loss 0.95679 | ppl    2.603\n",
            "| epoch   1 |  1150/ 1600 steps | loss 1.45466 | ppl    4.283\n",
            "| epoch   1 |  1200/ 1600 steps | loss 1.40708 | ppl    4.084\n",
            "| epoch   1 |  1250/ 1600 steps | loss 1.34834 | ppl    3.851\n",
            "| epoch   1 |  1300/ 1600 steps | loss 1.62096 | ppl    5.058\n",
            "| epoch   1 |  1350/ 1600 steps | loss 1.68223 | ppl    5.378\n",
            "| epoch   1 |  1400/ 1600 steps | loss 1.66453 | ppl    5.283\n",
            "| epoch   1 |  1450/ 1600 steps | loss 1.32208 | ppl    3.751\n",
            "| epoch   1 |  1500/ 1600 steps | loss 1.66026 | ppl    5.261\n",
            "| epoch   1 |  1550/ 1600 steps | loss 1.29354 | ppl    3.646\n",
            "from_scratch=True - VALIDATION ACCURACY 0.6195\n",
            "| epoch   2 |    50/ 1600 steps | loss 1.54039 | ppl    4.666\n",
            "| epoch   2 |   100/ 1600 steps | loss 1.53685 | ppl    4.650\n",
            "| epoch   2 |   150/ 1600 steps | loss 1.07172 | ppl    2.920\n",
            "| epoch   2 |   200/ 1600 steps | loss 1.50103 | ppl    4.486\n",
            "| epoch   2 |   250/ 1600 steps | loss 1.29389 | ppl    3.647\n",
            "| epoch   2 |   300/ 1600 steps | loss 0.57736 | ppl    1.781\n",
            "| epoch   2 |   350/ 1600 steps | loss 1.48114 | ppl    4.398\n",
            "| epoch   2 |   400/ 1600 steps | loss 1.23550 | ppl    3.440\n",
            "| epoch   2 |   450/ 1600 steps | loss 1.34306 | ppl    3.831\n",
            "| epoch   2 |   500/ 1600 steps | loss 1.16659 | ppl    3.211\n",
            "| epoch   2 |   550/ 1600 steps | loss 1.38819 | ppl    4.008\n",
            "| epoch   2 |   600/ 1600 steps | loss 0.89134 | ppl    2.438\n",
            "| epoch   2 |   650/ 1600 steps | loss 1.61386 | ppl    5.022\n",
            "| epoch   2 |   700/ 1600 steps | loss 1.44606 | ppl    4.246\n",
            "| epoch   2 |   750/ 1600 steps | loss 1.18311 | ppl    3.265\n",
            "| epoch   2 |   800/ 1600 steps | loss 1.20946 | ppl    3.352\n",
            "| epoch   2 |   850/ 1600 steps | loss 1.05399 | ppl    2.869\n",
            "| epoch   2 |   900/ 1600 steps | loss 1.27392 | ppl    3.575\n",
            "| epoch   2 |   950/ 1600 steps | loss 1.39626 | ppl    4.040\n",
            "| epoch   2 |  1000/ 1600 steps | loss 0.76202 | ppl    2.143\n",
            "| epoch   2 |  1050/ 1600 steps | loss 1.15911 | ppl    3.187\n",
            "| epoch   2 |  1100/ 1600 steps | loss 0.95860 | ppl    2.608\n",
            "| epoch   2 |  1150/ 1600 steps | loss 0.88011 | ppl    2.411\n",
            "| epoch   2 |  1200/ 1600 steps | loss 1.33344 | ppl    3.794\n",
            "| epoch   2 |  1250/ 1600 steps | loss 1.21647 | ppl    3.375\n",
            "| epoch   2 |  1300/ 1600 steps | loss 1.03088 | ppl    2.804\n",
            "| epoch   2 |  1350/ 1600 steps | loss 1.34018 | ppl    3.820\n",
            "| epoch   2 |  1400/ 1600 steps | loss 1.07773 | ppl    2.938\n",
            "| epoch   2 |  1450/ 1600 steps | loss 0.92656 | ppl    2.526\n",
            "| epoch   2 |  1500/ 1600 steps | loss 1.52935 | ppl    4.615\n",
            "| epoch   2 |  1550/ 1600 steps | loss 0.64954 | ppl    1.915\n",
            "from_scratch=True - VALIDATION ACCURACY 0.671\n",
            "| epoch   3 |    50/ 1600 steps | loss 0.73646 | ppl    2.089\n",
            "| epoch   3 |   100/ 1600 steps | loss 0.45056 | ppl    1.569\n",
            "| epoch   3 |   150/ 1600 steps | loss 0.23954 | ppl    1.271\n",
            "| epoch   3 |   200/ 1600 steps | loss 0.65516 | ppl    1.925\n",
            "| epoch   3 |   250/ 1600 steps | loss 0.57488 | ppl    1.777\n",
            "| epoch   3 |   300/ 1600 steps | loss 0.32971 | ppl    1.391\n",
            "| epoch   3 |   350/ 1600 steps | loss 0.81273 | ppl    2.254\n",
            "| epoch   3 |   400/ 1600 steps | loss 0.26180 | ppl    1.299\n",
            "| epoch   3 |   450/ 1600 steps | loss 0.79803 | ppl    2.221\n",
            "| epoch   3 |   500/ 1600 steps | loss 0.54371 | ppl    1.722\n",
            "| epoch   3 |   550/ 1600 steps | loss 0.58551 | ppl    1.796\n",
            "| epoch   3 |   600/ 1600 steps | loss 0.92287 | ppl    2.516\n",
            "| epoch   3 |   650/ 1600 steps | loss 0.69728 | ppl    2.008\n",
            "| epoch   3 |   700/ 1600 steps | loss 0.31706 | ppl    1.373\n",
            "| epoch   3 |   750/ 1600 steps | loss 0.50334 | ppl    1.654\n",
            "| epoch   3 |   800/ 1600 steps | loss 0.50253 | ppl    1.653\n",
            "| epoch   3 |   850/ 1600 steps | loss 0.52840 | ppl    1.696\n",
            "| epoch   3 |   900/ 1600 steps | loss 0.28944 | ppl    1.336\n",
            "| epoch   3 |   950/ 1600 steps | loss 0.45772 | ppl    1.580\n",
            "| epoch   3 |  1000/ 1600 steps | loss 1.03759 | ppl    2.822\n",
            "| epoch   3 |  1050/ 1600 steps | loss 0.57405 | ppl    1.775\n",
            "| epoch   3 |  1100/ 1600 steps | loss 0.52963 | ppl    1.698\n",
            "| epoch   3 |  1150/ 1600 steps | loss 0.72455 | ppl    2.064\n",
            "| epoch   3 |  1200/ 1600 steps | loss 0.26052 | ppl    1.298\n",
            "| epoch   3 |  1250/ 1600 steps | loss 0.76058 | ppl    2.140\n",
            "| epoch   3 |  1300/ 1600 steps | loss 0.48168 | ppl    1.619\n",
            "| epoch   3 |  1350/ 1600 steps | loss 0.33728 | ppl    1.401\n",
            "| epoch   3 |  1400/ 1600 steps | loss 0.68428 | ppl    1.982\n",
            "| epoch   3 |  1450/ 1600 steps | loss 0.52552 | ppl    1.691\n",
            "| epoch   3 |  1500/ 1600 steps | loss 0.58370 | ppl    1.793\n",
            "| epoch   3 |  1550/ 1600 steps | loss 0.49885 | ppl    1.647\n",
            "from_scratch=True - VALIDATION ACCURACY 0.696\n",
            "| epoch   4 |    50/ 1600 steps | loss 0.12522 | ppl    1.133\n",
            "| epoch   4 |   100/ 1600 steps | loss 0.04180 | ppl    1.043\n",
            "| epoch   4 |   150/ 1600 steps | loss 0.16807 | ppl    1.183\n",
            "| epoch   4 |   200/ 1600 steps | loss 0.00147 | ppl    1.001\n",
            "| epoch   4 |   250/ 1600 steps | loss 0.17167 | ppl    1.187\n",
            "| epoch   4 |   300/ 1600 steps | loss 0.19051 | ppl    1.210\n",
            "| epoch   4 |   350/ 1600 steps | loss 0.17382 | ppl    1.190\n",
            "| epoch   4 |   400/ 1600 steps | loss 0.00019 | ppl    1.000\n",
            "| epoch   4 |   450/ 1600 steps | loss 0.11632 | ppl    1.123\n",
            "| epoch   4 |   500/ 1600 steps | loss 0.37517 | ppl    1.455\n",
            "| epoch   4 |   550/ 1600 steps | loss 0.01178 | ppl    1.012\n",
            "| epoch   4 |   600/ 1600 steps | loss 0.12370 | ppl    1.132\n",
            "| epoch   4 |   650/ 1600 steps | loss 0.01724 | ppl    1.017\n",
            "| epoch   4 |   700/ 1600 steps | loss 0.23363 | ppl    1.263\n",
            "| epoch   4 |   750/ 1600 steps | loss 0.11004 | ppl    1.116\n",
            "| epoch   4 |   800/ 1600 steps | loss 0.78313 | ppl    2.188\n",
            "| epoch   4 |   850/ 1600 steps | loss 0.31446 | ppl    1.370\n",
            "| epoch   4 |   900/ 1600 steps | loss 0.00423 | ppl    1.004\n",
            "| epoch   4 |   950/ 1600 steps | loss 0.00018 | ppl    1.000\n",
            "| epoch   4 |  1000/ 1600 steps | loss 0.13760 | ppl    1.148\n",
            "| epoch   4 |  1050/ 1600 steps | loss 0.06795 | ppl    1.070\n",
            "| epoch   4 |  1100/ 1600 steps | loss 0.20676 | ppl    1.230\n",
            "| epoch   4 |  1150/ 1600 steps | loss 0.00021 | ppl    1.000\n",
            "| epoch   4 |  1200/ 1600 steps | loss 0.00336 | ppl    1.003\n",
            "| epoch   4 |  1250/ 1600 steps | loss 0.16976 | ppl    1.185\n",
            "| epoch   4 |  1300/ 1600 steps | loss 0.00011 | ppl    1.000\n",
            "| epoch   4 |  1350/ 1600 steps | loss 0.00017 | ppl    1.000\n",
            "| epoch   4 |  1400/ 1600 steps | loss 0.10660 | ppl    1.112\n",
            "| epoch   4 |  1450/ 1600 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch   4 |  1500/ 1600 steps | loss 0.28442 | ppl    1.329\n",
            "| epoch   4 |  1550/ 1600 steps | loss 0.16876 | ppl    1.184\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7145\n",
            "| epoch   5 |    50/ 1600 steps | loss 0.26248 | ppl    1.300\n",
            "| epoch   5 |   100/ 1600 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch   5 |   150/ 1600 steps | loss 0.00041 | ppl    1.000\n",
            "| epoch   5 |   200/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   5 |   250/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |   300/ 1600 steps | loss 0.13410 | ppl    1.144\n",
            "| epoch   5 |   350/ 1600 steps | loss 0.09217 | ppl    1.097\n",
            "| epoch   5 |   400/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   450/ 1600 steps | loss 0.19974 | ppl    1.221\n",
            "| epoch   5 |   500/ 1600 steps | loss 0.00023 | ppl    1.000\n",
            "| epoch   5 |   550/ 1600 steps | loss 0.08350 | ppl    1.087\n",
            "| epoch   5 |   600/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   650/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |   700/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   750/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |   800/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   5 |   850/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |   900/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |   950/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   5 |  1000/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |  1050/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1100/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   5 |  1200/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1250/ 1600 steps | loss 0.19865 | ppl    1.220\n",
            "| epoch   5 |  1300/ 1600 steps | loss 0.30448 | ppl    1.356\n",
            "| epoch   5 |  1350/ 1600 steps | loss 0.20644 | ppl    1.229\n",
            "| epoch   5 |  1400/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   5 |  1450/ 1600 steps | loss 0.45035 | ppl    1.569\n",
            "| epoch   5 |  1500/ 1600 steps | loss 0.01253 | ppl    1.013\n",
            "| epoch   5 |  1550/ 1600 steps | loss 0.05182 | ppl    1.053\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7105\n",
            "| epoch   6 |    50/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   6 |   100/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   6 |   150/ 1600 steps | loss 0.12902 | ppl    1.138\n",
            "| epoch   6 |   200/ 1600 steps | loss 0.00848 | ppl    1.009\n",
            "| epoch   6 |   250/ 1600 steps | loss 0.00404 | ppl    1.004\n",
            "| epoch   6 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   6 |   400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |   750/ 1600 steps | loss 0.01491 | ppl    1.015\n",
            "| epoch   6 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |   950/ 1600 steps | loss 0.15499 | ppl    1.168\n",
            "| epoch   6 |  1000/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   6 |  1050/ 1600 steps | loss 0.01068 | ppl    1.011\n",
            "| epoch   6 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   6 |  1250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |  1300/ 1600 steps | loss 0.00050 | ppl    1.000\n",
            "| epoch   6 |  1350/ 1600 steps | loss 0.32676 | ppl    1.386\n",
            "| epoch   6 |  1400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |  1450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |  1500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   6 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7445\n",
            "| epoch   7 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   7 |   100/ 1600 steps | loss 0.35721 | ppl    1.429\n",
            "| epoch   7 |   150/ 1600 steps | loss 0.00014 | ppl    1.000\n",
            "| epoch   7 |   200/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   7 |   250/ 1600 steps | loss 0.04578 | ppl    1.047\n",
            "| epoch   7 |   300/ 1600 steps | loss 0.10476 | ppl    1.110\n",
            "| epoch   7 |   350/ 1600 steps | loss 0.26193 | ppl    1.299\n",
            "| epoch   7 |   400/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   450/ 1600 steps | loss 0.00005 | ppl    1.000\n",
            "| epoch   7 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |   550/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   600/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   650/ 1600 steps | loss 0.35278 | ppl    1.423\n",
            "| epoch   7 |   700/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   750/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   800/ 1600 steps | loss 0.19040 | ppl    1.210\n",
            "| epoch   7 |   850/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   7 |   900/ 1600 steps | loss 0.01795 | ppl    1.018\n",
            "| epoch   7 |   950/ 1600 steps | loss 0.09142 | ppl    1.096\n",
            "| epoch   7 |  1000/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |  1050/ 1600 steps | loss 0.05006 | ppl    1.051\n",
            "| epoch   7 |  1100/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   7 |  1150/ 1600 steps | loss 0.10431 | ppl    1.110\n",
            "| epoch   7 |  1200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   7 |  1250/ 1600 steps | loss 0.26874 | ppl    1.308\n",
            "| epoch   7 |  1300/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   7 |  1350/ 1600 steps | loss 0.00643 | ppl    1.006\n",
            "| epoch   7 |  1400/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |  1450/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |  1500/ 1600 steps | loss 0.00087 | ppl    1.001\n",
            "| epoch   7 |  1550/ 1600 steps | loss 0.14608 | ppl    1.157\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7005\n",
            "| epoch   8 |    50/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   8 |   100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   150/ 1600 steps | loss 0.07529 | ppl    1.078\n",
            "| epoch   8 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |   350/ 1600 steps | loss 0.39214 | ppl    1.480\n",
            "| epoch   8 |   400/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch   8 |   450/ 1600 steps | loss 0.08353 | ppl    1.087\n",
            "| epoch   8 |   500/ 1600 steps | loss 0.00193 | ppl    1.002\n",
            "| epoch   8 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   750/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   850/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |   950/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1200/ 1600 steps | loss 0.21332 | ppl    1.238\n",
            "| epoch   8 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   8 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7385\n",
            "| epoch   9 |    50/ 1600 steps | loss 0.48446 | ppl    1.623\n",
            "| epoch   9 |   100/ 1600 steps | loss 0.19887 | ppl    1.220\n",
            "| epoch   9 |   150/ 1600 steps | loss 0.17743 | ppl    1.194\n",
            "| epoch   9 |   200/ 1600 steps | loss 0.00013 | ppl    1.000\n",
            "| epoch   9 |   250/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   9 |   300/ 1600 steps | loss 0.00008 | ppl    1.000\n",
            "| epoch   9 |   350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   400/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   9 |   450/ 1600 steps | loss 0.00007 | ppl    1.000\n",
            "| epoch   9 |   500/ 1600 steps | loss 0.16966 | ppl    1.185\n",
            "| epoch   9 |   550/ 1600 steps | loss 0.20215 | ppl    1.224\n",
            "| epoch   9 |   600/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   9 |   650/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   750/ 1600 steps | loss 0.20217 | ppl    1.224\n",
            "| epoch   9 |   800/ 1600 steps | loss 0.15196 | ppl    1.164\n",
            "| epoch   9 |   850/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |   900/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   950/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1000/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1200/ 1600 steps | loss 0.04760 | ppl    1.049\n",
            "| epoch   9 |  1250/ 1600 steps | loss 0.06402 | ppl    1.066\n",
            "| epoch   9 |  1300/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   9 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch   9 |  1400/ 1600 steps | loss 0.00202 | ppl    1.002\n",
            "| epoch   9 |  1450/ 1600 steps | loss 0.15244 | ppl    1.165\n",
            "| epoch   9 |  1500/ 1600 steps | loss 0.37700 | ppl    1.458\n",
            "| epoch   9 |  1550/ 1600 steps | loss 0.07810 | ppl    1.081\n",
            "from_scratch=True - VALIDATION ACCURACY 0.754\n",
            "| epoch  10 |    50/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   100/ 1600 steps | loss 0.55968 | ppl    1.750\n",
            "| epoch  10 |   150/ 1600 steps | loss 0.10115 | ppl    1.106\n",
            "| epoch  10 |   200/ 1600 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch  10 |   250/ 1600 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch  10 |   300/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   350/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   400/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  10 |   450/ 1600 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  10 |   500/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   650/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   700/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   750/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   800/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   850/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   900/ 1600 steps | loss 0.03213 | ppl    1.033\n",
            "| epoch  10 |   950/ 1600 steps | loss 0.17122 | ppl    1.187\n",
            "| epoch  10 |  1000/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1050/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1100/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1150/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  10 |  1250/ 1600 steps | loss 0.21288 | ppl    1.237\n",
            "| epoch  10 |  1300/ 1600 steps | loss 0.23928 | ppl    1.270\n",
            "| epoch  10 |  1350/ 1600 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch  10 |  1400/ 1600 steps | loss 0.09964 | ppl    1.105\n",
            "| epoch  10 |  1450/ 1600 steps | loss 0.13379 | ppl    1.143\n",
            "| epoch  10 |  1500/ 1600 steps | loss 0.00073 | ppl    1.001\n",
            "| epoch  10 |  1550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7565\n",
            "| epoch  11 |    50/ 1600 steps | loss 0.19193 | ppl    1.212\n",
            "| epoch  11 |   100/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  11 |   150/ 1600 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch  11 |   200/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   250/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   300/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   350/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   400/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   450/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   550/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   600/ 1600 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.765\n",
            "| epoch  12 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.764\n",
            "| epoch  13 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.765\n",
            "| epoch  14 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7665\n",
            "| epoch  15 |    50/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   600/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   650/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   700/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   750/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   800/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   850/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   900/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   950/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1000/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1050/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1100/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1150/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1200/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1250/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1300/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1350/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1400/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1450/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1500/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |  1550/ 1600 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.766\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [False, True]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model_ds = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model_ds.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model_ds.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    # @TODO: check batch size subtlety on the last element (we want to classify the last feature of the sentence!)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            model_ds,\n",
        "            downstream_path_data_train,\n",
        "            downstream_path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task=DS_TASK,\n",
        "            freeze_backbone=not from_scratch,\n",
        "            # batch_size=8,\n",
        "            batch_size=1, # TO AVOID THE NEED TO RETRIEVE THE RIGHT LAST TOKEN IN A BATCH\n",
        "            log_interval=50,\n",
        "            epoch=epoch,\n",
        "            name=\"_from_scratch\" if from_scratch else \"_pretrained\"\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            data_loader_validation,\n",
        "            model_ds\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "        print(f\"{from_scratch=} - VALIDATION ACCURACY {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RCpBIdTHojm6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(100.*np.array(from_scratch_valid_acc), \"-+r\", label=\"training from scratch\")\n",
        "plt.plot(100.*np.array(pretrained_valid_acc), \"-+b\", label=\"pretrained model\")\n",
        "plt.grid()\n",
        "plt.title(\"Classifier accuracy on a downstream task [Sentiment analyzis]\")\n",
        "plt.legend()\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====PRETRAINED MODEL======\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   1 |    50/  200 steps | loss 0.84755 | ppl    2.334\n",
            "| epoch   1 |   100/  200 steps | loss 0.81542 | ppl    2.260\n",
            "| epoch   1 |   150/  200 steps | loss 0.81310 | ppl    2.255\n",
            "from_scratch=False - VALIDATION ACCURACY 0.485\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   2 |    50/  200 steps | loss 0.82910 | ppl    2.291\n",
            "| epoch   2 |   100/  200 steps | loss 0.75473 | ppl    2.127\n",
            "| epoch   2 |   150/  200 steps | loss 0.77323 | ppl    2.167\n",
            "from_scratch=False - VALIDATION ACCURACY 0.525\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   3 |    50/  200 steps | loss 0.80890 | ppl    2.245\n",
            "| epoch   3 |   100/  200 steps | loss 0.70734 | ppl    2.029\n",
            "| epoch   3 |   150/  200 steps | loss 0.73741 | ppl    2.091\n",
            "from_scratch=False - VALIDATION ACCURACY 0.5595\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   4 |    50/  200 steps | loss 0.74747 | ppl    2.112\n",
            "| epoch   4 |   100/  200 steps | loss 0.72711 | ppl    2.069\n",
            "| epoch   4 |   150/  200 steps | loss 0.71059 | ppl    2.035\n",
            "from_scratch=False - VALIDATION ACCURACY 0.592\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   5 |    50/  200 steps | loss 0.71000 | ppl    2.034\n",
            "| epoch   5 |   100/  200 steps | loss 0.69792 | ppl    2.010\n",
            "| epoch   5 |   150/  200 steps | loss 0.68640 | ppl    1.987\n",
            "from_scratch=False - VALIDATION ACCURACY 0.614\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   6 |    50/  200 steps | loss 0.69634 | ppl    2.006\n",
            "| epoch   6 |   100/  200 steps | loss 0.67064 | ppl    1.955\n",
            "| epoch   6 |   150/  200 steps | loss 0.69645 | ppl    2.007\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6235\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   7 |    50/  200 steps | loss 0.70295 | ppl    2.020\n",
            "| epoch   7 |   100/  200 steps | loss 0.66076 | ppl    1.936\n",
            "| epoch   7 |   150/  200 steps | loss 0.70756 | ppl    2.029\n",
            "from_scratch=False - VALIDATION ACCURACY 0.638\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   8 |    50/  200 steps | loss 0.68062 | ppl    1.975\n",
            "| epoch   8 |   100/  200 steps | loss 0.63802 | ppl    1.893\n",
            "| epoch   8 |   150/  200 steps | loss 0.68882 | ppl    1.991\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6525\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch   9 |    50/  200 steps | loss 0.67686 | ppl    1.968\n",
            "| epoch   9 |   100/  200 steps | loss 0.63865 | ppl    1.894\n",
            "| epoch   9 |   150/  200 steps | loss 0.64587 | ppl    1.908\n",
            "from_scratch=False - VALIDATION ACCURACY 0.659\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  10 |    50/  200 steps | loss 0.66900 | ppl    1.952\n",
            "| epoch  10 |   100/  200 steps | loss 0.63422 | ppl    1.886\n",
            "| epoch  10 |   150/  200 steps | loss 0.64885 | ppl    1.913\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6545\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  11 |    50/  200 steps | loss 0.67135 | ppl    1.957\n",
            "| epoch  11 |   100/  200 steps | loss 0.63570 | ppl    1.888\n",
            "| epoch  11 |   150/  200 steps | loss 0.63885 | ppl    1.894\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6645\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  12 |    50/  200 steps | loss 0.63526 | ppl    1.888\n",
            "| epoch  12 |   100/  200 steps | loss 0.65079 | ppl    1.917\n",
            "| epoch  12 |   150/  200 steps | loss 0.64485 | ppl    1.906\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6665\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  13 |    50/  200 steps | loss 0.63500 | ppl    1.887\n",
            "| epoch  13 |   100/  200 steps | loss 0.63086 | ppl    1.879\n",
            "| epoch  13 |   150/  200 steps | loss 0.62019 | ppl    1.859\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6695\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  14 |    50/  200 steps | loss 0.61813 | ppl    1.855\n",
            "| epoch  14 |   100/  200 steps | loss 0.60252 | ppl    1.827\n",
            "| epoch  14 |   150/  200 steps | loss 0.66315 | ppl    1.941\n",
            "from_scratch=False - VALIDATION ACCURACY 0.684\n",
            "FREEZE TRANSFORMER BACKBONE\n",
            "| epoch  15 |    50/  200 steps | loss 0.62009 | ppl    1.859\n",
            "| epoch  15 |   100/  200 steps | loss 0.60173 | ppl    1.825\n",
            "| epoch  15 |   150/  200 steps | loss 0.60771 | ppl    1.836\n",
            "from_scratch=False - VALIDATION ACCURACY 0.6835\n",
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.77380 | ppl    2.168\n",
            "| epoch   1 |   100/  200 steps | loss 0.71998 | ppl    2.054\n",
            "| epoch   1 |   150/  200 steps | loss 0.71329 | ppl    2.041\n",
            "from_scratch=True - VALIDATION ACCURACY 0.5405\n",
            "| epoch   2 |    50/  200 steps | loss 0.62920 | ppl    1.876\n",
            "| epoch   2 |   100/  200 steps | loss 0.58830 | ppl    1.801\n",
            "| epoch   2 |   150/  200 steps | loss 0.60266 | ppl    1.827\n",
            "from_scratch=True - VALIDATION ACCURACY 0.719\n",
            "| epoch   3 |    50/  200 steps | loss 0.37435 | ppl    1.454\n",
            "| epoch   3 |   100/  200 steps | loss 0.34898 | ppl    1.418\n",
            "| epoch   3 |   150/  200 steps | loss 0.33941 | ppl    1.404\n",
            "from_scratch=True - VALIDATION ACCURACY 0.671\n",
            "| epoch   4 |    50/  200 steps | loss 0.17966 | ppl    1.197\n",
            "| epoch   4 |   100/  200 steps | loss 0.17204 | ppl    1.188\n",
            "| epoch   4 |   150/  200 steps | loss 0.14947 | ppl    1.161\n",
            "from_scratch=True - VALIDATION ACCURACY 0.698\n",
            "| epoch   5 |    50/  200 steps | loss 0.05681 | ppl    1.058\n",
            "| epoch   5 |   100/  200 steps | loss 0.04753 | ppl    1.049\n",
            "| epoch   5 |   150/  200 steps | loss 0.05361 | ppl    1.055\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7575\n",
            "| epoch   6 |    50/  200 steps | loss 0.00030 | ppl    1.000\n",
            "| epoch   6 |   100/  200 steps | loss 0.00070 | ppl    1.001\n",
            "| epoch   6 |   150/  200 steps | loss 0.00450 | ppl    1.005\n",
            "from_scratch=True - VALIDATION ACCURACY 0.757\n",
            "| epoch   7 |    50/  200 steps | loss 0.00006 | ppl    1.000\n",
            "| epoch   7 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
            "| epoch   7 |   150/  200 steps | loss 0.00089 | ppl    1.001\n",
            "from_scratch=True - VALIDATION ACCURACY 0.747\n",
            "| epoch   8 |    50/  200 steps | loss 0.00019 | ppl    1.000\n",
            "| epoch   8 |   100/  200 steps | loss 0.01865 | ppl    1.019\n",
            "| epoch   8 |   150/  200 steps | loss 0.01828 | ppl    1.018\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7525\n",
            "| epoch   9 |    50/  200 steps | loss 0.00229 | ppl    1.002\n",
            "| epoch   9 |   100/  200 steps | loss 0.00328 | ppl    1.003\n",
            "| epoch   9 |   150/  200 steps | loss 0.01874 | ppl    1.019\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7465\n",
            "| epoch  10 |    50/  200 steps | loss 0.01681 | ppl    1.017\n",
            "| epoch  10 |   100/  200 steps | loss 0.00304 | ppl    1.003\n",
            "| epoch  10 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7235\n",
            "| epoch  11 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7205\n",
            "| epoch  12 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7235\n",
            "| epoch  13 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.725\n",
            "| epoch  14 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.726\n",
            "| epoch  15 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "from_scratch=True - VALIDATION ACCURACY 0.7255\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [False, True]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model_ds = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model_ds.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model_ds.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    # @TODO: check batch size subtlety on the last element (we want to classify the last feature of the sentence!)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            model_ds,\n",
        "            downstream_path_data_train,\n",
        "            downstream_path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task=DS_TASK,\n",
        "            freeze_backbone=not from_scratch,\n",
        "            batch_size=8,\n",
        "            # batch_size=1, # TO AVOID THE NEED TO RETRIEVE THE RIGHT LAST TOKEN IN A BATCH\n",
        "            log_interval=50,\n",
        "            epoch=epoch,\n",
        "            name=\"_from_scratch\" if from_scratch else \"_pretrained\"\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            data_loader_validation,\n",
        "            model_ds\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "        print(f\"{from_scratch=} - VALIDATION ACCURACY {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.15 ('altegrad')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f3cfdeab8dd8f9900bd16266619de191cf0f5e09365d74b1fba1714dce58066"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
